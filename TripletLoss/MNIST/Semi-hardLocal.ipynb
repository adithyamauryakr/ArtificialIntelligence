{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Semi-hardLocal.ipynb","provenance":[{"file_id":"https://github.com/tensorflow/addons/blob/master/docs/tutorials/losses_triplet.ipynb","timestamp":1584947230478}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3rc1"}},"cells":[{"cell_type":"markdown","metadata":{"id":"btBUBGHzbG4k","colab_type":"text"},"source":["# Import TensorFlow 2.x."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WH_7-ZYZYblV","colab":{}},"source":["try:\n","  %tensorflow_version 2.x\n","except:\n","  pass\n","\n","import tensorflow as tf\n","import tensorflow.keras.layers as layers\n","import tensorflow.keras.models as models\n","\n","import numpy as np\n","np.random.seed(7)\n","\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8_k0DZxbWOh","colab_type":"text"},"source":["# Import TensorFlow datasets.\n","*   MNIST dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LC4YvwOoYblc","colab":{}},"source":["import tensorflow_datasets as tfds"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0_D7CZqkv_Hj"},"source":["# Load MNIST dataset."]},{"cell_type":"markdown","metadata":{"id":"jU21HsZqcodH","colab_type":"text"},"source":["### Load MNIST dataset.\n","* train split\n","* test split"]},{"cell_type":"code","metadata":{"id":"0YydNnlmCv1E","colab_type":"code","colab":{}},"source":["train_dataset, test_dataset = tfds.load(name=\"mnist\", split=['train', 'test'], as_supervised=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FGSFjNCYcyoz","colab_type":"text"},"source":["### Normalize dataset images."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iXvByj6wcT7d","colab":{}},"source":["def _normalize_image(image, label):\n","    image = tf.cast(image, tf.float32) / 255.\n","    return (image, label)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"28CQ2Q6Ic2gF","colab_type":"text"},"source":["### Create dataset batches."]},{"cell_type":"code","metadata":{"id":"7M_UzEgZG77Y","colab_type":"code","colab":{}},"source":["buffer_size = 1024\n","batch_size = 32"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a0hqhMkQC0Eg","colab_type":"code","colab":{}},"source":["train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size)\n","train_dataset = train_dataset.map(_normalize_image)\n","\n","test_dataset = test_dataset.batch(batch_size)\n","test_dataset = test_dataset.map(_normalize_image)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KR01t9v_fxbT"},"source":["# Create the model.\n","* No activation (or default linear activation) on last layer\n","* L2 normalized embeddings."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"djpoAvfWNyL5","colab":{}},"source":["model = models.Sequential([\n","    layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1)),\n","    layers.Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'),\n","    layers.MaxPooling2D(pool_size=2),\n","\n","    layers.Dropout(0.3),\n","\n","    layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'),\n","    layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'),\n","    layers.MaxPooling2D(pool_size=2),\n","\n","    layers.Dropout(0.3),\n","\n","    layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'),\n","    layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'),\n","    layers.MaxPooling2D(pool_size=2),\n","\n","    layers.Dropout(0.3),\n","    \n","    layers.Flatten(),\n","    layers.Dense(256, activation=None),                      # No activation on final dense layer\n","    layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalized embeddings\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dFBcPm9jeY9i","colab_type":"text"},"source":["### Show model summary."]},{"cell_type":"code","metadata":{"id":"P6YVxK4peQPC","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HYE-BxhOzFQp"},"source":["# Train the model."]},{"cell_type":"code","metadata":{"id":"axUoO4rOnJC1","colab_type":"code","colab":{}},"source":["def pairwise_distance(feature, squared=False):\n","    \"\"\"Computes the pairwise distance matrix with numerical stability.\n","    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n","    Args:\n","      feature: 2-D Tensor of size [number of data, feature dimension].\n","      squared: Boolean, whether or not to square the pairwise distances.\n","    Returns:\n","      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n","    \"\"\"\n","    # yapf: disable\n","    pairwise_distances_squared = tf.math.add(\n","        tf.math.reduce_sum(\n","            tf.math.square(feature),\n","            axis=[1],\n","            keepdims=True),\n","        tf.math.reduce_sum(\n","            tf.math.square(tf.transpose(feature)),\n","            axis=[0],\n","            keepdims=True)) - 2.0 * tf.matmul(feature, tf.transpose(feature))\n","    # yapf: enable\n","\n","    # Deal with numerical inaccuracies. Set small negatives to zero.\n","    pairwise_distances_squared = tf.math.maximum(pairwise_distances_squared,\n","                                                 0.0)\n","    # Get the mask where the zero distances are at.\n","    error_mask = tf.math.less_equal(pairwise_distances_squared, 0.0)\n","\n","    # Optionally take the sqrt.\n","    if squared:\n","        pairwise_distances = pairwise_distances_squared\n","    else:\n","        pairwise_distances = tf.math.sqrt(\n","            pairwise_distances_squared +\n","            tf.cast(error_mask, dtype=tf.dtypes.float32) * 1e-16)\n","\n","    # Undo conditionally adding 1e-16.\n","    pairwise_distances = tf.math.multiply(\n","        pairwise_distances,\n","        tf.cast(tf.math.logical_not(error_mask), dtype=tf.dtypes.float32))\n","\n","    num_data = tf.shape(feature)[0]\n","    # Explicitly set diagonals to zero.\n","    mask_offdiagonals = tf.ones_like(pairwise_distances) - tf.linalg.diag(\n","        tf.ones([num_data]))\n","    pairwise_distances = tf.math.multiply(pairwise_distances,\n","                                          mask_offdiagonals)\n","    return pairwise_distances"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bjjciPH_nJu1","colab_type":"code","colab":{}},"source":["def _masked_maximum(data, mask, dim=1):\n","    \"\"\"Computes the axis wise maximum over chosen elements.\n","    Args:\n","      data: 2-D float `Tensor` of size [n, m].\n","      mask: 2-D Boolean `Tensor` of size [n, m].\n","      dim: The dimension over which to compute the maximum.\n","    Returns:\n","      masked_maximums: N-D `Tensor`.\n","        The maximized dimension is of size 1 after the operation.\n","    \"\"\"\n","    axis_minimums = tf.math.reduce_min(data, dim, keepdims=True)\n","    masked_maximums = tf.math.reduce_max(\n","        tf.math.multiply(data - axis_minimums, mask), dim,\n","        keepdims=True) + axis_minimums\n","    return masked_maximums"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JMbon1kynL3t","colab_type":"code","colab":{}},"source":["def _masked_minimum(data, mask, dim=1):\n","    \"\"\"Computes the axis wise minimum over chosen elements.\n","    Args:\n","      data: 2-D float `Tensor` of size [n, m].\n","      mask: 2-D Boolean `Tensor` of size [n, m].\n","      dim: The dimension over which to compute the minimum.\n","    Returns:\n","      masked_minimums: N-D `Tensor`.\n","        The minimized dimension is of size 1 after the operation.\n","    \"\"\"\n","    axis_maximums = tf.math.reduce_max(data, dim, keepdims=True)\n","    masked_minimums = tf.math.reduce_min(\n","        tf.math.multiply(data - axis_maximums, mask), dim,\n","        keepdims=True) + axis_maximums\n","    return masked_minimums"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dra-4NDanNo3","colab_type":"code","colab":{}},"source":["def triplet_semihard_loss(y_true, y_pred, margin=1.0):\n","    \"\"\"Computes the triplet loss with semi-hard negative mining.\n","    Args:\n","      y_true: 1-D integer `Tensor` with shape [batch_size] of\n","        multiclass integer labels.\n","      y_pred: 2-D float `Tensor` of embedding vectors. Embeddings should\n","        be l2 normalized.\n","      margin: Float, margin term in the loss definition.\n","    \"\"\"\n","    labels, embeddings = y_true, y_pred\n","    # Reshape label tensor to [batch_size, 1].\n","    lshape = tf.shape(labels)\n","    labels = tf.reshape(labels, [lshape[0], 1])\n","\n","    # Build pairwise squared distance matrix.\n","    pdist_matrix = pairwise_distance(embeddings, squared=True)\n","    # Build pairwise binary adjacency matrix.\n","    adjacency = tf.math.equal(labels, tf.transpose(labels))\n","    # Invert so we can select negatives only.\n","    adjacency_not = tf.math.logical_not(adjacency)\n","\n","    batch_size = tf.size(labels)\n","\n","    # Compute the mask.\n","    pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n","    mask = tf.math.logical_and(\n","        tf.tile(adjacency_not, [batch_size, 1]),\n","        tf.math.greater(pdist_matrix_tile,\n","                        tf.reshape(tf.transpose(pdist_matrix), [-1, 1])))\n","    mask_final = tf.reshape(\n","        tf.math.greater(\n","            tf.math.reduce_sum(\n","                tf.cast(mask, dtype=tf.dtypes.float32), 1, keepdims=True),\n","            0.0), [batch_size, batch_size])\n","    mask_final = tf.transpose(mask_final)\n","\n","    adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n","    mask = tf.cast(mask, dtype=tf.dtypes.float32)\n","\n","    # negatives_outside: smallest D_an where D_an > D_ap.\n","    negatives_outside = tf.reshape(\n","        _masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n","    negatives_outside = tf.transpose(negatives_outside)\n","\n","    # negatives_inside: largest D_an.\n","    negatives_inside = tf.tile(\n","        _masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n","    semi_hard_negatives = tf.where(mask_final, negatives_outside,\n","                                   negatives_inside)\n","\n","    loss_mat = tf.math.add(margin, pdist_matrix - semi_hard_negatives)\n","\n","    mask_positives = tf.cast(\n","        adjacency, dtype=tf.dtypes.float32) - tf.linalg.diag(\n","            tf.ones([batch_size]))\n","\n","    # In lifted-struct, the authors multiply 0.5 for upper triangular\n","    #   in semihard, they take all positive pairs except the diagonal.\n","    num_positives = tf.math.reduce_sum(mask_positives)\n","\n","    triplet_loss = tf.math.truediv(\n","        tf.math.reduce_sum(\n","            tf.math.maximum(tf.math.multiply(loss_mat, mask_positives), 0.0)),\n","        num_positives)\n","\n","    return triplet_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4EewbsknPrw","colab_type":"code","colab":{}},"source":["class TripletSemiHardLoss(tf.keras.losses.Loss):\n","    \"\"\"Computes the triplet loss with semi-hard negative mining.\n","    The loss encourages the positive distances (between a pair of embeddings\n","    with the same labels) to be smaller than the minimum negative distance\n","    among which are at least greater than the positive distance plus the\n","    margin constant (called semi-hard negative) in the mini-batch.\n","    If no such negative exists, uses the largest negative distance instead.\n","    See: https://arxiv.org/abs/1503.03832.\n","    We expect labels `y_true` to be provided as 1-D integer `Tensor` with shape\n","    [batch_size] of multi-class integer labels. And embeddings `y_pred` must be\n","    2-D float `Tensor` of l2 normalized embedding vectors.\n","    Args:\n","      margin: Float, margin term in the loss definition. Default value is 1.0.\n","      name: Optional name for the op.\n","    \"\"\"\n","\n","    def __init__(self, margin=1.0, name=None, **kwargs):\n","        super(TripletSemiHardLoss, self).__init__(\n","            name=name, reduction=tf.keras.losses.Reduction.NONE)\n","        self.margin = margin\n","\n","    def call(self, y_true, y_pred):\n","        return triplet_semihard_loss(y_true, y_pred, self.margin)\n","\n","    def get_config(self):\n","        config = {\n","            \"margin\": self.margin,\n","        }\n","        base_config = super(TripletSemiHardLoss, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BeKqFNO0eik4","colab_type":"text"},"source":["### Compile the model."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NxfYhtiSzHf-","colab":{}},"source":["model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=TripletSemiHardLoss())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"teG8RiwJepDm","colab_type":"text"},"source":["### Train the model."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TGBYNGxgVDrj","colab":{}},"source":["history = model.fit(train_dataset, epochs=10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JZ93LDCHe0Wz","colab_type":"text"},"source":["# Evaluate the model.\n","* Create the embeddings for the test dataset.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1Y--0tK69SXf","colab":{}},"source":["embeddings = model.predict(test_dataset)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zpGyDY2UfKJy","colab_type":"text"},"source":["# Save the embeddings for visualization in the embedding projector."]},{"cell_type":"code","metadata":{"id":"owevuCOAfKpy","colab_type":"code","colab":{}},"source":["import io\n","\n","np.savetxt(\"embeddings-vecs.tsv\", embeddings, delimiter='\\t')\n","\n","meta_file = io.open('embeddings-meta.tsv', 'w', encoding='utf-8')\n","for image, labels in tfds.as_numpy(test_dataset):\n","    [meta_file.write(str(label) + \"\\n\") for label in labels]\n","meta_file.close()"],"execution_count":0,"outputs":[]}]}