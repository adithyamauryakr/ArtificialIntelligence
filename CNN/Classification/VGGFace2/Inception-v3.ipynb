{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Inception-v3.ipynb","provenance":[{"file_id":"1TLoWY-qtjZxs1ENduq31XPZp0zB0zH82","timestamp":1591340829118}],"collapsed_sections":[],"authorship_tag":"ABX9TyPrUuAG/ExS9GWa/YEM/Xt7"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JH7Z46TRjOfN","colab_type":"text"},"source":["# Check GPU version."]},{"cell_type":"code","metadata":{"id":"3gtroS-sjSOa","colab_type":"code","colab":{}},"source":["!nvidia-smi"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DLls6Lw8jay6","colab_type":"text"},"source":["# Mount google drive."]},{"cell_type":"code","metadata":{"id":"CrWdN-ItjeA-","colab_type":"code","outputId":"932d8dd5-dd87-4494-b646-d0774a3e7212","executionInfo":{"status":"ok","timestamp":1591440302915,"user_tz":-330,"elapsed":26645,"user":{"displayName":"keras vggface2","photoUrl":"","userId":"16214177126607203010"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2GanKpOKVCjl","colab_type":"text"},"source":["# Install TensorFlow-1.14 GPU."]},{"cell_type":"code","metadata":{"id":"KwU8LfwfTdKW","colab_type":"code","outputId":"57252f1b-92fc-4afe-cf92-82c9fa49b33c","executionInfo":{"status":"ok","timestamp":1591440398843,"user_tz":-330,"elapsed":95887,"user":{"displayName":"keras vggface2","photoUrl":"","userId":"16214177126607203010"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Select TensorFlow-1.x version.\n","%tensorflow_version 1.x\n","\n","# Uninstall previous TensorFlow version.\n","!pip uninstall tensorflow -y 1>/dev/null 2>/dev/null \n","!pip uninstall tensorflow-gpu -y 1>/dev/null 2>/dev/null \n","\n","# Install TensorFlow-1.14 and Keras-2.2.4.\n","!pip install --upgrade tensorflow-gpu==1.14.0 1>/dev/null 2>/dev/null \n","!pip install --upgrade tensorflow==1.14.0 1>/dev/null 2>/dev/null \n","!pip install --upgrade keras==2.2.4 1>/dev/null 2>/dev/null \n","!pip install --upgrade scipy 1>/dev/null 2>/dev/null "],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1o9-2_Q5WB2L","colab_type":"text"},"source":["# Restart runtime."]},{"cell_type":"markdown","metadata":{"id":"vhgcf2TojTrO","colab_type":"text"},"source":["# Set the root directory."]},{"cell_type":"code","metadata":{"id":"9OOolap6jT-y","colab_type":"code","outputId":"22dad164-29b9-4f15-9f70-70467955cdb1","executionInfo":{"status":"ok","timestamp":1591440426432,"user_tz":-330,"elapsed":6214,"user":{"displayName":"keras vggface2","photoUrl":"","userId":"16214177126607203010"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["import os\n","\n","root_dir = '/content/'\n","os.chdir(root_dir)\n","\n","!ls -al"],"execution_count":1,"outputs":[{"output_type":"stream","text":["total 20\n","drwxr-xr-x 1 root root 4096 Jun  6 10:45 .\n","drwxr-xr-x 1 root root 4096 Jun  6 10:40 ..\n","drwxr-xr-x 1 root root 4096 Jun  2 16:14 .config\n","drwx------ 4 root root 4096 Jun  6 10:45 drive\n","drwxr-xr-x 1 root root 4096 May 29 18:19 sample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yO7qbt-JWLH3","colab_type":"text"},"source":["# Import TensorFlow-1.14."]},{"cell_type":"code","metadata":{"id":"fkBgJBpiWLyA","colab_type":"code","outputId":"1bdcc56e-b119-4258-8c79-c53479ef9d54","executionInfo":{"status":"ok","timestamp":1591440427865,"user_tz":-330,"elapsed":4778,"user":{"displayName":"keras vggface2","photoUrl":"","userId":"16214177126607203010"}},"colab":{"base_uri":"https://localhost:8080/","height":479}},"source":["try:\n","  %tensorflow_version 1.x\n","except Exception:\n","  pass\n","\n","import numpy as np\n","np.random.seed(7)\n","\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"},{"output_type":"stream","text":["1.14.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"VwHrRvZueSP0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"601f6f8e-e667-4b7e-9049-e1fbaec9e930","executionInfo":{"status":"ok","timestamp":1591440432596,"user_tz":-330,"elapsed":1968,"user":{"displayName":"keras vggface2","photoUrl":"","userId":"16214177126607203010"}}},"source":["from numpy import genfromtxt\n","from keras import backend as K\n","K.set_image_data_format('channels_first')\n","\n","from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n","from keras.layers.core import Lambda, Flatten, Dense\n","\n","from keras.layers.normalization import BatchNormalization\n","from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n","\n","from keras.models import Model\n","\n","import cv2"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"y1-l4pDRepuO","colab_type":"code","colab":{}},"source":["_FLOATX = 'float32'\n","\n","def variable(value, dtype=_FLOATX, name=None):\n","    v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n","    _get_session().run(v.initializer)\n","    return v\n","\n","def shape(x):\n","    return x.get_shape()\n","\n","def square(x):\n","    return tf.square(x)\n","\n","def zeros(shape, dtype=_FLOATX, name=None):\n","    return variable(np.zeros(shape), dtype, name)\n","'''\n","def concatenate(tensors, axis=-1):\n","    if axis < 0:\n","        axis = axis % len(tensors[0].get_shape())\n","    return tf.concat(axis, tensors)\n","'''\n","def LRN2D(x):\n","    return tf.nn.lrn(x, alpha=1e-4, beta=0.75)\n","\n","def custom_conv2d_bn(x,\n","              layer=None,\n","              cv1_out=None,\n","              cv1_filter=(1, 1),\n","              cv1_strides=(1, 1),\n","              cv2_out=None,\n","              cv2_filter=(3, 3),\n","              cv2_strides=(1, 1),\n","              padding=None):\n","    num = '' if cv2_out == None else '1'\n","    tensor = Conv2D(cv1_out, cv1_filter, strides=cv1_strides, data_format='channels_first', name=layer+'_conv'+num)(x)\n","    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+num)(tensor)\n","    tensor = Activation('relu')(tensor)\n","    if padding == None:\n","        return tensor\n","    tensor = ZeroPadding2D(padding=padding, data_format='channels_first')(tensor)\n","    if cv2_out == None:\n","        return tensor\n","    tensor = Conv2D(cv2_out, cv2_filter, strides=cv2_strides, data_format='channels_first', name=layer+'_conv'+'2')(tensor)\n","    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+'2')(tensor)\n","    tensor = Activation('relu')(tensor)\n","    return tensor\n","\n","WEIGHTS = [\n","  'conv1', 'bn1', 'conv2', 'bn2', 'conv3', 'bn3',\n","  'inception_3a_1x1_conv', 'inception_3a_1x1_bn',\n","  'inception_3a_pool_conv', 'inception_3a_pool_bn',\n","  'inception_3a_5x5_conv1', 'inception_3a_5x5_conv2', 'inception_3a_5x5_bn1', 'inception_3a_5x5_bn2',\n","  'inception_3a_3x3_conv1', 'inception_3a_3x3_conv2', 'inception_3a_3x3_bn1', 'inception_3a_3x3_bn2',\n","  'inception_3b_3x3_conv1', 'inception_3b_3x3_conv2', 'inception_3b_3x3_bn1', 'inception_3b_3x3_bn2',\n","  'inception_3b_5x5_conv1', 'inception_3b_5x5_conv2', 'inception_3b_5x5_bn1', 'inception_3b_5x5_bn2',\n","  'inception_3b_pool_conv', 'inception_3b_pool_bn',\n","  'inception_3b_1x1_conv', 'inception_3b_1x1_bn',\n","  'inception_3c_3x3_conv1', 'inception_3c_3x3_conv2', 'inception_3c_3x3_bn1', 'inception_3c_3x3_bn2',\n","  'inception_3c_5x5_conv1', 'inception_3c_5x5_conv2', 'inception_3c_5x5_bn1', 'inception_3c_5x5_bn2',\n","  'inception_4a_3x3_conv1', 'inception_4a_3x3_conv2', 'inception_4a_3x3_bn1', 'inception_4a_3x3_bn2',\n","  'inception_4a_5x5_conv1', 'inception_4a_5x5_conv2', 'inception_4a_5x5_bn1', 'inception_4a_5x5_bn2',\n","  'inception_4a_pool_conv', 'inception_4a_pool_bn',\n","  'inception_4a_1x1_conv', 'inception_4a_1x1_bn',\n","  'inception_4e_3x3_conv1', 'inception_4e_3x3_conv2', 'inception_4e_3x3_bn1', 'inception_4e_3x3_bn2',\n","  'inception_4e_5x5_conv1', 'inception_4e_5x5_conv2', 'inception_4e_5x5_bn1', 'inception_4e_5x5_bn2',\n","  'inception_5a_3x3_conv1', 'inception_5a_3x3_conv2', 'inception_5a_3x3_bn1', 'inception_5a_3x3_bn2',\n","  'inception_5a_pool_conv', 'inception_5a_pool_bn',\n","  'inception_5a_1x1_conv', 'inception_5a_1x1_bn',\n","  'inception_5b_3x3_conv1', 'inception_5b_3x3_conv2', 'inception_5b_3x3_bn1', 'inception_5b_3x3_bn2',\n","  'inception_5b_pool_conv', 'inception_5b_pool_bn',\n","  'inception_5b_1x1_conv', 'inception_5b_1x1_bn',\n","  'dense_layer'\n","]\n","\n","conv_shape = {\n","  'conv1': [64, 3, 7, 7],\n","  'conv2': [64, 64, 1, 1],\n","  'conv3': [192, 64, 3, 3],\n","  'inception_3a_1x1_conv': [64, 192, 1, 1],\n","  'inception_3a_pool_conv': [32, 192, 1, 1],\n","  'inception_3a_5x5_conv1': [16, 192, 1, 1],\n","  'inception_3a_5x5_conv2': [32, 16, 5, 5],\n","  'inception_3a_3x3_conv1': [96, 192, 1, 1],\n","  'inception_3a_3x3_conv2': [128, 96, 3, 3],\n","  'inception_3b_3x3_conv1': [96, 256, 1, 1],\n","  'inception_3b_3x3_conv2': [128, 96, 3, 3],\n","  'inception_3b_5x5_conv1': [32, 256, 1, 1],\n","  'inception_3b_5x5_conv2': [64, 32, 5, 5],\n","  'inception_3b_pool_conv': [64, 256, 1, 1],\n","  'inception_3b_1x1_conv': [64, 256, 1, 1],\n","  'inception_3c_3x3_conv1': [128, 320, 1, 1],\n","  'inception_3c_3x3_conv2': [256, 128, 3, 3],\n","  'inception_3c_5x5_conv1': [32, 320, 1, 1],\n","  'inception_3c_5x5_conv2': [64, 32, 5, 5],\n","  'inception_4a_3x3_conv1': [96, 640, 1, 1],\n","  'inception_4a_3x3_conv2': [192, 96, 3, 3],\n","  'inception_4a_5x5_conv1': [32, 640, 1, 1,],\n","  'inception_4a_5x5_conv2': [64, 32, 5, 5],\n","  'inception_4a_pool_conv': [128, 640, 1, 1],\n","  'inception_4a_1x1_conv': [256, 640, 1, 1],\n","  'inception_4e_3x3_conv1': [160, 640, 1, 1],\n","  'inception_4e_3x3_conv2': [256, 160, 3, 3],\n","  'inception_4e_5x5_conv1': [64, 640, 1, 1],\n","  'inception_4e_5x5_conv2': [128, 64, 5, 5],\n","  'inception_5a_3x3_conv1': [96, 1024, 1, 1],\n","  'inception_5a_3x3_conv2': [384, 96, 3, 3],\n","  'inception_5a_pool_conv': [96, 1024, 1, 1],\n","  'inception_5a_1x1_conv': [256, 1024, 1, 1],\n","  'inception_5b_3x3_conv1': [96, 736, 1, 1],\n","  'inception_5b_3x3_conv2': [384, 96, 3, 3],\n","  'inception_5b_pool_conv': [96, 736, 1, 1],\n","  'inception_5b_1x1_conv': [256, 736, 1, 1],\n","}\n","\n","def load_weights_from_FaceNet(FRmodel):\n","    # Load weights from csv files (which was exported from Openface torch model)\n","    weights = WEIGHTS\n","    weights_dict = load_weights()\n","\n","    # Set layer weights of the model\n","    for name in weights:\n","        if FRmodel.get_layer(name) != None:\n","            FRmodel.get_layer(name).set_weights(weights_dict[name])\n","        elif model.get_layer(name) != None:\n","            model.get_layer(name).set_weights(weights_dict[name])\n","\n","def load_weights():\n","    # Set weights path\n","    dirPath = '/content/drive/My Drive/models/inception'\n","    fileNames = filter(lambda f: not f.startswith('.'), os.listdir(dirPath))\n","    paths = {}\n","    weights_dict = {}\n","\n","    for n in fileNames:\n","        paths[n.replace('.csv', '')] = dirPath + '/' + n\n","\n","    for name in WEIGHTS:\n","        if 'conv' in name:\n","            conv_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n","            conv_w = np.reshape(conv_w, conv_shape[name])\n","            conv_w = np.transpose(conv_w, (2, 3, 1, 0))\n","            conv_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n","            weights_dict[name] = [conv_w, conv_b]     \n","        elif 'bn' in name:\n","            bn_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n","            bn_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n","            bn_m = genfromtxt(paths[name + '_m'], delimiter=',', dtype=None)\n","            bn_v = genfromtxt(paths[name + '_v'], delimiter=',', dtype=None)\n","            weights_dict[name] = [bn_w, bn_b, bn_m, bn_v]\n","        elif 'dense' in name:\n","            dense_w = genfromtxt(dirPath+'/dense_w.csv', delimiter=',', dtype=None)\n","            dense_w = np.reshape(dense_w, (128, 736))\n","            dense_w = np.transpose(dense_w, (1, 0))\n","            dense_b = genfromtxt(dirPath+'/dense_b.csv', delimiter=',', dtype=None)\n","            weights_dict[name] = [dense_w, dense_b]\n","\n","    return weights_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6T_KyxhnNSG","colab_type":"code","colab":{}},"source":["def compute_image_features(image_path, model):\n","    img1 = cv2.imread(image_path, 1)\n","    img1 = cv2.resize(img1, (96, 96))\n","    img = img1[...,::-1]\n","    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n","    x_train = np.array([img])\n","    embedding = model.predict_on_batch(x_train)\n","    return embedding"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKRpUlcreOto","colab_type":"code","colab":{}},"source":["def inception_block_1a(X):\n","    \"\"\"\n","    Implementation of an inception block\n","    \"\"\"\n","    \n","    X_3x3 = Conv2D(96, (1, 1), data_format='channels_first', name ='inception_3a_3x3_conv1')(X)\n","    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name = 'inception_3a_3x3_bn1')(X_3x3)\n","    X_3x3 = Activation('relu')(X_3x3)\n","    X_3x3 = ZeroPadding2D(padding=(1, 1), data_format='channels_first')(X_3x3)\n","    X_3x3 = Conv2D(128, (3, 3), data_format='channels_first', name='inception_3a_3x3_conv2')(X_3x3)\n","    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_3x3_bn2')(X_3x3)\n","    X_3x3 = Activation('relu')(X_3x3)\n","    \n","    X_5x5 = Conv2D(16, (1, 1), data_format='channels_first', name='inception_3a_5x5_conv1')(X)\n","    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_5x5_bn1')(X_5x5)\n","    X_5x5 = Activation('relu')(X_5x5)\n","    X_5x5 = ZeroPadding2D(padding=(2, 2), data_format='channels_first')(X_5x5)\n","    X_5x5 = Conv2D(32, (5, 5), data_format='channels_first', name='inception_3a_5x5_conv2')(X_5x5)\n","    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_5x5_bn2')(X_5x5)\n","    X_5x5 = Activation('relu')(X_5x5)\n","\n","    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n","    X_pool = Conv2D(32, (1, 1), data_format='channels_first', name='inception_3a_pool_conv')(X_pool)\n","    X_pool = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_pool_bn')(X_pool)\n","    X_pool = Activation('relu')(X_pool)\n","    X_pool = ZeroPadding2D(padding=((3, 4), (3, 4)), data_format='channels_first')(X_pool)\n","\n","    X_1x1 = Conv2D(64, (1, 1), data_format='channels_first', name='inception_3a_1x1_conv')(X)\n","    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_1x1_bn')(X_1x1)\n","    X_1x1 = Activation('relu')(X_1x1)\n","        \n","    # CONCAT\n","    inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n","\n","    return inception\n","\n","def inception_block_1b(X):\n","    X_3x3 = Conv2D(96, (1, 1), data_format='channels_first', name='inception_3b_3x3_conv1')(X)\n","    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_3x3_bn1')(X_3x3)\n","    X_3x3 = Activation('relu')(X_3x3)\n","    X_3x3 = ZeroPadding2D(padding=(1, 1), data_format='channels_first')(X_3x3)\n","    X_3x3 = Conv2D(128, (3, 3), data_format='channels_first', name='inception_3b_3x3_conv2')(X_3x3)\n","    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_3x3_bn2')(X_3x3)\n","    X_3x3 = Activation('relu')(X_3x3)\n","\n","    X_5x5 = Conv2D(32, (1, 1), data_format='channels_first', name='inception_3b_5x5_conv1')(X)\n","    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_5x5_bn1')(X_5x5)\n","    X_5x5 = Activation('relu')(X_5x5)\n","    X_5x5 = ZeroPadding2D(padding=(2, 2), data_format='channels_first')(X_5x5)\n","    X_5x5 = Conv2D(64, (5, 5), data_format='channels_first', name='inception_3b_5x5_conv2')(X_5x5)\n","    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_5x5_bn2')(X_5x5)\n","    X_5x5 = Activation('relu')(X_5x5)\n","\n","    X_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3), data_format='channels_first')(X)\n","    X_pool = Conv2D(64, (1, 1), data_format='channels_first', name='inception_3b_pool_conv')(X_pool)\n","    X_pool = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_pool_bn')(X_pool)\n","    X_pool = Activation('relu')(X_pool)\n","    X_pool = ZeroPadding2D(padding=(4, 4), data_format='channels_first')(X_pool)\n","\n","    X_1x1 = Conv2D(64, (1, 1), data_format='channels_first', name='inception_3b_1x1_conv')(X)\n","    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_1x1_bn')(X_1x1)\n","    X_1x1 = Activation('relu')(X_1x1)\n","\n","    inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n","\n","    return inception\n","\n","def inception_block_1c(X):\n","    X_3x3 = custom_conv2d_bn(X,\n","                           layer='inception_3c_3x3',\n","                           cv1_out=128,\n","                           cv1_filter=(1, 1),\n","                           cv2_out=256,\n","                           cv2_filter=(3, 3),\n","                           cv2_strides=(2, 2),\n","                           padding=(1, 1))\n","\n","    X_5x5 = custom_conv2d_bn(X,\n","                           layer='inception_3c_5x5',\n","                           cv1_out=32,\n","                           cv1_filter=(1, 1),\n","                           cv2_out=64,\n","                           cv2_filter=(5, 5),\n","                           cv2_strides=(2, 2),\n","                           padding=(2, 2))\n","\n","    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n","    X_pool = ZeroPadding2D(padding=((0, 1), (0, 1)), data_format='channels_first')(X_pool)\n","\n","    inception = concatenate([X_3x3, X_5x5, X_pool], axis=1)\n","\n","    return inception\n","\n","def inception_block_2a(X):\n","    X_3x3 = custom_conv2d_bn(X,\n","                           layer='inception_4a_3x3',\n","                           cv1_out=96,\n","                           cv1_filter=(1, 1),\n","                           cv2_out=192,\n","                           cv2_filter=(3, 3),\n","                           cv2_strides=(1, 1),\n","                           padding=(1, 1))\n","    X_5x5 = custom_conv2d_bn(X,\n","                           layer='inception_4a_5x5',\n","                           cv1_out=32,\n","                           cv1_filter=(1, 1),\n","                           cv2_out=64,\n","                           cv2_filter=(5, 5),\n","                           cv2_strides=(1, 1),\n","                           padding=(2, 2))\n","\n","    X_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3), data_format='channels_first')(X)\n","    X_pool = custom_conv2d_bn(X_pool,\n","                           layer='inception_4a_pool',\n","                           cv1_out=128,\n","                           cv1_filter=(1, 1),\n","                           padding=(2, 2))\n","    X_1x1 = custom_conv2d_bn(X,\n","                           layer='inception_4a_1x1',\n","                           cv1_out=256,\n","                           cv1_filter=(1, 1))\n","    inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n","\n","    return inception\n","\n","def inception_block_2b(X):\n","    #inception4e\n","    X_3x3 = custom_conv2d_bn(X,\n","                           layer='inception_4e_3x3',\n","                           cv1_out=160,\n","                           cv1_filter=(1, 1),\n","                           cv2_out=256,\n","                           cv2_filter=(3, 3),\n","                           cv2_strides=(2, 2),\n","                           padding=(1, 1))\n","    X_5x5 = custom_conv2d_bn(X,\n","                           layer='inception_4e_5x5',\n","                           cv1_out=64,\n","                           cv1_filter=(1, 1),\n","                           cv2_out=128,\n","                           cv2_filter=(5, 5),\n","                           cv2_strides=(2, 2),\n","                           padding=(2, 2))\n","    \n","    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n","    X_pool = ZeroPadding2D(padding=((0, 1), (0, 1)), data_format='channels_first')(X_pool)\n","\n","    inception = concatenate([X_3x3, X_5x5, X_pool], axis=1)\n","\n","    return inception\n","\n","def inception_block_3a(X):\n","    X_3x3 = custom_conv2d_bn(X,\n","                           layer='inception_5a_3x3',\n","                           cv1_out=96,\n","                           cv1_filter=(1, 1),\n","                           cv2_out=384,\n","                           cv2_filter=(3, 3),\n","                           cv2_strides=(1, 1),\n","                           padding=(1, 1))\n","    X_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3), data_format='channels_first')(X)\n","    X_pool = custom_conv2d_bn(X_pool,\n","                           layer='inception_5a_pool',\n","                           cv1_out=96,\n","                           cv1_filter=(1, 1),\n","                           padding=(1, 1))\n","    X_1x1 = custom_conv2d_bn(X,\n","                           layer='inception_5a_1x1',\n","                           cv1_out=256,\n","                           cv1_filter=(1, 1))\n","\n","    inception = concatenate([X_3x3, X_pool, X_1x1], axis=1)\n","\n","    return inception\n","\n","def inception_block_3b(X):\n","    X_3x3 = custom_conv2d_bn(X,\n","                           layer='inception_5b_3x3',\n","                           cv1_out=96,\n","                           cv1_filter=(1, 1),\n","                           cv2_out=384,\n","                           cv2_filter=(3, 3),\n","                           cv2_strides=(1, 1),\n","                           padding=(1, 1))\n","    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n","    X_pool = custom_conv2d_bn(X_pool,\n","                           layer='inception_5b_pool',\n","                           cv1_out=96,\n","                           cv1_filter=(1, 1))\n","    X_pool = ZeroPadding2D(padding=(1, 1), data_format='channels_first')(X_pool)\n","\n","    X_1x1 = custom_conv2d_bn(X,\n","                           layer='inception_5b_1x1',\n","                           cv1_out=256,\n","                           cv1_filter=(1, 1))\n","    inception = concatenate([X_3x3, X_pool, X_1x1], axis=1)\n","\n","    return inception\n","\n","def faceRecoModel(input_shape):\n","    \"\"\"\n","    Implementation of the Inception model used for FaceNet\n","    \n","    Arguments:\n","    input_shape -- shape of the images of the dataset\n","\n","    Returns:\n","    model -- a Model() instance in Keras\n","    \"\"\"\n","        \n","    # Define the input as a tensor with shape input_shape\n","    X_input = Input(input_shape)\n","\n","    # Zero-Padding\n","    X = ZeroPadding2D((3, 3))(X_input)\n","    \n","    # First Block\n","    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n","    X = BatchNormalization(axis = 1, name = 'bn1')(X)\n","    X = Activation('relu')(X)\n","    \n","    # Zero-Padding + MAXPOOL\n","    X = ZeroPadding2D((1, 1))(X)\n","    X = MaxPooling2D((3, 3), strides = 2)(X)\n","    \n","    # Second Block\n","    X = Conv2D(64, (1, 1), strides = (1, 1), name = 'conv2')(X)\n","    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn2')(X)\n","    X = Activation('relu')(X)\n","    \n","    # Zero-Padding + MAXPOOL\n","    X = ZeroPadding2D((1, 1))(X)\n","\n","    # Second Block\n","    X = Conv2D(192, (3, 3), strides = (1, 1), name = 'conv3')(X)\n","    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn3')(X)\n","    X = Activation('relu')(X)\n","    \n","    # Zero-Padding + MAXPOOL\n","    X = ZeroPadding2D((1, 1))(X)\n","    X = MaxPooling2D(pool_size = 3, strides = 2)(X)\n","    \n","    # Inception 1: a/b/c\n","    X = inception_block_1a(X)\n","    X = inception_block_1b(X)\n","    X = inception_block_1c(X)\n","    \n","    # Inception 2: a/b\n","    X = inception_block_2a(X)\n","    X = inception_block_2b(X)\n","    \n","    # Inception 3: a/b\n","    X = inception_block_3a(X)\n","    X = inception_block_3b(X)\n","    \n","    # Top layer\n","    X = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), data_format='channels_first')(X)\n","    X = Flatten()(X)\n","    X = Dense(128, name='dense_layer')(X)\n","    \n","    # L2 normalization\n","    X = Lambda(lambda  x: K.l2_normalize(x,axis=1))(X)\n","\n","    # Create model instance\n","    model = Model(inputs = X_input, outputs = X, name='FaceRecoModel')\n","        \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TuYvkgV-hL-T","colab_type":"code","outputId":"059fdc7e-b027-470b-8c2d-cd648f2fd67a","executionInfo":{"status":"ok","timestamp":1591440466171,"user_tz":-330,"elapsed":6161,"user":{"displayName":"keras vggface2","photoUrl":"","userId":"16214177126607203010"}},"colab":{"base_uri":"https://localhost:8080/","height":275}},"source":["FRmodel = faceRecoModel(input_shape=(3, 96, 96))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wskd0mg4lmMA","colab_type":"code","colab":{}},"source":["load_weights_from_FaceNet(FRmodel)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hsZPs6vgnXuV","colab_type":"code","colab":{}},"source":["image_shape = (3, 96, 96)\n","aligned_image_dir = '/content/drive/My Drive/aligned_images/'\n","test_image_dir = '/content/drive/My Drive/test_images/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4_RBG7lnj1W","colab_type":"code","colab":{}},"source":["def create_database(aligned_image_dir):\n","  image_features = {}\n","  image_filenames = os.listdir(aligned_image_dir)\n","  for image_filename in image_filenames:\n","    identifier = image_filename.split('.jpg')\n","    identifier = identifier[0]\n","\n","    image_path = os.path.join(aligned_image_dir, image_filename)\n","    current_features = compute_image_features(image_path, FRmodel)\n","\n","    image_features[identifier] = current_features\n","  return(image_features)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mQn5E2CnxdE","colab_type":"code","colab":{}},"source":["database = create_database(aligned_image_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bqU98XBSovRU","colab_type":"code","colab":{}},"source":["def verify_person(image_path, identity, database, model, threshold=0.7):\n","  current_encoding = compute_image_features(image_path, model) \n","  distance = np.linalg.norm(current_encoding - database[identity])\n","  if ( distance < threshold ):        \n","        status = True\n","  else:        \n","        status = False\n","\n","  return(status, distance)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6j1CjBxo4bs","colab_type":"code","colab":{}},"source":["def verify_persons(database, test_image_dir):\n","  image_filenames = os.listdir(test_image_dir)\n","  for image_filename in image_filenames:\n","    identifier = image_filename.split('.jpg')\n","    identifier = identifier[0]\n","    identifier = 'EmmaWatson'\n","\n","    image_path = os.path.join(test_image_dir, image_filename)    \n","    status, distance = verify_person(image_path, identifier, database, FRmodel)\n","    print('**************************************************')\n","    print('ground truth -', identifier)\n","    print('status -',status)\n","    print('distance -',distance)\n","    print('**************************************************')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IUJHqDbLqiqJ","colab_type":"code","outputId":"8ab21f06-cea9-4310-b2f1-11d4716fe2f5","executionInfo":{"status":"ok","timestamp":1591440740819,"user_tz":-330,"elapsed":2182,"user":{"displayName":"keras vggface2","photoUrl":"","userId":"16214177126607203010"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["verify_persons(database, test_image_dir)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["**************************************************\n","ground truth - EmmaWatson\n","status - True\n","distance - 0.0\n","**************************************************\n","**************************************************\n","ground truth - EmmaWatson\n","status - False\n","distance - 1.001035\n","**************************************************\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NnH-JMYiOC6v","colab_type":"text"},"source":["# Identify person using pre-computed image features."]},{"cell_type":"code","metadata":{"id":"Ot9bu_yjOFWf","colab_type":"code","colab":{}},"source":["def identify_person(database, current_encoding, threshold=0.7):\n","  person_name = 'unknown'\n","  minimum_distance = float('inf')\n","  for person in database:\n","    current_distance = np.linalg.norm(current_encoding - database[person])\n","    if(current_distance < minimum_distance):\n","      minimum_distance = current_distance\n","      person_name = person\n","\n","  if(minimum_distance > threshold):\n","    person_name = 'unknown'\n","\n","  return(person_name, minimum_distance)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ZPqyKhNNxRy","colab_type":"text"},"source":["# Test one-shot recognition."]},{"cell_type":"code","metadata":{"id":"gqA6nBr-Nz9o","colab_type":"code","colab":{}},"source":["def identify_persons(database, test_image_dir):\n","  image_filenames = os.listdir(test_image_dir)\n","  for image_filename in image_filenames:\n","    identifier = image_filename.split('.jpg')\n","    identifier = identifier[0]\n","\n","    image_path = os.path.join(test_image_dir, image_filename)\n","    current_encoding = compute_image_features(image_path, FRmodel)\n","    person_name, minimum_distance = identify_person(database, current_encoding)\n","    print('**************************************************')\n","    print('ground truth -', identifier)\n","    print('predicted -',person_name)\n","    print('distance -',minimum_distance)\n","    print('**************************************************')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hs-DXnLJQt6c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"ea3b5581-a7f0-4a8b-9285-92a64bd9b02d","executionInfo":{"status":"ok","timestamp":1591440829500,"user_tz":-330,"elapsed":1661,"user":{"displayName":"keras vggface2","photoUrl":"","userId":"16214177126607203010"}}},"source":["identify_persons(database, test_image_dir)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["**************************************************\n","ground truth - EmmaWatson\n","predicted - EmmaWatson\n","distance - 0.0\n","**************************************************\n","**************************************************\n","ground truth - PriyankaChopra\n","predicted - unknown\n","distance - 1.001035\n","**************************************************\n"],"name":"stdout"}]}]}