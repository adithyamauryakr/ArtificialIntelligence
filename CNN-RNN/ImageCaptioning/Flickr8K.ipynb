{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Flickr8K.ipynb","provenance":[{"file_id":"https://github.com/hlamba28/Automatic-Image-Captioning/blob/master/Automatic%20Image%20Captioning.ipynb","timestamp":1592902831312}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"woowFiNxacba","colab_type":"text"},"source":["# Check GPU version."]},{"cell_type":"code","metadata":{"id":"qPYvCaVFaebA","colab_type":"code","colab":{}},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J8e6KznSag1q","colab_type":"text"},"source":["# Mount google drive."]},{"cell_type":"code","metadata":{"id":"0Gy9o7qRajfo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592990005682,"user_tz":-330,"elapsed":1640,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"887ecc44-ca8d-4778-a547-8a436b2a4546"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TfssNM9Galtd","colab_type":"text"},"source":["# Install TensorFlow-1.14 GPU."]},{"cell_type":"code","metadata":{"id":"VUiNkEKZamBi","colab_type":"code","colab":{}},"source":["# Select TensorFlow-1.x version.\n","%tensorflow_version 1.x\n","\n","# Uninstall previous TensorFlow version.\n","!pip uninstall tensorflow -y 1>/dev/null 2>/dev/null \n","!pip uninstall tensorflow-gpu -y 1>/dev/null 2>/dev/null \n","\n","# Install TensorFlow-1.14.\n","!pip install --upgrade tensorflow==1.14.0 1>/dev/null 2>/dev/null \n","!pip install --upgrade tensorflow-gpu==1.14.0 1>/dev/null 2>/dev/null "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yEp6815tas8v","colab_type":"text"},"source":["# Restart the runtime."]},{"cell_type":"markdown","metadata":{"id":"bsJ0ZNJ1axM0","colab_type":"text"},"source":["# Set the root directory."]},{"cell_type":"code","metadata":{"id":"NvbjEihMatQK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"executionInfo":{"status":"ok","timestamp":1592990014022,"user_tz":-330,"elapsed":5817,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"972b79a5-1008-406d-bb6e-ec917cc880e9"},"source":["import os\n","\n","root_dir = '/content/'\n","os.chdir(root_dir)\n","\n","!ls -al"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 1087752\n","drwxr-xr-x 1 root root       4096 Jun 24 05:04 .\n","drwxr-xr-x 1 root root       4096 Jun 24 05:01 ..\n","drwxr-xr-x 1 root root       4096 Jun 19 16:15 .config\n","drwx------ 4 root root       4096 Jun 24 05:03 drive\n","drwxr-xr-x 3 root root       4096 Jun 24 05:17 Flickr8K\n","-rw-r--r-- 1 root root 1113826626 Jun 24 05:03 Flickr8K.tar.gz\n","drwxr-xr-x 1 root root       4096 Jun 17 16:18 sample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q8q0R6BKa2RC","colab_type":"text"},"source":["# Import TensorFlow-1.14."]},{"cell_type":"code","metadata":{"id":"6QfwAFpqa2ll","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":479},"executionInfo":{"status":"ok","timestamp":1592990015036,"user_tz":-330,"elapsed":4395,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"816ad906-2745-43c8-e2d2-36ecfbdc7f08"},"source":["try:\n","  %tensorflow_version 1.x\n","except Exception:\n","  pass\n","\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","\n","import tensorflow.keras.layers as layers\n","import tensorflow.keras.models as models\n","\n","import numpy as np\n","np.random.seed(7)\n","\n","import matplotlib.pyplot as plot\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"},{"output_type":"stream","text":["1.14.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"1tSLcy05fS5D","colab_type":"text"},"source":["# Download Flickr8K dataset."]},{"cell_type":"markdown","metadata":{"id":"_SOUamO-g71W","colab_type":"text"},"source":["### Download dataset."]},{"cell_type":"code","metadata":{"id":"1j2GwaGJfZuU","colab_type":"code","colab":{}},"source":["!gdown --id 15IPp8p_b4BrLuOIWAmm1jknt0Ip-WZ-F\n","!ls -al"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ca6-J-cug_6r","colab_type":"text"},"source":["### Extract dataset."]},{"cell_type":"code","metadata":{"id":"98PCsJFYhCsn","colab_type":"code","colab":{}},"source":["!tar -xzf Flickr8K.tar.gz\n","!ls -al\n","!ls -al Flickr8K"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ouxcpy_wm0Yg","colab_type":"text"},"source":["# Load raw descriptions."]},{"cell_type":"code","metadata":{"id":"mbaOgwCHnKOm","colab_type":"code","colab":{}},"source":["Flickr8K_root_dir = 'Flickr8K'\n","dataset_images_dir = os.path.join(Flickr8K_root_dir, 'images') + '/'\n","descriptions_filename = os.path.join(Flickr8K_root_dir, 'token.txt')\n","train_dataset_filename = os.path.join(Flickr8K_root_dir, 'train_images.txt')\n","test_dataset_filename = os.path.join(Flickr8K_root_dir, 'test_images.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qx2xr9hAtroH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592993520388,"user_tz":-330,"elapsed":1332,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}}},"source":["image_features_root_dir = 'Inception-v3'\n","processed_descriptions_filename = os.path.join(image_features_root_dir, 'descriptions.txt')\n","train_features_filename = os.path.join(image_features_root_dir, 'train_features.pkl')\n","test_features_filename = os.path.join(image_features_root_dir, 'test_features.pkl')"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bas4ZJOoaAB8","colab_type":"code","colab":{}},"source":["def load_document(descriptions_filename):\n","\ttext_file = open(descriptions_filename, 'r')\n","\ttext_data = text_file.read()\n","\ttext_file.close()\n","\treturn( text_data )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3jogfgpgoCX7","colab_type":"text"},"source":["### Load raw descriptions."]},{"cell_type":"code","metadata":{"id":"OWX06z0gnltN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1592990021315,"user_tz":-330,"elapsed":1462,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"70f836c6-b529-4628-9c74-4cfedbc6d13c"},"source":["raw_descriptions = load_document(descriptions_filename)\n","print(raw_descriptions[:300])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n","1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n","1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n","1000268201_693b08cb0e.jpg#3\tA little girl climbing the s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MbQOPef_pf7a","colab_type":"text"},"source":["# Parse descriptions."]},{"cell_type":"code","metadata":{"id":"6qEK_ruIaACF","colab_type":"code","colab":{}},"source":["def parse_descriptions(raw_descriptions):\n","\tmapping = dict()\n","\n","\tfor line in raw_descriptions.split('\\n'):\n","\n","\t\ttokens = line.split()\n","\t\tif len(line) < 2:\n","\t\t\tcontinue\n","\n","\t\timage_id, image_descriptions = tokens[0], tokens[1:]\n","\t\timage_id = image_id.split('.')[0]\n","\n","\t\timage_descriptions = ' '.join(image_descriptions)\n","\n","\t\tif image_id not in mapping:\n","\t\t\tmapping[image_id] = list()\n","\n","\t\tmapping[image_id].append(image_descriptions)\n","\t\n","\treturn mapping"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5WZQEsc-pu7t","colab_type":"text"},"source":["### Parse descriptions."]},{"cell_type":"code","metadata":{"id":"kYmrIgpfps4b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592990025627,"user_tz":-330,"elapsed":1811,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"11ccf457-4681-4f0d-b8f6-0bb0b923cfb4"},"source":["descriptions = parse_descriptions(raw_descriptions)\n","print('loaded - %d descriptions.' % len(descriptions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loaded - 8092 descriptions.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"094uiA6Iqzfp","colab_type":"text"},"source":["### View descriptions keys."]},{"cell_type":"code","metadata":{"id":"vFznn8VcaACL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1592990027444,"user_tz":-330,"elapsed":867,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"0728914f-0d97-47c6-9dfa-475fbfb2cf81"},"source":["list(descriptions.keys())[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1000268201_693b08cb0e',\n"," '1001773457_577c3a7d70',\n"," '1002674143_1b742ab4b8',\n"," '1003163366_44323f5815',\n"," '1007129816_e794419615']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"2d-1bS38rAQ4","colab_type":"text"},"source":["### View sample descriptions."]},{"cell_type":"code","metadata":{"id":"1hs3xkfQaACS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1592990029049,"user_tz":-330,"elapsed":768,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"9b6592b7-ef42-445f-ac5b-d92b22f69544"},"source":["descriptions['1000268201_693b08cb0e']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['A child in a pink dress is climbing up a set of stairs in an entry way .',\n"," 'A girl going into a wooden building .',\n"," 'A little girl climbing into a wooden playhouse .',\n"," 'A little girl climbing the stairs to her playhouse .',\n"," 'A little girl in a pink dress going into a wooden cabin .']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"THRVlD4aaACa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1592990032213,"user_tz":-330,"elapsed":1467,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"d688e69e-e45d-4a29-be6f-71e5cbe8a6ff"},"source":["descriptions['1001773457_577c3a7d70']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['A black dog and a spotted dog are fighting',\n"," 'A black dog and a tri-colored dog playing with each other on the road .',\n"," 'A black dog and a white dog with brown spots are staring at each other in the street .',\n"," 'Two dogs of different breeds looking at each other on the road .',\n"," 'Two dogs on pavement moving toward each other .']"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"cyOPvXgarN__","colab_type":"text"},"source":["# Clean descriptions."]},{"cell_type":"code","metadata":{"id":"FFr1mMzzaACg","colab_type":"code","colab":{}},"source":["import string\n","\n","def clean_descriptions(descriptions):\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor key, desc_list in descriptions.items():\n","\t\tfor i in range(len(desc_list)):\n","\t\t\tdesc = desc_list[i]\n","\t\t\t# tokenize\n","\t\t\tdesc = desc.split()\n","\t\t\t# convert to lower case\n","\t\t\tdesc = [word.lower() for word in desc]\n","\t\t\t# remove punctuation from each token\n","\t\t\tdesc = [w.translate(table) for w in desc]\n","\t\t\t# remove hanging 's' and 'a'\n","\t\t\tdesc = [word for word in desc if len(word)>1]\n","\t\t\t# remove tokens with numbers in them\n","\t\t\tdesc = [word for word in desc if word.isalpha()]\n","\t\t\t# store as string\n","\t\t\tdesc_list[i] =  ' '.join(desc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x7-V7-_grm4U","colab_type":"text"},"source":["### Clean descriptions."]},{"cell_type":"code","metadata":{"id":"IaufeZKDrnRi","colab_type":"code","colab":{}},"source":["clean_descriptions(descriptions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pzaZlsV2sAio","colab_type":"text"},"source":["### View cleaned sample descriptions."]},{"cell_type":"code","metadata":{"id":"3NFGdzKhaACr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1592990040558,"user_tz":-330,"elapsed":1916,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"e4667dc2-17aa-4455-8325-2162938b1951"},"source":["descriptions['1000268201_693b08cb0e']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['child in pink dress is climbing up set of stairs in an entry way',\n"," 'girl going into wooden building',\n"," 'little girl climbing into wooden playhouse',\n"," 'little girl climbing the stairs to her playhouse',\n"," 'little girl in pink dress going into wooden cabin']"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"ZibUDcc4aACw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1592990040561,"user_tz":-330,"elapsed":845,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"9a464f64-2ae9-41cd-e9bb-7940c4dee5f1"},"source":["descriptions['1001773457_577c3a7d70']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['black dog and spotted dog are fighting',\n"," 'black dog and tricolored dog playing with each other on the road',\n"," 'black dog and white dog with brown spots are staring at each other in the street',\n"," 'two dogs of different breeds looking at each other on the road',\n"," 'two dogs on pavement moving toward each other']"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"2WoD1A8Ut3ty","colab_type":"text"},"source":["# Create vocabulary of words."]},{"cell_type":"code","metadata":{"id":"-MRrtIFFaAC3","colab_type":"code","colab":{}},"source":["def create_vocabulary(descriptions):\n","\tall_descriptions = set() \n","\tfor key in descriptions.keys():\n","\t\t[all_descriptions.update(current_descriptions.split()) for current_descriptions in descriptions[key]]\n","\treturn( all_descriptions )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bOLkLmz0uSSj","colab_type":"text"},"source":["### Create vocabulary of words."]},{"cell_type":"code","metadata":{"id":"mMyWSp9auQx1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592990046386,"user_tz":-330,"elapsed":1096,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"64d59ca2-cf36-43a7-a51e-9ff4b806e788"},"source":["vocabulary = create_vocabulary(descriptions)\n","print('vcabulary size -', len(vocabulary))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vcabulary size - 8763\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Kosa_BzPviem","colab_type":"text"},"source":["# Save descriptions."]},{"cell_type":"code","metadata":{"id":"f0wtiQUVaAC9","colab_type":"code","colab":{}},"source":["def save_descriptions(descriptions, filename):\n","\tlines = list()\n","\tfor key, description_list in descriptions.items():\n","\t\tfor description in description_list:\n","\t\t\tlines.append(key + ' ' + description)\n","\t\n","\ttext_data = '\\n'.join(lines)\n","\ttext_file = open(filename, 'w')\n","\ttext_file.write(text_data)\n","\ttext_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K7w95kN5w7QL","colab_type":"text"},"source":["### Save descriptions."]},{"cell_type":"code","metadata":{"id":"JPJvozdHwEPW","colab_type":"code","colab":{}},"source":["save_descriptions(descriptions, processed_descriptions_filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t6oaIkdXw1GW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1592990056518,"user_tz":-330,"elapsed":5310,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"15c5814f-9821-46df-f061-e89dd920eae7"},"source":["!ls -al 'Inception-v3'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 13192\n","drwxr-xr-x 3 root root    4096 Jun 24 05:17 .\n","drwxr-xr-x 1 root root    4096 Jun 24 05:04 ..\n","-rw-r--r-- 1 root root 2918552 Oct 14  2013 CrowdFlowerAnnotations.txt\n","-rw-r--r-- 1 root root 2943284 Jun 24 09:14 descriptions.txt\n","-rw-r--r-- 1 root root   25801 Oct 10  2013 dev_images.txt\n","-rw-r--r-- 1 root root  346674 Oct 14  2013 ExpertAnnotations.txt\n","drwxr-xr-x 2 root root  425984 Oct  3  2012 images\n","-rw-r--r-- 1 root root 3244761 Feb 16  2012 lemma.token.txt\n","-rw-r--r-- 1 root root    1821 Oct 14  2013 readme.txt\n","-rw-r--r-- 1 root root   25775 Oct 10  2013 test_images.txt\n","-rw-r--r-- 1 root root 3395237 Oct 14  2013 token.txt\n","-rw-r--r-- 1 root root  154678 Oct 10  2013 train_images.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4K6fLzoxxudU","colab_type":"text"},"source":["# Load dataset splits."]},{"cell_type":"code","metadata":{"id":"WnFouHIaaADC","colab_type":"code","colab":{}},"source":["def load_dataset(filename):\n","\ttext_data = load_document(filename)\n"," \n","\tdataset = list()\n","\tfor line in text_data.split('\\n'):\n","\n","\t\tif len(line) < 1:\n","\t\t\tcontinue\n","\n","\t\tidentifier = line.split('.')[0]\n","\t\tdataset.append(identifier)\n","\t\n","\treturn set(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8s3Nt6_Ex1M0","colab_type":"text"},"source":["### Load training dataset split."]},{"cell_type":"code","metadata":{"id":"7ve0B6MwyBlZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592990059708,"user_tz":-330,"elapsed":1313,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"26b42f70-f853-4b6d-d5a2-b2fd27f5b07c"},"source":["train_dataset = load_dataset(train_dataset_filename)\n","print('number of train dataset samples -', len(train_dataset))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["number of train dataset samples - 6000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Urgz3L401TRm","colab_type":"text"},"source":["# Create a list of all image filenames in the directory."]},{"cell_type":"code","metadata":{"id":"JsZiplwg3rte","colab_type":"code","colab":{}},"source":["import glob"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FexxS3OuaADR","colab_type":"code","colab":{}},"source":["image_filenames = glob.glob(dataset_images_dir + '*.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iChaNPxsnGr4","colab_type":"text"},"source":["### View sample image filenames."]},{"cell_type":"code","metadata":{"id":"Kfd0G1iWm5rr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592990091578,"user_tz":-330,"elapsed":1290,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"718b8d92-5fa0-4506-cb46-2f4949e3ed0a"},"source":["print(image_filenames[:5])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Flickr8K/images/3541915243_956c1aa8ef.jpg', 'Flickr8K/images/3331900249_5872e90b25.jpg', 'Flickr8K/images/2245618207_fa486ba2b7.jpg', 'Flickr8K/images/431410325_f4916b5460.jpg', 'Flickr8K/images/1067790824_f3cc97239b.jpg']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UjAaXCfp2dNb","colab_type":"text"},"source":["### Read the train image filenames."]},{"cell_type":"code","metadata":{"id":"PfRiTwBbaADV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592990224531,"user_tz":-330,"elapsed":2354,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"7de72ab9-e130-4da3-e408-02309e786ad9"},"source":["train_image_filenames = set(open(train_dataset_filename, 'r').read().strip().split('\\n'))\n","\n","train_images = []\n","for image_filename in image_filenames: \n","    if image_filename[len(dataset_images_dir):] in train_image_filenames: \n","        train_images.append(image_filename)\n","print('number of training samples -',len(train_images))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["number of training samples - 6000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lewaDbjznUeK","colab_type":"text"},"source":["### View sample train image filenames."]},{"cell_type":"code","metadata":{"id":"91912F5jnVHZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592990229146,"user_tz":-330,"elapsed":1233,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"a4f5a5ed-e77d-49ac-ba06-3c153d55ed5a"},"source":["print(train_images[:5])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Flickr8K/images/3541915243_956c1aa8ef.jpg', 'Flickr8K/images/3331900249_5872e90b25.jpg', 'Flickr8K/images/2245618207_fa486ba2b7.jpg', 'Flickr8K/images/431410325_f4916b5460.jpg', 'Flickr8K/images/1067790824_f3cc97239b.jpg']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8pE0Dbfg4bZ_","colab_type":"text"},"source":["### Read the test image filenames."]},{"cell_type":"code","metadata":{"id":"uL6QaXoXaADf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592990266085,"user_tz":-330,"elapsed":1377,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"5e238e94-9328-4508-884f-c3fc35ca2fda"},"source":["test_image_filenames = set(open(test_dataset_filename, 'r').read().strip().split('\\n'))\n","\n","test_images = []\n","for image_filename in image_filenames: \n","    if image_filename[len(dataset_images_dir):] in test_image_filenames: \n","        test_images.append(image_filename)\n","print('number of test samples -',len(test_images))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["number of test samples - 1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HdEOEmFAnhCM","colab_type":"text"},"source":["### View sample test image filenames."]},{"cell_type":"code","metadata":{"id":"irtzM_J1nksp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592990269923,"user_tz":-330,"elapsed":1972,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"6eac3221-5f2a-4137-8da2-16b42453524a"},"source":["print(test_images[:5])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Flickr8K/images/3364026240_645d533fda.jpg', 'Flickr8K/images/3437147889_4cf26dd525.jpg', 'Flickr8K/images/3254817653_632e840423.jpg', 'Flickr8K/images/2215136723_960edfea49.jpg', 'Flickr8K/images/3393343330_b13df4d8ec.jpg']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7qAF_41kOoGz","colab_type":"text"},"source":["# Load cleaned descriptions."]},{"cell_type":"code","metadata":{"id":"MALJn7cZaADs","colab_type":"code","colab":{}},"source":["def load_cleaned_descriptions(filename, dataset):\n","\ttext_data = load_document(filename)\n"," \n","\tdescriptions = dict()\n","\tfor line in text_data.split('\\n'):\n","\t\ttokens = line.split()\n","\n","\t\timage_id, image_description = tokens[0], tokens[1:]\n","\n","\t\tif image_id in dataset:\n","\t\t\tif image_id not in descriptions:\n","\t\t\t\tdescriptions[image_id] = list()\n","\n","\t\t\tcurrent_description = 'startseq ' + ' '.join(image_description) + ' endseq'\n","\t\t\tdescriptions[image_id].append(current_description)\n","\t \n","\treturn( descriptions )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLfYrMpKO4Or","colab_type":"text"},"source":["### Load cleaned decsriptions for train dataset split."]},{"cell_type":"code","metadata":{"id":"LCiR3Md_O3ZJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592990280851,"user_tz":-330,"elapsed":1215,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"c6e5b781-0099-4dd4-a352-15aa87ad0d1e"},"source":["training_descriptions = load_cleaned_descriptions(processed_descriptions_filename, train_dataset)\n","print('number of training descriptions -' , len(training_descriptions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["number of training descriptions - 6000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5eEpkM0yQnRT","colab_type":"text"},"source":["# Preprocess an input image."]},{"cell_type":"code","metadata":{"id":"iytHve1uQqPH","colab_type":"code","colab":{}},"source":["from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.applications.inception_v3 import preprocess_input\n","from tensorflow.keras.preprocessing import image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0blZBZaiaADy","colab_type":"code","colab":{}},"source":["def preprocess(image_path):\n","    input_image = image.load_img(image_path, target_size=(299, 299))    \n","    input_image = image.img_to_array(input_image)\n","\n","    input_image = np.expand_dims(input_image, axis=0)\n","    input_image = preprocess_input(input_image)\n","    return( input_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"edB-UbIqRyLT","colab_type":"text"},"source":["# Create feature extractor model."]},{"cell_type":"markdown","metadata":{"id":"oSQGhyXyR5g1","colab_type":"text"},"source":["### Load Inception-v3 model."]},{"cell_type":"code","metadata":{"id":"5zic_z4TaAD5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1592990314728,"user_tz":-330,"elapsed":11878,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"e38c6bea-5263-4689-b436-9ac60795a311"},"source":["base_model = InceptionV3(weights='imagenet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5d9wSTUZSDv4","colab_type":"text"},"source":["### Create feature extractor model."]},{"cell_type":"code","metadata":{"id":"PH4oVvhZaAED","colab_type":"code","colab":{}},"source":["feature_extractor = models.Model(base_model.input, base_model.layers[-2].output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vi6D8V_gSyLj","colab_type":"text"},"source":["# Encode input images."]},{"cell_type":"markdown","metadata":{"id":"5I1p8YVHS1zR","colab_type":"text"},"source":["### Create function to encode input images."]},{"cell_type":"code","metadata":{"id":"9SS4YlelaAEK","colab_type":"code","colab":{}},"source":["def encode_image(image_filename):\n","    input_image = preprocess(image_filename)\n","    image_features = feature_extractor.predict(input_image)\n","    image_features = np.reshape(image_features, image_features.shape[1])\n","    return( image_features )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QCkFTTDWo3CB","colab_type":"text"},"source":["### Create function to encode dataset."]},{"cell_type":"code","metadata":{"id":"XCa1gpC2o8We","colab_type":"code","colab":{}},"source":["from time import time\n","\n","def encode_images(input_images):\n","  start = time()\n","  feature_dictionary = {}\n","  for input_image in input_images:\n","    #print(input_image)\n","    feature_dictionary[input_image[len(dataset_images_dir):]] = encode_image(input_image)\n","\n","  print(\"time taken in seconds -\", ( time()-start ))\n","\n","  return(feature_dictionary)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9n6ZDgonqOwL","colab_type":"text"},"source":["### Encode train dataset."]},{"cell_type":"code","metadata":{"id":"zteQmNc3rJq_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592992850754,"user_tz":-330,"elapsed":1569,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}}},"source":["import pickle"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ISmW4kfhr_1Q","colab_type":"text"},"source":["### Encode train images and store features in a file."]},{"cell_type":"code","metadata":{"id":"52tU6oFXpPmO","colab_type":"code","colab":{}},"source":["train_features = encode_images(train_images)\n","with open(train_features_filename, \"wb\") as pickle_file:\n","    pickle.dump(train_features, pickle_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7tTJpKgtr3ih","colab_type":"text"},"source":["### Encode test images and store features in a file."]},{"cell_type":"code","metadata":{"id":"ZwMVFLRgrYro","colab_type":"code","colab":{}},"source":["test_features = encode_images(test_images)\n","with open(test_features_filename, \"wb\") as pickle_file:\n","    pickle.dump(test_features, pickle_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wP_1qhYHs3Rq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1592993399578,"user_tz":-330,"elapsed":8633,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"b78f0197-c3fe-4eab-df76-9a85fed6c966"},"source":["!ls -al Inception-v3"],"execution_count":54,"outputs":[{"output_type":"stream","text":["total 1087756\n","drwxr-xr-x 1 root root       4096 Jun 24 10:09 .\n","drwxr-xr-x 1 root root       4096 Jun 24 05:01 ..\n","drwxr-xr-x 1 root root       4096 Jun 19 16:15 .config\n","drwx------ 4 root root       4096 Jun 24 05:03 drive\n","drwxr-xr-x 3 root root       4096 Jun 24 10:07 Flickr8K\n","-rw-r--r-- 1 root root 1113826626 Jun 24 05:03 Flickr8K.tar.gz\n","drwxr-xr-x 2 root root       4096 Jun 24 10:07 Inception-v3\n","drwxr-xr-x 1 root root       4096 Jun 17 16:18 sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uMxsm13htUUS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592993309963,"user_tz":-330,"elapsed":6598,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}}},"source":["#!tar -czf Inception-v3.tar.gz Inception-v3\n","#!mv Inception-v3.tar.gz '/content/drive/My Drive/datasets/Flickr8K/.'"],"execution_count":52,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qVkdyNVF0vb7","colab_type":"text"},"source":["# OR"]},{"cell_type":"markdown","metadata":{"id":"9sDPAZr90xy2","colab_type":"text"},"source":["### Download image descriptions and Inception-v3 encoded image features from google drive."]},{"cell_type":"code","metadata":{"id":"Y8J02plY1JbM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1592993992142,"user_tz":-330,"elapsed":9718,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"c70efb7e-c4c0-4def-8cc0-a9ea6c255675"},"source":["!gdown --id 1SaRwMBTcdwZfDSGlvURnPfAEsbJj5o6T\n","!ls -al"],"execution_count":57,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1SaRwMBTcdwZfDSGlvURnPfAEsbJj5o6T\n","To: /content/Inception-v3.tar.gz\n","53.0MB [00:00, 167MB/s]\n","total 1139472\n","drwxr-xr-x 1 root root       4096 Jun 24 10:19 .\n","drwxr-xr-x 1 root root       4096 Jun 24 05:01 ..\n","drwxr-xr-x 1 root root       4096 Jun 19 16:15 .config\n","drwx------ 4 root root       4096 Jun 24 05:03 drive\n","drwxr-xr-x 3 root root       4096 Jun 24 10:07 Flickr8K\n","-rw-r--r-- 1 root root 1113826626 Jun 24 05:03 Flickr8K.tar.gz\n","drwxr-xr-x 2 root root       4096 Jun 24 10:07 Inception-v3-OLD\n","-rw-r--r-- 1 root root   52954905 Jun 24 10:19 Inception-v3.tar.gz\n","drwxr-xr-x 1 root root       4096 Jun 17 16:18 sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m_j6IDS41pGK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1592994001969,"user_tz":-330,"elapsed":6848,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"426ce9a5-75f8-4cb6-984c-b8a4d6b5296e"},"source":["!tar -xzf Inception-v3.tar.gz\n","!ls -al"],"execution_count":58,"outputs":[{"output_type":"stream","text":["total 1139476\n","drwxr-xr-x 1 root root       4096 Jun 24 10:19 .\n","drwxr-xr-x 1 root root       4096 Jun 24 05:01 ..\n","drwxr-xr-x 1 root root       4096 Jun 19 16:15 .config\n","drwx------ 4 root root       4096 Jun 24 05:03 drive\n","drwxr-xr-x 3 root root       4096 Jun 24 10:07 Flickr8K\n","-rw-r--r-- 1 root root 1113826626 Jun 24 05:03 Flickr8K.tar.gz\n","drwxr-xr-x 2 root root       4096 Jun 24 10:07 Inception-v3\n","drwxr-xr-x 2 root root       4096 Jun 24 10:07 Inception-v3-OLD\n","-rw-r--r-- 1 root root   52954905 Jun 24 10:19 Inception-v3.tar.gz\n","drwxr-xr-x 1 root root       4096 Jun 17 16:18 sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i2V07qfh1ugZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1592994097595,"user_tz":-330,"elapsed":9823,"user":{"displayName":"Keras TensorFlow","photoUrl":"","userId":"04878055958636665837"}},"outputId":"5c5ad193-5d07-4150-d477-b2f8d964c2b2"},"source":["!rm Inception-v3.tar.gz\n","!ls -al\n","!ls -al Inception-v3"],"execution_count":61,"outputs":[{"output_type":"stream","text":["rm: cannot remove 'Inception-v3.tar.gz': No such file or directory\n","total 1087760\n","drwxr-xr-x 1 root root       4096 Jun 24 10:20 .\n","drwxr-xr-x 1 root root       4096 Jun 24 05:01 ..\n","drwxr-xr-x 1 root root       4096 Jun 19 16:15 .config\n","drwx------ 4 root root       4096 Jun 24 05:03 drive\n","drwxr-xr-x 3 root root       4096 Jun 24 10:07 Flickr8K\n","-rw-r--r-- 1 root root 1113826626 Jun 24 05:03 Flickr8K.tar.gz\n","drwxr-xr-x 2 root root       4096 Jun 24 10:07 Inception-v3\n","drwxr-xr-x 2 root root       4096 Jun 24 10:07 Inception-v3-OLD\n","drwxr-xr-x 1 root root       4096 Jun 17 16:18 sample_data\n","total 59520\n","drwxr-xr-x 2 root root     4096 Jun 24 10:07 .\n","drwxr-xr-x 1 root root     4096 Jun 24 10:20 ..\n","-rw-r--r-- 1 root root  2943284 Jun 24 09:14 descriptions.txt\n","-rw-r--r-- 1 root root  8284163 Jun 24 10:06 test_features.pkl\n","-rw-r--r-- 1 root root 49708076 Jun 24 10:02 train_features.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-FhTk2WYaAEg","colab_type":"code","colab":{}},"source":["train_features = load(open(\"../../storage/image_caption/dataset/Pickle/encoded_train_images.pkl\", \"rb\"))\n","print('Photos: train=%d' % len(train_features))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BvFgj29EaABu","colab_type":"code","colab":{}},"source":["from numpy import array\n","import pandas as pd\n","from PIL import Image\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n","                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n","from keras.optimizers import Adam, RMSprop\n","from keras.layers.wrappers import Bidirectional\n","from keras.layers.merge import add\n","\n","from keras import Input, layers\n","from keras import optimizers\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6KoCf43HaAEq","colab_type":"code","colab":{}},"source":["# Create a list of all the training captions\n","all_train_captions = []\n","for key, val in train_descriptions.items():\n","    for cap in val:\n","        all_train_captions.append(cap)\n","len(all_train_captions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n62oxJE3aAEx","colab_type":"code","colab":{}},"source":["# Consider only words which occur at least 10 times in the corpus\n","word_count_threshold = 10\n","word_counts = {}\n","nsents = 0\n","for sent in all_train_captions:\n","    nsents += 1\n","    for w in sent.split(' '):\n","        word_counts[w] = word_counts.get(w, 0) + 1\n","\n","vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n","print('preprocessed words %d -> %d' % (len(word_counts), len(vocab)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nZEjYITraAE6","colab_type":"code","colab":{}},"source":["ixtoword = {}\n","wordtoix = {}\n","\n","ix = 1\n","for w in vocab:\n","    wordtoix[w] = ix\n","    ixtoword[ix] = w\n","    ix += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U8wRBweyaAE-","colab_type":"code","colab":{}},"source":["vocab_size = len(ixtoword) + 1 # one for appended 0's\n","vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lNYPVKZiaAFI","colab_type":"code","colab":{}},"source":["# convert a dictionary of clean descriptions to a list of descriptions\n","def to_lines(descriptions):\n","\tall_desc = list()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.append(d) for d in descriptions[key]]\n","\treturn all_desc\n","\n","# calculate the length of the description with the most words\n","def max_length(descriptions):\n","\tlines = to_lines(descriptions)\n","\treturn max(len(d.split()) for d in lines)\n","\n","# determine the maximum sequence length\n","max_length = max_length(train_descriptions)\n","print('Description Length: %d' % max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6aH3mV8zaAFU","colab_type":"code","colab":{}},"source":["# data generator, intended to be used in a call to model.fit_generator()\n","def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n","    X1, X2, y = list(), list(), list()\n","    n=0\n","    # loop for ever over images\n","    while 1:\n","        for key, desc_list in descriptions.items():\n","            n+=1\n","            # retrieve the photo feature\n","            photo = photos[key+'.jpg']\n","            for desc in desc_list:\n","                # encode the sequence\n","                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n","                # split one sequence into multiple X, y pairs\n","                for i in range(1, len(seq)):\n","                    # split into input and output pair\n","                    in_seq, out_seq = seq[:i], seq[i]\n","                    # pad input sequence\n","                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","                    # encode output sequence\n","                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","                    # store\n","                    X1.append(photo)\n","                    X2.append(in_seq)\n","                    y.append(out_seq)\n","            # yield the batch data\n","            if n==num_photos_per_batch:\n","                yield [[array(X1), array(X2)], array(y)]\n","                X1, X2, y = list(), list(), list()\n","                n=0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMQsiGu_aAFY","colab_type":"code","colab":{}},"source":["# Load Glove vectors\n","glove_dir = '../../storage/glove'\n","embeddings_index = {} # empty dictionary\n","f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n","\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","print('Found %s word vectors.' % len(embeddings_index))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Py9IH6iMaAFc","colab_type":"code","colab":{}},"source":["embedding_dim = 200\n","\n","# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","for word, i in wordtoix.items():\n","    #if i < max_words:\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # Words not found in the embedding index will be all zeros\n","        embedding_matrix[i] = embedding_vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLfvgbxyaAFf","colab_type":"code","colab":{}},"source":["embedding_matrix.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJvLeCGmaAFi","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XPTaw8XYaAFs","colab_type":"code","colab":{}},"source":["inputs1 = Input(shape=(2048,))\n","fe1 = Dropout(0.5)(inputs1)\n","fe2 = Dense(256, activation='relu')(fe1)\n","inputs2 = Input(shape=(max_length,))\n","se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n","se2 = Dropout(0.5)(se1)\n","se3 = LSTM(256)(se2)\n","decoder1 = add([fe2, se3])\n","decoder2 = Dense(256, activation='relu')(decoder1)\n","outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","model = Model(inputs=[inputs1, inputs2], outputs=outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mgcywZ76aAF2","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewuMTOZxaAF-","colab_type":"code","colab":{}},"source":["model.layers[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHB5VX2caAGE","colab_type":"code","colab":{}},"source":["model.layers[2].set_weights([embedding_matrix])\n","model.layers[2].trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BbywH0wRaAGU","colab_type":"code","colab":{}},"source":["model.compile(loss='categorical_crossentropy', optimizer='adam')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2rkP65EeaAGa","colab_type":"code","colab":{}},"source":["epochs = 10\n","number_pics_per_bath = 3\n","steps = len(train_descriptions)//number_pics_per_bath"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AaEutnvraAGe","colab_type":"code","colab":{}},"source":["for i in range(epochs):\n","    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n","    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n","    model.save('./model_weights/model_' + str(i) + '.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FVkPCl2-aAGp","colab_type":"code","colab":{}},"source":["for i in range(epochs):\n","    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n","    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n","    model.save('./model_weights/model_' + str(i) + '.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bWOFwpqNaAGu","colab_type":"code","colab":{}},"source":["model.optimizer.lr = 0.0001\n","epochs = 10\n","number_pics_per_bath = 6\n","steps = len(train_descriptions)//number_pics_per_bath"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cS3TWfhQaAG-","colab_type":"code","colab":{}},"source":["for i in range(epochs):\n","    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n","    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n","    #model.save('./model_weights/model_' + str(i) + '.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q8vCIj39aAHJ","colab_type":"code","colab":{}},"source":["model.save_weights('./model_weights/model_30.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xXvGStpjaAHV","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UP-MrUhsaAHY","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iLtP6QNYaAHb","colab_type":"code","colab":{}},"source":["model.load_weights('./model_weights/model_30.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1pALYeGaaAHg","colab_type":"code","colab":{}},"source":["images = '../../storage/image_caption/dataset/Flicker8k_Dataset/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"doAkyBwEaAHi","colab_type":"code","colab":{}},"source":["with open(\"../../storage/image_caption/dataset/Pickle/encoded_test_images.pkl\", \"rb\") as encoded_pickle:\n","    encoding_test = load(encoded_pickle)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4wgkUedaAHl","colab_type":"code","colab":{}},"source":["def greedySearch(photo):\n","    in_text = 'startseq'\n","    for i in range(max_length):\n","        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n","        sequence = pad_sequences([sequence], maxlen=max_length)\n","        yhat = model.predict([photo,sequence], verbose=0)\n","        yhat = np.argmax(yhat)\n","        word = ixtoword[yhat]\n","        in_text += ' ' + word\n","        if word == 'endseq':\n","            break\n","    final = in_text.split()\n","    final = final[1:-1]\n","    final = ' '.join(final)\n","    return final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8jSw8gC4aAHv","colab_type":"code","colab":{}},"source":["z+=1\n","pic = list(encoding_test.keys())[z]\n","image = encoding_test[pic].reshape((1,2048))\n","x=plt.imread(images+pic)\n","plt.imshow(x)\n","plt.show()\n","print(\"Greedy:\",greedySearch(image))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LIuLP8M5aAH0","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}