{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AttGAN-Attributes.ipynb","provenance":[{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN/AttGAN-Attributes.ipynb","timestamp":1590712094179},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN/AttGAN-Attributes.ipynb","timestamp":1590651859474},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN/AttGAN-Attributes.ipynb","timestamp":1590281449691},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN/AttGAN-Attributes.ipynb","timestamp":1590218877594},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN/AttGAN-CelebA.ipynb","timestamp":1590148482943},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN/AttGAN-CelebA.ipynb","timestamp":1589877536521},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN/AttGAN-CelebA.ipynb","timestamp":1589783736864},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN/AttGAN-GP.ipynb","timestamp":1589708618751},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN/AttGAN-GP.ipynb","timestamp":1589258404891},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN.ipynb","timestamp":1589201385600},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN.ipynb","timestamp":1588851525549},{"file_id":"1jh3lx63SznwpbVoRYlVcw-gPm1LEliR_","timestamp":1580294178249}],"private_outputs":true,"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-oVvsCRzl64G","colab_type":"text"},"source":["# AttGAN"]},{"cell_type":"markdown","metadata":{"id":"s0Kbndo1OYXy","colab_type":"text"},"source":["# Abstract - IEEE Xplore\n","Facial attribute editing aims to manipulate single or multiple attributes on a given face image, i.e., to generate a new face image with desired attributes while preserving other details. Recently, the generative adversarial net (GAN) and encoder-decoder architecture are usually incorporated to handle this task with promising results. Based on the encoder-decoder architecture, facial attribute editing is achieved by decoding the latent representation of a given face conditioned on the desired attributes. Some existing methods attempt to establish an attribute-independent latent representation for further attribute editing. However, such attribute-independent constraint on the latent representation is excessive because it restricts the capacity of the latent representation and may result in information loss, leading to over-smooth or distorted generation. Instead of imposing constraints on the latent representation, in this work, we propose to apply an attribute classification constraint to the generated image to just guarantee the correct change of desired attributes, i.e., to change what you want. Meanwhile, the reconstruction learning is introduced to preserve attribute-excluding details, in other words, to only change what you want. Besides, the adversarial learning is employed for visually realistic editing. These three components cooperate with each other forming an effective framework for high quality facial attribute editing, referred as AttGAN. Furthermore, the proposed method is extended for attribute style manipulation in an unsupervised manner. Experiments on two wild datasets, CelebA and LFW, show that the proposed method outperforms the state-of-the-art on realistic attribute editing with other facial details well preserved."]},{"cell_type":"markdown","metadata":{"id":"p7Mhcdz5mfue","colab_type":"text"},"source":["# References\n","* [AttGAN: Facial Attribute Editing by Only Changing What You Want - IEEE Xplore ](https://ieeexplore.ieee.org/document/8718508)\n","* [AttGAN: Facial Attribute Editing by Only Changing What You Want - arXiv.org](https://arxiv.org/abs/1711.10678)\n","* [AttGAN-Tensorflow](https://github.com/LynnHo/AttGAN-Tensorflow)\n","* [AttGAN-PyTorch](https://github.com/elvisyjlin/AttGAN-PyTorch)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qpK3sP-HeMka","colab_type":"text"},"source":["# Prerequisite\n","* Align CelebA dataset images using [align_images](https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/CelebA/align_images.ipynb) script.\n","* Download the preprocessed CelebA dataset using this [link](https://drive.google.com/file/d/1diaLDdB-dNMsPhJX0uco4155ghi4KMXK/view?usp=sharing)."]},{"cell_type":"markdown","metadata":{"id":"4LXy2rlgkSCs","colab_type":"text"},"source":["# Import TensorFlow 2.x."]},{"cell_type":"code","metadata":{"id":"gpqilhyfkT7y","colab_type":"code","colab":{}},"source":["try:\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import tensorflow as tf\n","tf.random.set_seed(7)\n","\n","import tensorflow.keras.layers as layers\n","import tensorflow.keras.models as models\n","\n","import numpy as np\n","np.random.seed(7)\n","\n","import matplotlib.pyplot as plot\n","\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jaPsmNYtn2dm","colab_type":"text"},"source":["# Set the root directory."]},{"cell_type":"code","metadata":{"id":"AnnvJx4dn432","colab_type":"code","colab":{}},"source":["import os\n","\n","root_dir = '/content/'\n","os.chdir(root_dir)\n","\n","!ls -al"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FnMh0LYKlIfL","colab_type":"text"},"source":["# Download aligned CelebA dataset from goolge drive."]},{"cell_type":"markdown","metadata":{"id":"eEJf4zCnjhUw","colab_type":"text"},"source":["### Mount google drive."]},{"cell_type":"code","metadata":{"id":"DijemfFpjmT4","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xUgz0xmI1cOt","colab_type":"text"},"source":["### Copy aligned CelebA dataset from gdrive."]},{"cell_type":"code","metadata":{"id":"DbyviiD2hvsa","colab_type":"code","colab":{}},"source":["!ls -al '/content/drive/My Drive/CelebA/img_align_celeba.tar.gz'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J74njE9SkiOB","colab_type":"code","colab":{}},"source":["!cp '/content/drive/My Drive/CelebA/img_align_celeba.tar.gz' ."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5xMWanM2RXZ","colab_type":"code","colab":{}},"source":["!ls -al"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5xB8rwUc1q5m","colab_type":"text"},"source":["### OR "]},{"cell_type":"markdown","metadata":{"id":"F0BPV5hP1tKu","colab_type":"text"},"source":["### Copy aligned CelebA dataset using google drive shared link."]},{"cell_type":"code","metadata":{"id":"Ykc7JxtT11RV","colab_type":"code","colab":{}},"source":["!gdown --id 1z5bpHVrciXmE8obe8NeYa3u9zWvdLWwS"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AZzk154OVAU_","colab_type":"code","colab":{}},"source":["!ls -al"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WMjnMkSmSmOj","colab_type":"text"},"source":["### OR"]},{"cell_type":"code","metadata":{"id":"Q12FU6iaSofr","colab_type":"code","colab":{}},"source":["!gdown --id 1diaLDdB-dNMsPhJX0uco4155ghi4KMXK"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R1Oam6d3iIhi","colab_type":"code","colab":{}},"source":["!ls -al"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-dP4zUFgjuP3","colab_type":"text"},"source":["### Extract the dataset."]},{"cell_type":"code","metadata":{"id":"G34ZL8_ZiJ8I","colab_type":"code","colab":{}},"source":["!tar -xzf img_align_celeba.tar.gz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GXlcOrMwK_1K","colab_type":"code","colab":{}},"source":["#!mv img_align_celeba.tar.gz '/content/drive/My Drive/CelebA/.'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZPiKvgskO8A","colab_type":"code","colab":{}},"source":["!rm -rf img_align_celeba.tar.gz"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0fGc6B_vb7R9","colab_type":"text"},"source":["### Verify dataset contents."]},{"cell_type":"code","metadata":{"id":"Db5SUslpiOq7","colab_type":"code","colab":{}},"source":["!ls -al\n","!ls -al img_align_celeba\n","!ls -l img_align_celeba/images | wc -l"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ybvLePWpWGe","colab_type":"text"},"source":["# Create dictionaries for facial attributes.\n","* Attributes to identifiers\n","* Identifiers to attributes"]},{"cell_type":"code","metadata":{"id":"lypmtexHpSE6","colab_type":"code","colab":{}},"source":["attributes_to_identifiers = {\n","    '5_o_Clock_Shadow': 0, \n","    'Arched_Eyebrows': 1, \n","    'Attractive': 2,       \n","    'Bags_Under_Eyes': 3,           \n","    'Bald': 4, \n","    'Bangs': 5, \n","    'Big_Lips': 6,           \n","    'Big_Nose': 7, \n","    'Black_Hair': 8, \n","    'Blond_Hair': 9, \n","    'Blurry': 10,           \n","    'Brown_Hair': 11, \n","    'Bushy_Eyebrows': 12, \n","    'Chubby': 13,           \n","    'Double_Chin': 14, \n","    'Eyeglasses': 15, \n","    'Goatee': 16, \n","    'Gray_Hair': 17, \n","    'Heavy_Makeup': 18, \n","    'High_Cheekbones': 19,          \n","    'Male': 20, \n","    'Mouth_Slightly_Open': 21, \n","    'Mustache': 22, \n","    'Narrow_Eyes': 23, \n","    'No_Beard': 24, \n","    'Oval_Face': 25,           \n","    'Pale_Skin': 26, \n","    'Pointy_Nose': 27, \n","    'Receding_Hairline': 28,           \n","    'Rosy_Cheeks': 29, \n","    'Sideburns': 30, \n","    'Smiling': 31,           \n","    'Straight_Hair': 32, \n","    'Wavy_Hair': 33, \n","    'Wearing_Earrings': 34,           \n","    'Wearing_Hat': 35, \n","    'Wearing_Lipstick': 36,           \n","    'Wearing_Necklace': 37, \n","    'Wearing_Necktie': 38, \n","    'Young': 39\n","    }"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jnvG-L46l_H9","colab_type":"code","colab":{}},"source":["identifiers_to_attributes = {v: k for k, v in attributes_to_identifiers.items()}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tqiUJI5XlGrB","colab_type":"text"},"source":["# Prepare CelebA dataset in TensorFlow dataset format.\n"]},{"cell_type":"code","metadata":{"id":"ecspeHeVqruE","colab_type":"code","colab":{}},"source":["image_root_dir = 'img_align_celeba/images'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IeIR9wrUs4DP","colab_type":"code","colab":{}},"source":["train_label_filename = 'img_align_celeba/train_label.txt'\n","val_label_filename = 'img_align_celeba/val_label.txt'\n","test_label_filename = 'img_align_celeba/test_label.txt'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJ1hAr14QXs-","colab_type":"code","colab":{}},"source":["batch_size = 32"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EGXIEl5RqS4V","colab_type":"code","colab":{}},"source":["def create_celeba_dataset(image_root_dir, attribute_filename):\n","  image_names = np.genfromtxt(attribute_filename, dtype=str, usecols=0)\n","  image_filename_array = np.array([os.path.join(image_root_dir, image_name) for image_name in image_names])\n","\n","  attributes_array = np.genfromtxt(attribute_filename, dtype=float, usecols=range(1, 41))  \n","\n","  number_of_batches = len(image_filename_array) // batch_size  \n","\n","  memory_data = (image_filename_array, attributes_array)  \n","  dataset = tf.data.Dataset.from_tensor_slices(memory_data)\n","  \n","  return(dataset, number_of_batches)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"obFo7G9HsZG5","colab_type":"text"},"source":["# Preprocess the dataset."]},{"cell_type":"code","metadata":{"id":"7WkMuosARVFj","colab_type":"code","colab":{}},"source":["default_attribute_names = [\n"," 'Bald', \n"," 'Bangs', \n"," 'Black_Hair', \n"," 'Blond_Hair', \n"," 'Brown_Hair', \n"," 'Bushy_Eyebrows', \n"," 'Eyeglasses', \n"," 'Male', \n"," 'Mouth_Slightly_Open', \n"," 'Mustache', \n"," 'No_Beard', \n"," 'Pale_Skin', \n"," 'Young'\n"," ]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ufUSBdwRqp0","colab_type":"code","colab":{}},"source":["number_of_attributes = len(default_attribute_names)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FCNsbdVkVMM","colab_type":"code","colab":{}},"source":["image_load_shape = (143, 143, 3)\n","image_shape = (128, 128, 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vA-dXi37kQhf","colab_type":"code","colab":{}},"source":["buffer_size = 512"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E8FRZ2T3-59S","colab_type":"text"},"source":["### Load image using an filename."]},{"cell_type":"code","metadata":{"id":"uo12sLtv-6UO","colab_type":"code","colab":{}},"source":["def load_image(image_filename):\n","  input_image = tf.io.read_file(image_filename)\n","  input_image = tf.image.decode_jpeg(input_image, 3)\n","  return(input_image)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ZbZMMuQsk7x","colab_type":"text"},"source":["### Normalize image to [-1, 1]."]},{"cell_type":"code","metadata":{"id":"_ktprTPusjuz","colab_type":"code","colab":{}},"source":["def normalize_image(image):\n","  image = tf.cast(image, tf.float32)\n","  image = tf.clip_by_value(image, 0, 255) / 127.5 - 1\n","  return(image)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjY1eyBXsz0F","colab_type":"text"},"source":["### Random crop image."]},{"cell_type":"code","metadata":{"id":"zF4_Phj7s0J6","colab_type":"code","colab":{}},"source":["def random_crop(image):\n","  cropped_image = tf.image.random_crop(image, size=image_shape)\n","  return(cropped_image)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TtWjAS4XtisQ","colab_type":"text"},"source":["### Apply random jitter to input image."]},{"cell_type":"code","metadata":{"id":"YKO5hc_pti_S","colab_type":"code","colab":{}},"source":["def random_jitter(image):  \n","  image = tf.image.resize(image, [image_load_shape[0], image_load_shape[1]])  \n","  image = random_crop(image)\n","  image = tf.image.random_flip_left_right(image)\n","  return(image)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSN1M5XFuN5G","colab_type":"text"},"source":["### Define preprocessing for train dataset split."]},{"cell_type":"code","metadata":{"id":"72L03Hqex1Ge","colab_type":"code","colab":{}},"source":["def preprocess_attributes(attributes_array):\n","  selected_attributes = []\n","  for attribute_name in default_attribute_names:\n","    index = attributes_to_identifiers[attribute_name]\n","    selected_attributes.append(attributes_array[index])\n","\n","  selected_attributes = tf.convert_to_tensor(selected_attributes)\n","  selected_attributes = (selected_attributes + 1) // 2 \n","  selected_attributes = selected_attributes * 1.   \n","  return(selected_attributes)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8HOOpVQauOMn","colab_type":"code","colab":{}},"source":["def preprocess_train_dataset(image_filename, attributes):  \n","  \n","  image = load_image(image_filename)\n","  image = random_jitter(image)\n","  image = normalize_image(image)\n","\n","  attributes = preprocess_attributes(attributes)\n","  return(image, attributes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YU8coU3QuU24","colab_type":"text"},"source":["### Define preprocessing for test dataset split."]},{"cell_type":"code","metadata":{"id":"gpN0_XaXuVOp","colab_type":"code","colab":{}},"source":["def preprocess_test_dataset(image_filename, attributes):    \n","  \n","  image = load_image(image_filename)\n","  image = tf.image.resize(image, [image_shape[0], image_shape[1]]) \n","  image = normalize_image(image)\n","\n","  attributes = preprocess_attributes(attributes)\n","  return(image, attributes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_x_x37erxELV","colab_type":"text"},"source":["### Preprocess train dataset split."]},{"cell_type":"code","metadata":{"id":"I2r9-5qZx3Vz","colab_type":"code","colab":{}},"source":["auto_tune = tf.data.experimental.AUTOTUNE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFoWc3pNP3T6","colab_type":"code","colab":{}},"source":["train_dataset, number_of_batches = create_celeba_dataset(image_root_dir, train_label_filename)\n","print('number of batches -', number_of_batches)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkZ_hKW5xEl3","colab_type":"code","colab":{}},"source":["train_dataset = train_dataset.map(preprocess_train_dataset, num_parallel_calls=auto_tune)\n","train_dataset = train_dataset.shuffle(buffer_size)\n","train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n","train_dataset = train_dataset.prefetch(auto_tune)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6BMN0ODAs9a","colab_type":"text"},"source":["# Configuration parameters."]},{"cell_type":"code","metadata":{"id":"IasodU8I6ahW","colab_type":"code","colab":{}},"source":["d_gradient_penalty_weight = 10.0\n","d_attribute_loss_weight = 1.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MFOoZ6QAAtck","colab_type":"code","colab":{}},"source":["g_attribute_loss_weight = 10.0\n","g_reconstruction_loss_weight = 100.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hdykd1OKjkIx","colab_type":"code","colab":{}},"source":["load_previous_weights = False\n","save_current_weights = False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2LH4xT28snC","colab_type":"text"},"source":["# Create the optimizer.\n","\n","*   Adam optimizer\n","*   Learning rate = 0.0002\n","*   β1 = 0.5\n","*   β2 = 0.999"]},{"cell_type":"code","metadata":{"id":"ZX0koQkmQc_P","colab_type":"code","colab":{}},"source":["base_learning_rate = 0.0002"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hKY-zWNQQN7z","colab_type":"code","colab":{}},"source":["maximum_epochs = 60\n","start_decay_epoch = 30"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EfF5Nv0k90FH","colab_type":"code","colab":{}},"source":["def create_optimizer(base_learning_rate, maximum_epochs, start_decay_epoch, current_epoch):\n","  if (current_epoch >= start_decay_epoch):\n","     current_learning_rate = base_learning_rate * (1 - 1 / (maximum_epochs - start_decay_epoch + 1) * (current_epoch - start_decay_epoch + 1))\n","  else:\n","     current_learning_rate = base_learning_rate\n","\n","  print('epochs -', current_epoch, 'learning rate -', current_learning_rate)\n","\n","  optimizer = tf.optimizers.Adam(learning_rate=current_learning_rate, beta_1=0.5, beta_2=0.999)\n","  return(optimizer)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q4jt3WYtlO1r","colab_type":"text"},"source":["# Create an encoder model."]},{"cell_type":"code","metadata":{"id":"CfdYqi2k6BmK","colab_type":"code","colab":{}},"source":["from functools import partial"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rpVO3KDJU4RO","colab_type":"code","colab":{}},"source":["class Encoder(models.Model):\n","\n","  def __init__(self, encoder_dimension=64, downsamplings_layers=5, \n","               name='attgan-encoder', **kwargs):\n","    super(Encoder, self).__init__(name=name, **kwargs)\n","    \n","    self._encoder_dimension = encoder_dimension\n","    self._downsamplings_layers = downsamplings_layers\n","\n","    self._encoders = []    \n","    filters = self._encoder_dimension  \n","    for block_index in range(self._downsamplings_layers):\n","      block_name = 'block-' + str(block_index + 1)\n","\n","      current_encoder = self._convolution_block(filters, 4, name=block_name)\n","      self._encoders.append(current_encoder)\n","\n","      filters = filters * 2\n","\n","  def _convolution_block(self, filters, kernel_size, \n","                         activation_fn=tf.nn.leaky_relu, batch_norm=True, \n","                         input_shape=None, name=None):\n","    \n","    if( input_shape is None ):\n","      conv = partial(layers.Conv2D)\n","    else:\n","      conv = partial(layers.Conv2D, input_shape=input_shape)\n","\n","    blocks = [\n","      conv(filters, \n","           (kernel_size, kernel_size), \n","           strides = (2,2), \n","           padding=\"same\", \n","           kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev=0.02), \n","           use_bias = True, \n","           bias_initializer = tf.keras.initializers.Constant(value=0.0), \n","           name='conv')\n","        ]\n","\n","    if( batch_norm ):\n","      blocks.append(layers.BatchNormalization(name='bnorm'))\n","\n","    if(activation_fn is not None):\n","      if(activation_fn == tf.nn.leaky_relu):        \n","        blocks.append(layers.LeakyReLU(alpha=0.2, name='act'))\n","      else:\n","        blocks.append(layers.Activation(activation_fn, name='act'))\n","\n","    return(models.Sequential(blocks, name=name))\n","\n","  def call(self, input_image, training=True):\n","    image_features = []\n","\n","    layer_input = input_image\n","    for current_encoder in self._encoders:\n","      layer_input = current_encoder(layer_input, training=training)\n","      image_features.append(layer_input)\n","\n","    return(image_features)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XxZnWhtXyPGA","colab_type":"code","colab":{}},"source":["sample_images, sample_attributes = next(iter(train_dataset))\n","\n","encoder_model = Encoder(encoder_dimension=64, downsamplings_layers=5)\n","image_features = encoder_model.predict(sample_images)\n","print('number of image features -', len(image_features))\n","for index, image_feature in enumerate(image_features):\n","  print('image feature -', (index +1), image_feature.shape)\n","\n","encoder_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45xdnUJiFS56","colab_type":"text"},"source":["# Concatenate features and attributes."]},{"cell_type":"code","metadata":{"id":"Z_dLXktO6Hk5","colab_type":"code","colab":{}},"source":["def concatenate(list_of_features, list_of_attributes, layer_name):\n","  list_of_features = list(list_of_features) if isinstance(list_of_features, (list, tuple)) else [list_of_features]\n","  list_of_attributes = list(list_of_attributes) if isinstance(list_of_attributes, (list, tuple)) else [list_of_attributes]\n","  for index, attributes in enumerate(list_of_attributes):\n","        attributes = tf.reshape(attributes, [-1, 1, 1, attributes.shape[-1]], name=layer_name + 'reshape')\n","        attributes = tf.tile(attributes, [1, list_of_features[0].shape[1], list_of_features[0].shape[2], 1], name=layer_name + 'tile')\n","        list_of_attributes[index] = attributes\n","  return tf.concat(list_of_features + list_of_attributes, axis=-1, name=layer_name + 'concat')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_-pu_vcplanL","colab_type":"text"},"source":["# Create a decoder layer."]},{"cell_type":"code","metadata":{"id":"ycPCd3AiWmES","colab_type":"code","colab":{}},"source":["class Decoder(models.Model):\n","\n","  def __init__(self, decoder_dimension=64, upsamplings_layers=5, shortcut_layers=1, inject_layers=1,\n","               name='attgan-decoder', **kwargs):\n","    super(Decoder, self).__init__(name=name, **kwargs)\n","    \n","    self._decoder_dimension = decoder_dimension\n","    self._upsamplings_layers = upsamplings_layers\n","    self._shortcut_layers = shortcut_layers\n","    self._inject_layers = inject_layers\n","\n","    self._decoders = []\n","    filters = self._decoder_dimension  \n","    for block_index in range(self._upsamplings_layers - 1):\n","      block_name = 'block-' + str(block_index + 1)\n","\n","      current_decoder = self._convolution_block(filters, 4, name=block_name)\n","      self._decoders.append(current_decoder)\n","\n","      filters = filters * 2\n","\n","    current_decoder = self._convolution_block(3, 4, activation_fn=tf.nn.tanh, batch_norm=False, name='block-5')\n","    self._decoders.append(current_decoder)    \n","\n","  def _convolution_block(self, filters, kernel_size, \n","                         activation_fn=tf.nn.leaky_relu, batch_norm=True, \n","                         input_shape=None, name=''):\n","    \n","    if( input_shape is None ):\n","      dconv = partial(layers.Conv2DTranspose)\n","    else:\n","      dconv = partial(layers.Conv2DTranspose, input_shape=input_shape)\n","\n","    blocks = [\n","      dconv(filters, \n","            (kernel_size, kernel_size), \n","            strides = (2,2), \n","            padding = \"same\", \n","            kernel_initializer = tf.keras.initializers.RandomNormal(stddev=0.02),\n","            use_bias = True, \n","            bias_initializer = tf.keras.initializers.Constant(value=0.0),\n","            name = 'dconv')\n","        ]\n","\n","    if( batch_norm ):\n","      blocks.append(layers.BatchNormalization(name='bnorm'))\n","\n","    if(activation_fn is not None):\n","      if(activation_fn == tf.nn.leaky_relu):        \n","        blocks.append(layers.LeakyReLU(alpha=0.2, name='act'))\n","      else:\n","        blocks.append(layers.Activation(activation_fn, name='act'))\n","\n","    return(models.Sequential(blocks, name=name))\n","\n","  def call(self, inputs, training=True):\n","\n","    input_features, input_attributes = inputs    \n","\n","    layer_name = 'block-0-shortcut-'    \n","    layer_input = concatenate(input_features[-1], input_attributes, layer_name=layer_name)    \n","    for block_index in range(self._upsamplings_layers):\n","      layer_name = 'block-' + str(block_index + 1) + '-'\n","\n","      decoder_layer = self._decoders[block_index]\n","      layer_input = decoder_layer(layer_input, training)\n","\n","      if (self._shortcut_layers > block_index):\n","        shortcut_name = layer_name + 'shortcut-'\n","        layer_input = concatenate([layer_input, input_features[-2 - block_index]], [], layer_name=shortcut_name)\n","\n","      if (self._inject_layers > block_index):\n","        inject_name = layer_name + 'inject-'\n","        layer_input = concatenate(layer_input, input_attributes, layer_name=inject_name)      \n","\n","    return(layer_input)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TiKg7WWzlYr","colab_type":"code","colab":{}},"source":["sample_images, sample_attributes = next(iter(train_dataset))\n","\n","decoder_model = Decoder(decoder_dimension=64, upsamplings_layers=5, shortcut_layers=1, inject_layers=1)\n","\n","image_features = encoder_model.predict(sample_images)\n","generated_images = decoder_model.predict([image_features, sample_attributes])\n","decoder_model.summary()\n","print('generated images -', generated_images.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iT1NghHgl_Eh","colab_type":"text"},"source":["# Create a discriminator or classification model.\n"]},{"cell_type":"code","metadata":{"id":"P3kFTSm4IIP6","colab_type":"code","colab":{}},"source":["import tensorflow_addons as tfa"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"igGOMcsScbhI","colab_type":"code","colab":{}},"source":["class Discriminator(models.Model):\n","\n","  def __init__(self, number_of_attributes=40, \n","               discriminator_dimension=64, dense_dimension=1024, downsamplings_layers=5,\n","               name='attgan-discriminator', **kwargs):\n","    super(Discriminator, self).__init__(name=name, **kwargs)\n","    \n","    self._number_of_attributes = number_of_attributes\n","    self._discriminator_dimension = discriminator_dimension\n","    self._dense_dimension = dense_dimension\n","    self._downsamplings_layers = downsamplings_layers\n","    \n","    self._features = None\n","    self._classifier = None\n","    self._discriminator = None\n","\n","    self._create_features()\n","    self._create_classifier()\n","    self._create_discriminator()\n","\n","  def _create_features(self):\n","    self._features = models.Sequential(name=\"features\")\n","    filters = self._discriminator_dimension  \n","    for block_index in range(self._downsamplings_layers):\n","      block_name = 'block-' + str(block_index + 1)\n","\n","      current_features = self._convolution_block(filters, 4, name=block_name)\n","      self._features.add(current_features)\n","\n","      filters = filters * 2\n","\n","  def _create_classifier(self):\n","    self._classifier = models.Sequential(name='classifier')\n","    self._classifier.add(self._dense_block(self._dense_dimension, activation_fn=tf.nn.leaky_relu, name='dense'))\n","    self._classifier.add(self._dense_block(self._number_of_attributes, activation_fn=None, name='predictions'))\n","\n","  def _create_discriminator(self):\n","    self._discriminator = models.Sequential(name='discriminator')\n","    self._discriminator.add(self._dense_block(self._dense_dimension, activation_fn=tf.nn.leaky_relu, name='dense'))\n","    self._discriminator.add(self._dense_block(1, activation_fn=None, name='predictions'))\n","\n","\n","  def _convolution_block(self, filters, kernel_size, \n","                         activation_fn=tf.nn.leaky_relu, batch_norm=True, \n","                         input_shape=None, name=''):\n","    \n","    if( input_shape is None ):\n","      conv = partial(layers.Conv2D)\n","    else:\n","      conv = partial(layers.Conv2D, input_shape=input_shape)\n","\n","    blocks = [\n","      conv(filters, (kernel_size, kernel_size), \n","           strides = (2,2), \n","           padding = \"same\", \n","           kernel_initializer = tf.keras.initializers.RandomNormal(stddev=0.02), \n","           use_bias = True, \n","           bias_initializer = tf.keras.initializers.Constant(value=0.0),\n","           name = 'conv')\n","        ]\n","\n","    if( batch_norm ):\n","      blocks.append(tfa.layers.InstanceNormalization(name='inorm'))\n","\n","    if(activation_fn is not None):\n","      if(activation_fn == tf.nn.leaky_relu):        \n","        blocks.append(layers.LeakyReLU(alpha=0.2, name='act'))\n","      else:\n","        blocks.append(layers.Activation(activation_fn, name='act'))\n","\n","    return(models.Sequential(blocks, name=name))\n","\n","  def _dense_block(self, filters, \n","                         activation_fn=tf.nn.leaky_relu, batch_norm=False, \n","                         input_shape=None, name=None):\n","    \n","    if( input_shape is None ):\n","      dense = partial(layers.Dense)\n","    else:\n","      dense = partial(layers.Dense, input_shape=input_shape)\n","\n","    blocks = [\n","      dense(filters, \n","            kernel_initializer = tf.keras.initializers.RandomNormal(stddev=0.02), \n","            use_bias = True, \n","            bias_initializer = tf.keras.initializers.Constant(value=0.0), \n","            name = 'dense')\n","        ]\n","\n","    if( batch_norm ):\n","      blocks.append(tfa.layers.BatchNormalization(name='bnorm'))\n","\n","    if(activation_fn is not None):\n","      if(activation_fn == tf.nn.leaky_relu):        \n","        blocks.append(layers.LeakyReLU(alpha=0.2, name='act'))\n","      else:\n","        blocks.append(layers.Activation(activation_fn, name='act'))\n","\n","    return(models.Sequential(blocks, name=name))\n","\n","  def call(self, input_image, training=True):\n","\n","    layer_input = input_image   \n","    layer_input = self._features(layer_input, training=training)\n","    layer_input = layers.Flatten()(layer_input)\n","\n","    classifier_predictions = self._classifier(layer_input, training=training)\n","    discriminator_prediction = self._discriminator(layer_input, training=training)\n","    \n","    return(discriminator_prediction, classifier_predictions)     "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9XuvFcaTMVY","colab_type":"code","colab":{}},"source":["sample_images, sample_attributes = next(iter(train_dataset))\n","\n","discriminator_model = Discriminator(number_of_attributes, discriminator_dimension=64, dense_dimension=1024, downsamplings_layers=5)\n","discriminator_prediction, classifier_predictions = discriminator_model.predict(sample_images)\n","discriminator_model.summary()\n","print('discriminator prediction -', discriminator_prediction.shape)\n","print('classifier predictions -', classifier_predictions.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kd22Y_kzhNPf","colab_type":"text"},"source":["### Load previous model weights.\n","* Encoder model weights\n","* Decoder model weights\n","* Discriminator model weights\n","\n"]},{"cell_type":"code","metadata":{"id":"Qn9K_wgwBgJE","colab_type":"code","colab":{}},"source":["import os \n","\n","def encoder_filename():\n","  return('encoder.h5')\n","\n","def encoder_gdrive_filename(weight_root_dir='/content/drive/My Drive/models/AttGAN/'):    \n","  return(os.path.join(weight_root_dir, encoder_filename()))\n","\n","def decoder_filename():\n","  return('decoder.h5')\n","\n","def decoder_gdrive_filename(weight_root_dir='/content/drive/My Drive/models/AttGAN/'):    \n","  return(os.path.join(weight_root_dir, decoder_filename()))\n","\n","def discriminator_filename():\n","  return('discriminator.h5')\n","\n","def discriminator_gdrive_filename(weight_root_dir='/content/drive/My Drive/models/AttGAN/'):  \n","  return(os.path.join(weight_root_dir, discriminator_filename()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNv6MAX5hQFL","colab_type":"code","colab":{}},"source":["if(load_previous_weights):\n","  encoder_model.load_weights(encoder_gdrive_filename())\n","  decoder_model.load_weights(decoder_gdrive_filename())\n","  discriminator_model.load_weights(discriminator_gdrive_filename())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DmRb8Ok3-db5","colab_type":"text"},"source":["# Compute generator loss."]},{"cell_type":"code","metadata":{"id":"CfAvkKJj-dxp","colab_type":"code","colab":{}},"source":["def compute_generator_loss(input_image, input_attributes):\n","\n","  target_attributes = tf.random.shuffle(input_attributes)\n","\n","  scaled_input_attributes = input_attributes * 2. - 1.\n","  scaled_target_attributes = target_attributes * 2. - 1.\n","\n","  # Generator\n","  image_features = encoder_model(input_image, training=True)\n","\n","  reconstructed_image = decoder_model([image_features, scaled_input_attributes], training=True)\n","  fake_image = decoder_model([image_features, scaled_target_attributes], training=True)\n","\n","  # Discriminator\n","  fake_image_prediction, fake_image_attributes = discriminator_model(fake_image, training=False)\n","\n","  fake_image_prediction_loss = tf.reduce_mean(-fake_image_prediction)\n","  fake_image_attributes_loss = tf.compat.v1.losses.sigmoid_cross_entropy(target_attributes, fake_image_attributes)  \n","  \n","  image_reconstruction_loss = tf.compat.v1.losses.absolute_difference(input_image, reconstructed_image)\n","   \n","  generator_loss = (  fake_image_prediction_loss \n","                    + fake_image_attributes_loss * g_attribute_loss_weight \n","                    + image_reconstruction_loss * g_reconstruction_loss_weight\n","                    )  \n","  '''\n","  print('fake_prediction', fake_image_prediction_loss.numpy(),\n","        'fake_attributes', fake_image_attributes_loss.numpy(),\n","        'image_reconstruction', image_reconstruction_loss.numpy(),\n","        'generator_loss', generator_loss.numpy())\n","  '''\n","  \n","  return(generator_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfb8o9za6Blc","colab_type":"code","colab":{}},"source":["sample_images, sample_attributes = next(iter(train_dataset))\n","generator_loss = compute_generator_loss(sample_images, sample_attributes)\n","print(generator_loss.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HFdHJd1Gq11W","colab_type":"text"},"source":["# Compute discriminator loss."]},{"cell_type":"code","metadata":{"id":"SYtGK3BRq2P8","colab_type":"code","colab":{}},"source":["def compute_discriminator_loss(input_image, input_attributes):\n","\n","  target_attributes = tf.random.shuffle(input_attributes)\n","  scaled_target_attributes = target_attributes * 2. - 1.\n","\n","  # Generate\n","  image_features = encoder_model(input_image, training=False)\n","  fake_image = decoder_model([image_features, scaled_target_attributes], training=False)\n","\n","  # Discriminate\n","  real_image_prediction, real_image_attributes = discriminator_model(input_image, training=True)\n","  fake_image_prediction, fake_image_attributes = discriminator_model(fake_image, training=True)\n","\n","  # Discriminator losses\n","  real_image_gan_loss = tf.reduce_mean(-real_image_prediction)\n","  fake_image_gan_loss = tf.reduce_mean(fake_image_prediction)  \n","\n","  with tf.GradientTape() as gp_tape:\n","                alpha = tf.random.uniform([batch_size],0.,1.,dtype=tf.float32)\n","                alpha = tf.reshape(alpha,(-1,1,1,1))\n","                sample_images = input_image + alpha * (fake_image - input_image)\n","                #sample_images = tf.clip_by_value(sample_images, -1., 1.)\n","\n","                gp_tape.watch(sample_images)\n","                sample_predictions = discriminator_model(sample_images, training=False)\n","                sample_predictions = sample_predictions[0]\n","                \n","  gradients = gp_tape.gradient(sample_predictions, sample_images)                \n","  grad_l2 = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1,2,3]))\n","  gradient_penalty_value = tf.reduce_mean((grad_l2-1) ** 2)  \n","\n","  real_image_attributes_loss = tf.compat.v1.losses.sigmoid_cross_entropy(input_attributes, real_image_attributes)  \n","\n","  discriminator_loss = (  real_image_gan_loss \n","                        + fake_image_gan_loss \n","                        + gradient_penalty_value * d_gradient_penalty_weight \n","                        + real_image_attributes_loss * d_attribute_loss_weight                        \n","                        )  \n","  '''\n","  print('real_gan', real_image_gan_loss.numpy(),\n","        'fake_gan', fake_image_gan_loss.numpy(),\n","        'gp_value', gradient_penalty_value.numpy(),\n","        'real_attributes', real_image_attributes_loss.numpy(),\n","        'discriminator_loss', discriminator_loss.numpy())\n","  '''\n","  return(discriminator_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6o8t5PfsSBn","colab_type":"code","colab":{}},"source":["sample_images, sample_attributes = next(iter(train_dataset))\n","discriminator_loss = compute_discriminator_loss(sample_images, sample_attributes)\n","print(discriminator_loss.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hf1OgYwVm4fi","colab_type":"text"},"source":["# Train the model."]},{"cell_type":"code","metadata":{"id":"4NXxgonbX_o6","colab_type":"code","colab":{}},"source":["model_loss_frequency = 1000\n","model_save_frequency = 2000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d5X43kesJbGw","colab_type":"code","colab":{}},"source":["def save_models():\n","  if(save_current_weights):  \n","    encoder_model.save_weights(encoder_gdrive_filename())        \n","    decoder_model.save_weights(decoder_gdrive_filename())      \n","    discriminator_model.save_weights(discriminator_gdrive_filename())  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ZCPazylTSYK","colab_type":"code","colab":{}},"source":["image_filename = 'img_align_celeba/images/202520.jpg'\n","input_image = load_image(image_filename)\n","input_image = tf.image.resize(input_image, [image_shape[0], image_shape[1]], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR) \n","input_image = normalize_image(input_image)\n","input_image = tf.expand_dims(input_image, axis=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_69we1O76Kg","colab_type":"code","colab":{}},"source":["image_attributes =[-1., -1., -1., -1., 1., -1., 1., -1., 1., -1., 1., -1., 1.]\n","image_attributes = tf.expand_dims(image_attributes, axis=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1K-k1f0i7uJi","colab_type":"code","colab":{}},"source":["print('input image - ', input_image.shape)\n","print('image attributes -', image_attributes.shape)\n","plot.imshow(input_image[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6E63fz9hVADp","colab_type":"code","colab":{}},"source":["image_features = encoder_model.predict(input_image)\n","generated_images = decoder_model.predict([image_features, image_attributes])\n","plot.imshow(generated_images[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4lrRYJG_4mb","colab_type":"code","colab":{}},"source":["start_epoch = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJVAe5MrfCSN","colab_type":"code","colab":{}},"source":["def train(train_dataset, maximum_epochs, start_decay_epoch, start_epoch):  \n","\n","  g_optimizer = create_optimizer(base_learning_rate, maximum_epochs, start_decay_epoch, start_epoch)\n","  d_optimizer = create_optimizer(base_learning_rate, maximum_epochs, start_decay_epoch, start_epoch)\n","\n","  for current_epoch in range(start_epoch, 5): #maximum_epochs):\n","\n","    batch_index = 0    \n","    for dataset_batch in train_dataset:  \n","      batch_index = batch_index + 1    \n","\n","      images, attributes = dataset_batch\n","\n","      if(batch_index%6 == 0):\n","        with tf.GradientTape() as generator_tape:                      \n","          generator_loss = compute_generator_loss(images, attributes)      \n","          \n","        generator_gradients = generator_tape.gradient(generator_loss, [*encoder_model.trainable_variables, *decoder_model.trainable_variables])\n","        g_optimizer.apply_gradients(zip(generator_gradients, [*encoder_model.trainable_variables, *decoder_model.trainable_variables]))  \n","\n","      else:\n","        with tf.GradientTape() as discriminator_tape:              \n","          discriminator_loss = compute_discriminator_loss(images, attributes)  \n","\n","        discriminator_gradients = discriminator_tape.gradient(discriminator_loss, discriminator_model.trainable_variables)\n","        d_optimizer.apply_gradients(zip(discriminator_gradients, discriminator_model.trainable_variables))        \n","\n","      if(batch_index%model_loss_frequency == 0):\n","        print('epoch -', current_epoch, 'generator loss -', generator_loss.numpy(), 'discriminator loss -', discriminator_loss.numpy())\n","        image_features = encoder_model.predict(input_image)\n","        generated_images = decoder_model.predict([image_features, image_attributes])   \n","        generated_images = (generated_images + 1.) / 2. * 255.     \n","        generated_images = generated_images.astype(np.uint8)\n","        plot.imshow(generated_images[0])\n","        plot.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZWMK-qlUfUf-","colab_type":"code","colab":{}},"source":["train(train_dataset, maximum_epochs, start_decay_epoch, start_epoch)"],"execution_count":0,"outputs":[]}]}