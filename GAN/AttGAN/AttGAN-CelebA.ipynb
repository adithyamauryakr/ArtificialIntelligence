{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AttGAN-CelebA.ipynb","provenance":[{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN/AttGAN-CelebA.ipynb","timestamp":1589877536521},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN/AttGAN-CelebA.ipynb","timestamp":1589783736864},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN/AttGAN-GP.ipynb","timestamp":1589708618751},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN/AttGAN-GP.ipynb","timestamp":1589258404891},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN.ipynb","timestamp":1589201385600},{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN.ipynb","timestamp":1588851525549},{"file_id":"1jh3lx63SznwpbVoRYlVcw-gPm1LEliR_","timestamp":1580294178249}],"private_outputs":true,"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-oVvsCRzl64G","colab_type":"text"},"source":["# AttGAN"]},{"cell_type":"markdown","metadata":{"id":"p7Mhcdz5mfue","colab_type":"text"},"source":["# References\n","* [AttGAN: Facial Attribute Editing by Only Changing What You Want - IEEE Xplore ](https://ieeexplore.ieee.org/document/8718508)\n","* [AttGAN: Facial Attribute Editing by Only Changing What You Want - arXiv.org](https://arxiv.org/abs/1711.10678)\n","* [AttGAN-Tensorflow](https://github.com/LynnHo/AttGAN-Tensorflow)\n","* [AttGAN-PyTorch](https://github.com/elvisyjlin/AttGAN-PyTorch)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qpK3sP-HeMka","colab_type":"text"},"source":["# Prerequisite\n","* Align CelebA dataset images using [align_images](https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/CelebA/align_images.ipynb) script.\n","* Download the preprocessed CelebA dataset using this [link](https://drive.google.com/file/d/1diaLDdB-dNMsPhJX0uco4155ghi4KMXK/view?usp=sharing)."]},{"cell_type":"markdown","metadata":{"id":"pKa85vKe0hW9","colab_type":"text"},"source":["# Install gdown Python package."]},{"cell_type":"code","metadata":{"id":"nY0sLk470jTD","colab_type":"code","colab":{}},"source":["!pip install -U --no-cache-dir gdown"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwVm1Dt2kYIo","colab_type":"text"},"source":["# Install Tensorflow-addons.\n","* InstanceNormalization"]},{"cell_type":"code","metadata":{"id":"qojNeXFvkZP1","colab_type":"code","colab":{}},"source":["!pip install tensorflow-addons"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4LXy2rlgkSCs","colab_type":"text"},"source":["# Import TensorFlow 2.x."]},{"cell_type":"code","metadata":{"id":"gpqilhyfkT7y","colab_type":"code","colab":{}},"source":["try:\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import tensorflow as tf\n","import tensorflow.keras.layers as layers\n","import tensorflow.keras.models as models\n","\n","import numpy as np\n","np.random.seed(7)\n","\n","import matplotlib.pyplot as plot\n","\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jaPsmNYtn2dm","colab_type":"text"},"source":["# Set the root directory."]},{"cell_type":"code","metadata":{"id":"AnnvJx4dn432","colab_type":"code","colab":{}},"source":["import os\n","\n","root_dir = '/content/'\n","os.chdir(root_dir)\n","\n","!ls -al"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FnMh0LYKlIfL","colab_type":"text"},"source":["# Download the CelebA dataset from gdrive."]},{"cell_type":"markdown","metadata":{"id":"eEJf4zCnjhUw","colab_type":"text"},"source":["### Mount the gdrive."]},{"cell_type":"code","metadata":{"id":"DijemfFpjmT4","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ddPsw17CjlYP","colab_type":"text"},"source":["### Copy aligned CelebA dataset."]},{"cell_type":"code","metadata":{"id":"DbyviiD2hvsa","colab_type":"code","colab":{}},"source":["!ls -al '/content/drive/My Drive/CelebA/img_align_celeba.tar.gz'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J74njE9SkiOB","colab_type":"code","colab":{}},"source":["!cp '/content/drive/My Drive/CelebA/img_align_celeba.tar.gz' ."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R1Oam6d3iIhi","colab_type":"code","colab":{}},"source":["!ls -al"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-dP4zUFgjuP3","colab_type":"text"},"source":["### Extract the dataset."]},{"cell_type":"code","metadata":{"id":"G34ZL8_ZiJ8I","colab_type":"code","colab":{}},"source":["!tar -xzf img_align_celeba.tar.gz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZPiKvgskO8A","colab_type":"code","colab":{}},"source":["!rm -rf img_align_celeba.tar.gz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Db5SUslpiOq7","colab_type":"code","colab":{}},"source":["!ls -al\n","!ls -al img_align_celeba\n","!ls -l img_align_celeba/images | wc -l"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ybvLePWpWGe","colab_type":"text"},"source":["# Create dictionaries for facial attributes.\n","* Attributes to identifiers\n","* Identifiers to attributes"]},{"cell_type":"code","metadata":{"id":"lypmtexHpSE6","colab_type":"code","colab":{}},"source":["attributes_to_identifiers = {\n","    '5_o_Clock_Shadow': 0, \n","    'Arched_Eyebrows': 1, \n","    'Attractive': 2,       \n","    'Bags_Under_Eyes': 3,           \n","    'Bald': 4, \n","    'Bangs': 5, \n","    'Big_Lips': 6,           \n","    'Big_Nose': 7, \n","    'Black_Hair': 8, \n","    'Blond_Hair': 9, \n","    'Blurry': 10,           \n","    'Brown_Hair': 11, \n","    'Bushy_Eyebrows': 12, \n","    'Chubby': 13,           \n","    'Double_Chin': 14, \n","    'Eyeglasses': 15, \n","    'Goatee': 16, \n","    'Gray_Hair': 17, \n","    'Heavy_Makeup': 18, \n","    'High_Cheekbones': 19,          \n","    'Male': 20, \n","    'Mouth_Slightly_Open': 21, \n","    'Mustache': 22, \n","    'Narrow_Eyes': 23, \n","    'No_Beard': 24, \n","    'Oval_Face': 25,           \n","    'Pale_Skin': 26, \n","    'Pointy_Nose': 27, \n","    'Receding_Hairline': 28,           \n","    'Rosy_Cheeks': 29, \n","    'Sideburns': 30, \n","    'Smiling': 31,           \n","    'Straight_Hair': 32, \n","    'Wavy_Hair': 33, \n","    'Wearing_Earrings': 34,           \n","    'Wearing_Hat': 35, \n","    'Wearing_Lipstick': 36,           \n","    'Wearing_Necklace': 37, \n","    'Wearing_Necktie': 38, \n","    'Young': 39\n","    }"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jnvG-L46l_H9","colab_type":"code","colab":{}},"source":["identifiers_to_attributes = {v: k for k, v in attributes_to_identifiers.items()}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tqiUJI5XlGrB","colab_type":"text"},"source":["# Prepare CelebA dataset in TensorFlow dataset format."]},{"cell_type":"code","metadata":{"id":"ecspeHeVqruE","colab_type":"code","colab":{}},"source":["image_root_dir = 'img_align_celeba/images'\n","train_label_filename = 'img_align_celeba/train_label.txt'\n","val_label_filename = 'img_align_celeba/val_label.txt'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EGXIEl5RqS4V","colab_type":"code","colab":{}},"source":["def create_celeba_dataset(image_root_dir, attribute_filename):\n","  image_names = np.genfromtxt(attribute_filename, dtype=str, usecols=0)\n","  image_filename_array = np.array([os.path.join(image_root_dir, image_name) for image_name in image_names])\n","\n","  attributes_array = np.genfromtxt(attribute_filename, dtype=float, usecols=range(1, 41))    \n","\n","  memory_data = (image_filename_array, attributes_array)  \n","  dataset = tf.data.Dataset.from_tensor_slices(memory_data)\n","\n","  return(dataset)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GD53Y6uhq9zu","colab_type":"code","colab":{}},"source":["train_dataset = create_celeba_dataset(image_root_dir, train_label_filename)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"obFo7G9HsZG5","colab_type":"text"},"source":["# Preprocess the dataset."]},{"cell_type":"code","metadata":{"id":"0FCNsbdVkVMM","colab_type":"code","colab":{}},"source":["number_of_attributes = 40\n","image_load_shape = (143, 143, 3)\n","image_shape = (128, 128, 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vA-dXi37kQhf","colab_type":"code","colab":{}},"source":["buffer_size = 512\n","batch_size = 32"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Drxx1GeToHXq","colab_type":"text"},"source":["### Create test image."]},{"cell_type":"code","metadata":{"id":"liJ-_Q1-oJwc","colab_type":"code","colab":{}},"source":["def create_test_image(image_shape):\n","  test_image = np.random.rand(image_shape[0], image_shape[1], image_shape[2])\n","  return(test_image)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQjjubhXrCRx","colab_type":"text"},"source":["### Create test image batch."]},{"cell_type":"code","metadata":{"id":"QevokFc8rHDl","colab_type":"code","colab":{}},"source":["def create_test_image_batch(image_shape):\n","  test_image_batch = np.random.rand(batch_size, image_shape[0], image_shape[1], image_shape[2])\n","  return(test_image_batch)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1NCj0YBdnq7Z","colab_type":"text"},"source":["### Create test attributes."]},{"cell_type":"code","metadata":{"id":"PRACL0ownKV2","colab_type":"code","colab":{}},"source":["def create_test_attributes():\n","  test_attributes = np.random.rand(number_of_attributes)\n","  return(test_attributes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sROKRqGTr7Mr","colab_type":"text"},"source":["### Create test attribute batch."]},{"cell_type":"code","metadata":{"id":"fipUjXuEr7ja","colab_type":"code","colab":{}},"source":["def create_test_attribute_batch():\n","  test_attribute_batch = np.random.rand(batch_size, number_of_attributes)\n","  return(test_attribute_batch)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GcTonAcenxuk","colab_type":"text"},"source":["### Test created test attributes."]},{"cell_type":"code","metadata":{"id":"vhydn9rmn3IB","colab_type":"code","colab":{}},"source":["test_attributes = create_test_attributes()\n","print(test_attributes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E8FRZ2T3-59S","colab_type":"text"},"source":["### Load image using filename."]},{"cell_type":"code","metadata":{"id":"uo12sLtv-6UO","colab_type":"code","colab":{}},"source":["def load_image(image_filename):\n","  input_image = tf.io.read_file(image_filename)\n","  input_image = tf.image.decode_jpeg(input_image, 3)\n","  return(input_image)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ZbZMMuQsk7x","colab_type":"text"},"source":["### Normalize the image to [-1, 1]."]},{"cell_type":"code","metadata":{"id":"_ktprTPusjuz","colab_type":"code","colab":{}},"source":["def normalize_image(image):\n","  image = tf.cast(image, tf.float32)\n","  image = tf.clip_by_value(image, 0, 255) / 127.5 - 1\n","  return(image)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W2U1frPQk-6m","colab_type":"text"},"source":["### Test image normalization."]},{"cell_type":"code","metadata":{"id":"zjvtUqfdum2k","colab_type":"code","colab":{}},"source":["input_image = create_test_image(image_shape)\n","output_image = normalize_image(input_image)\n","print('input image shape',input_image.shape)\n","print('output image shape',output_image.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjY1eyBXsz0F","colab_type":"text"},"source":["### Random crop the image."]},{"cell_type":"code","metadata":{"id":"zF4_Phj7s0J6","colab_type":"code","colab":{}},"source":["def random_crop(image):\n","  cropped_image = tf.image.random_crop(image, size=image_shape)\n","  return(cropped_image)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3N6smGZ3lFH7","colab_type":"text"},"source":["### Test random croping of image."]},{"cell_type":"code","metadata":{"id":"hUjEwqh7u8Ck","colab_type":"code","colab":{}},"source":["input_image = create_test_image(image_load_shape)\n","output_image = random_crop(input_image)\n","print('input image shape',input_image.shape)\n","print('output image shape',output_image.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TtWjAS4XtisQ","colab_type":"text"},"source":["### Random jitter the image."]},{"cell_type":"code","metadata":{"id":"YKO5hc_pti_S","colab_type":"code","colab":{}},"source":["def random_jitter(image):  \n","  image = tf.image.resize(image, [image_load_shape[0], image_load_shape[1]],\n","                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)  \n","  image = random_crop(image)\n","  image = tf.image.random_flip_left_right(image)\n","  return(image)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Veqf7xXKlRIP","colab_type":"text"},"source":["### Test random jittering of image."]},{"cell_type":"code","metadata":{"id":"dfu6UeB3wI76","colab_type":"code","colab":{}},"source":["input_image = create_test_image(image_load_shape)\n","output_image = random_jitter(input_image)\n","print('input image shape',input_image.shape)\n","print('output image shape',output_image.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSN1M5XFuN5G","colab_type":"text"},"source":["### Preprocess train dataset."]},{"cell_type":"code","metadata":{"id":"72L03Hqex1Ge","colab_type":"code","colab":{}},"source":["def compute_attributes(attributes_array):\n","  attributes_array = (attributes_array + 1) // 2 \n","  attributes_array = attributes_array * 1.   \n","  return(attributes_array)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8HOOpVQauOMn","colab_type":"code","colab":{}},"source":["def preprocess_train_dataset(image_filename, attributes):  \n","  \n","  image = load_image(image_filename)\n","  image = random_jitter(image)\n","  image = normalize_image(image)\n","\n","  attributes = compute_attributes(attributes)\n","  return(image, attributes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YU8coU3QuU24","colab_type":"text"},"source":["### Preprocess the test dataset."]},{"cell_type":"code","metadata":{"id":"gpN0_XaXuVOp","colab_type":"code","colab":{}},"source":["def preprocess_test_dataset(image_filename, attributes):    \n","  \n","  image = load_image(image_filename)\n","  image = tf.image.resize(image, [image_shape[0], image_shape[1]], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR) \n","  image = normalize_image(image)\n","\n","  attributes = compute_attributes(attributes)\n","  return(image, attributes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_x_x37erxELV","colab_type":"text"},"source":["### Preprocess dataset splits."]},{"cell_type":"code","metadata":{"id":"I2r9-5qZx3Vz","colab_type":"code","colab":{}},"source":["auto_tune = tf.data.experimental.AUTOTUNE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkZ_hKW5xEl3","colab_type":"code","colab":{}},"source":["train_dataset = train_dataset.map(preprocess_train_dataset, num_parallel_calls=auto_tune)\n","train_dataset = train_dataset.shuffle(buffer_size)\n","train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n","train_dataset = train_dataset.prefetch(auto_tune)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6BMN0ODAs9a","colab_type":"text"},"source":["# Configuration parameters."]},{"cell_type":"code","metadata":{"id":"nVyhDnO0knVx","colab_type":"code","colab":{}},"source":["epochs = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bqE111ZFkhiK","colab_type":"code","colab":{}},"source":["adversarial_loss_mode = 'wgan'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IasodU8I6ahW","colab_type":"code","colab":{}},"source":["d_gradient_penalty_weight = 10.0\n","d_attribute_loss_weight = 1.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MFOoZ6QAAtck","colab_type":"code","colab":{}},"source":["g_attribute_loss_weight = 10.0\n","g_reconstruction_loss_weight = 100.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hdykd1OKjkIx","colab_type":"code","colab":{}},"source":["load_previous_weights = True\n","save_current_weights = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wx1DmYekmIhb","colab_type":"code","colab":{}},"source":["epsilon = 1e-7"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U_WrESOBUbDU","colab_type":"text"},"source":["# Compute gradient penalty."]},{"cell_type":"code","metadata":{"id":"EN5HZSefdTMf","colab_type":"code","colab":{}},"source":["def gradient_penalty(discriminator, real_image, fake_image):\n","    sample_shape = [tf.shape(real_image)[0]] + [1] * (real_image.shape.ndims - 1)\n","    alpha = tf.random.uniform(shape=sample_shape, minval=0., maxval=1.)\n","\n","    sample_image = real_image + alpha * (fake_image - real_image)\n","    sample_image.set_shape(real_image.get_shape().as_list())   \n","    \n","    with tf.GradientTape(watch_accessed_variables=False) as tape:\n","        tape.watch(sample_image)\n","\n","        predictions = discriminator(sample_image, training=False)\n","        if isinstance(predictions, tuple):\n","            predictions = predictions[0]\n","\n","    gradients = tape.gradient(predictions, sample_image)[0]    \n","    gradients = tf.reshape(gradients, [tf.shape(gradients)[0], -1])\n","    norm = tf.norm(epsilon + gradients, axis=1)    \n","    gp_value = tf.reduce_mean((norm - 1.) ** 2)    \n","\n","    return(gp_value)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2LH4xT28snC","colab_type":"text"},"source":["# Create the optimizer.\n","\n","*   Adam optimizer\n","*   Learning rate = 0.0002\n","*   β1 = 0.5\n","*   β2 = 0.999"]},{"cell_type":"code","metadata":{"id":"l8UuboMN8s9z","colab_type":"code","colab":{}},"source":["encoder_optimizer = tf.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n","decoder_optimizer = tf.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n","\n","discriminator_optimizer = tf.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q4jt3WYtlO1r","colab_type":"text"},"source":["# Create the encoder model."]},{"cell_type":"code","metadata":{"id":"V0dmsQaCeU_T","colab_type":"code","colab":{}},"source":["def create_encoder_model(image_shape, encoder_dimension=64, downsamplings_layers=5):\n","  input_image = layers.Input(shape=image_shape, name='input_image')\n","\n","  output_units = encoder_dimension    \n","\n","  encoder_layer = input_image  \n","  feature_layers = []\n","\n","  for layer_index in range(downsamplings_layers):\n","      layer_name = 'block-' + str(layer_index + 1) + '-'\n","\n","      encoder_layer = layers.Conv2D(output_units, (4,4), strides=(2,2), padding='same', name=layer_name + 'conv')(encoder_layer)\n","      encoder_layer = layers.BatchNormalization(name=layer_name + 'bn')(encoder_layer)\n","      encoder_layer = layers.LeakyReLU(alpha=0.2, name=layer_name + 'features')(encoder_layer)\n","\n","      feature_layers.append(encoder_layer)\n","      output_units = output_units * 2    \n","    \n","  # Create the encoder model.\n","  encoder_model = models.Model(inputs=input_image, outputs=feature_layers, name='encoder')  \n"," \n","  return(encoder_model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KTHVLwwgqqFQ","colab_type":"text"},"source":["### Test the encoder model."]},{"cell_type":"code","metadata":{"id":"KsGuR6-N2FcS","colab_type":"code","colab":{}},"source":["encoder = create_encoder_model(image_shape, encoder_dimension=64, downsamplings_layers=5)\n","encoder.summary()\n","\n","input_image = create_test_image_batch(image_shape)\n","output_features = encoder(input_image)\n","print('number of feature layers',len(output_features))\n","for block_index in range(len(output_features)):\n","    print('block-' +str(block_index) +' features shape -', output_features[block_index].shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45xdnUJiFS56","colab_type":"text"},"source":["# Concatenate features and attributes."]},{"cell_type":"code","metadata":{"id":"Z_dLXktO6Hk5","colab_type":"code","colab":{}},"source":["def concatenate(list_of_features, list_of_attributes, layer_name):\n","  list_of_features = list(list_of_features) if isinstance(list_of_features, (list, tuple)) else [list_of_features]\n","  list_of_attributes = list(list_of_attributes) if isinstance(list_of_attributes, (list, tuple)) else [list_of_attributes]\n","  for index, attributes in enumerate(list_of_attributes):\n","        attributes = tf.reshape(attributes, [-1, 1, 1, attributes.shape[-1]], name=layer_name + 'reshape')\n","        attributes = tf.tile(attributes, [1, list_of_features[0].shape[1], list_of_features[0].shape[2], 1], name=layer_name + 'tile')\n","        list_of_attributes[index] = attributes\n","  return tf.concat(list_of_features + list_of_attributes, axis=-1, name=layer_name + 'concat')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_-pu_vcplanL","colab_type":"text"},"source":["# Create the decoder model."]},{"cell_type":"code","metadata":{"id":"udd9yVCliRhC","colab_type":"code","colab":{}},"source":["def create_decoder_model(encoder, number_of_attributes=40, decoder_dimension=64, upsamplings_layers=5, shortcut_layers=1, inject_layers=1):\n","\n","  feature_layers = encoder.outputs\n","  number_of_feature_layers = len(feature_layers)\n","\n","  input_features = []\n","  for layer_index in range(number_of_feature_layers):\n","    layer_name = 'block-' + str(layer_index + 1) + '-features'\n","    layer_shape = feature_layers[layer_index].shape\n","    layer_shape = layer_shape[1:]\n","    feature_layer = layers.Input(shape=layer_shape, name=layer_name) \n","    input_features.append(feature_layer) \n","\n","  input_attributes = layers.Input(shape=number_of_attributes, name='input_attributes')  \n","  output_units = decoder_dimension\n","\n","  layer_name = 'block-0-shortcut-'\n","  decoder_layer = concatenate(input_features[-1], input_attributes, layer_name=layer_name)\n","  for layer_index in range(upsamplings_layers - 1):\n","      layer_name = 'block-' + str(layer_index + 1) + '-'\n","      decoder_layer = layers.Conv2DTranspose(output_units, (4, 4), strides=(2,2), padding='same', name=layer_name+'convt')(decoder_layer)\n","      decoder_layer = layers.BatchNormalization(name=layer_name+'bn')(decoder_layer)\n","      decoder_layer = layers.LeakyReLU(alpha=0.2, name=layer_name+'lrelu')(decoder_layer)\n","\n","      if (shortcut_layers > layer_index):\n","        shortcut_name = layer_name + 'shortcut-'\n","        decoder_layer = concatenate([decoder_layer, input_features[-2 - layer_index]], [], layer_name=shortcut_name)\n","\n","      if (inject_layers > layer_index):\n","        inject_name = layer_name + 'inject-'\n","        decoder_layer = concatenate(decoder_layer, input_attributes, layer_name=inject_name)\n","\n","      output_units = output_units * 2\n","\n","  decoder_layer = layers.Conv2DTranspose(3, (4, 4), strides=(2,2), padding='same', name='block-5-convt')(decoder_layer)\n","  generated_image = layers.Activation('tanh', name='generated_image')(decoder_layer)\n","\n","  # Create the decoder model.\n","  decoder_model = models.Model(inputs=[input_features, input_attributes], outputs=generated_image, name='decoder')\n","\n","  return(decoder_model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ifhkSCMGq3gX","colab_type":"text"},"source":["### Test the decoder model."]},{"cell_type":"code","metadata":{"id":"3qYdNQM12Nzl","colab_type":"code","colab":{}},"source":["encoder = create_encoder_model(image_shape, encoder_dimension=64, downsamplings_layers=5)\n","encoder.summary()\n","\n","decoder = create_decoder_model(encoder, number_of_attributes=40, decoder_dimension=64, upsamplings_layers=5, shortcut_layers=1, inject_layers=1)\n","decoder.summary()\n","\n","input_image = create_test_image_batch(image_shape)\n","attributes = create_test_attribute_batch()\n","\n","encoded_input = encoder(input_image)\n","decoded_output = decoder([encoded_input, attributes])\n","print(decoded_output.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iT1NghHgl_Eh","colab_type":"text"},"source":["# Create the discriminator / classification model."]},{"cell_type":"code","metadata":{"id":"P3kFTSm4IIP6","colab_type":"code","colab":{}},"source":["import tensorflow_addons as tfa"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kj_BDNi9H7s5","colab_type":"code","colab":{}},"source":["def create_discriminator_model(image_shape, number_of_attributes=40, discriminator_dimension=64, dense_dimension=1024, downsamplings_layers=5):\n","    input_image = layers.Input(shape=image_shape, name='input_image')  \n","    input_attributes = layers.Input(shape=number_of_attributes, name='input_attributes')  \n","\n","    output_units = discriminator_dimension\n","    input_layer = input_image\n","    \n","    for layer_index in range(downsamplings_layers): \n","        input_layer = layers.Conv2D(output_units, (4,4), strides=(2,2), padding='same')(input_layer)         \n","        input_layer = tfa.layers.InstanceNormalization()(input_layer)\n","        input_layer = layers.LeakyReLU(alpha=0.2)(input_layer)\n","\n","        output_units = output_units * 2\n","\n","    input_layer = layers.Flatten()(input_layer)\n","\n","    discriminator_output = layers.Dense(dense_dimension)(input_layer) \n","    discriminator_output = tfa.layers.InstanceNormalization()(discriminator_output)     \n","    discriminator_output = layers.LeakyReLU(alpha=0.2)(discriminator_output)\n","    discriminator_output = layers.Dense(1, activation=None)(discriminator_output)    \n","      \n","    attribute_output = layers.Dense(dense_dimension)(input_layer)  \n","    attribute_output = tfa.layers.InstanceNormalization()(attribute_output)         \n","    attribute_output = layers.LeakyReLU(alpha=0.2)(attribute_output)\n","    attribute_output = layers.Dense(number_of_attributes, activation=None)(attribute_output)    \n","\n","    # Create the discriminator model.\n","    discriminator_model = models.Model(inputs=[input_image, input_attributes], outputs=[discriminator_output, attribute_output], name='discriminator')\n","\n","    return(discriminator_model)    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pF9BS_GgH89d","colab_type":"text"},"source":["### Test the discriminator model."]},{"cell_type":"code","metadata":{"id":"6Row8xgKH8DD","colab_type":"code","colab":{}},"source":["input_image = create_test_image_batch(image_shape)\n","\n","discriminator = create_discriminator_model(image_shape, number_of_attributes=40, discriminator_dimension=64, dense_dimension=1024, downsamplings_layers=5)\n","discriminator_prediction, attribute_prediction = discriminator(input_image)\n","\n","print('discriminator prediction shape', discriminator_prediction.shape)\n","print('attribute prediction shape', attribute_prediction.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"46wGfhmb1mAY","colab_type":"text"},"source":["# Create adversarial loss functions.\n","*   Generator loss function\n","*   Discriminator loss function"]},{"cell_type":"markdown","metadata":{"id":"dNVC14t63f_a","colab_type":"text"},"source":["## WGAN loss functions.\n","*   Generator loss function\n","*   Discriminator loss function"]},{"cell_type":"code","metadata":{"id":"SRmIcg_33WoD","colab_type":"code","colab":{}},"source":["def wgan_loss_functions():\n","    def discriminator_loss_function(real_logit, fake_logit):\n","        real_loss = - tf.reduce_mean(real_logit)\n","        fake_loss = tf.reduce_mean(fake_logit)\n","        return(real_loss, fake_loss)\n","\n","    def generator_loss_function(fake_logit):\n","        fake_loss = - tf.reduce_mean(fake_logit)\n","        return(fake_loss)\n","\n","    return(discriminator_loss_function, generator_loss_function)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CP56ouow1mgf","colab_type":"code","colab":{}},"source":["def adversarial_loss_functions(adversarial_loss_mode):\n","  if(adversarial_loss_mode == 'wgan'):\n","    return(wgan_loss_functions())\n","  else:\n","    return(wgan_loss_functions())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qshxq7QobTQI","colab_type":"text"},"source":["# Create different models and loss functions.\n","* Encoder model\n","* Decoder model\n","* Discriminator model\n","* Discriminator loss function\n","* Generator loss function"]},{"cell_type":"code","metadata":{"id":"BXNN-azN4r05","colab_type":"code","colab":{}},"source":["encoder = create_encoder_model(image_shape, encoder_dimension=64, downsamplings_layers=5)\n","decoder = create_decoder_model(encoder, number_of_attributes=40, decoder_dimension=64,)\n","discriminator = create_discriminator_model(image_shape, number_of_attributes=40, discriminator_dimension=64, dense_dimension=1024, downsamplings_layers=5)\n","\n","discriminator_loss_function, generator_loss_function = adversarial_loss_functions(adversarial_loss_mode)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XWS2A5gHffud","colab_type":"text"},"source":["# Create the generator model."]},{"cell_type":"code","metadata":{"id":"6TKgnY06fgLY","colab_type":"code","colab":{}},"source":["input_image = layers.Input(shape=image_shape, name='input_image')  \n","input_attributes = layers.Input(shape=number_of_attributes, name='input_attributes')\n","\n","encoded_features = encoder(input_image)\n","generated_image = decoder([encoded_features, input_attributes])\n","\n","generator_model = models.Model(inputs=[input_image, input_attributes], outputs=generated_image, name='generator')\n","generator_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2kCJSqUEpVxx","colab_type":"text"},"source":["### Test the generator model."]},{"cell_type":"code","metadata":{"id":"zZ4pIeb2pWJp","colab_type":"code","colab":{}},"source":["input_image = create_test_image_batch(image_shape)\n","attributes = create_test_attribute_batch()\n","generated_image = generator_model([input_image, attributes])\n","\n","print('input image shape',input_image.shape)\n","print('generated image shape',generated_image.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kd22Y_kzhNPf","colab_type":"text"},"source":["### Load previous model weights.\n","* Encoder model weights\n","* Decoder model weights\n","* Discriminator model weights\n","\n"]},{"cell_type":"code","metadata":{"id":"Qn9K_wgwBgJE","colab_type":"code","colab":{}},"source":["import os \n","\n","def encoder_filename():\n","  return('encoder.h5')\n","\n","def encoder_gdrive_filename(weight_root_dir='/content/drive/My Drive/models/AttGAN/'):    \n","  return(os.path.join(weight_root_dir, encoder_filename()))\n","\n","def decoder_filename():\n","  return('decoder.h5')\n","\n","def decoder_gdrive_filename(weight_root_dir='/content/drive/My Drive/models/AttGAN/'):    \n","  return(os.path.join(weight_root_dir, decoder_filename()))\n","\n","def discriminator_filename():\n","  return('discriminator.h5')\n","\n","def discriminator_gdrive_filename(weight_root_dir='/content/drive/My Drive/models/AttGAN/'):  \n","  return(os.path.join(weight_root_dir, discriminator_filename()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNv6MAX5hQFL","colab_type":"code","colab":{}},"source":["if(load_previous_weights):\n","  encoder.load_weights(encoder_gdrive_filename())\n","  decoder.load_weights(decoder_gdrive_filename())\n","  discriminator.load_weights(discriminator_gdrive_filename())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DmRb8Ok3-db5","colab_type":"text"},"source":["# Compute the generator loss."]},{"cell_type":"code","metadata":{"id":"CfAvkKJj-dxp","colab_type":"code","colab":{}},"source":["def compute_generator_loss(input_image, input_attributes):\n","\n","  target_attributes = tf.random.shuffle(input_attributes)\n","\n","  scaled_input_attributes = input_attributes * 2. - 1.\n","  scaled_target_attributes = target_attributes * 2. - 1.\n","\n","  # Generator\n","  input_features = encoder(input_image, training=True)\n","  reconstructed_image = decoder([input_features, scaled_input_attributes], training=True)\n","  fake_image = decoder([input_features, scaled_target_attributes], training=True)\n","\n","  # Discriminator\n","  fake_image_prediction, fake_image_attributes = discriminator(fake_image, training=False)\n","\n","  fake_image_prediction_loss = generator_loss_function(fake_image_prediction)\n","  fake_image_attributes_loss = tf.compat.v1.losses.sigmoid_cross_entropy(target_attributes, fake_image_attributes)  \n","  \n","  input_image_reconstruction_loss = tf.compat.v1.losses.absolute_difference(input_image, reconstructed_image)\n","   \n","  generator_loss = (  fake_image_prediction_loss \n","                    + fake_image_attributes_loss * g_attribute_loss_weight \n","                    + input_image_reconstruction_loss * g_reconstruction_loss_weight\n","                    )  \n","\n","  return(generator_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kMVU0PDctPdo","colab_type":"text"},"source":["### Test computation of the generator loss."]},{"cell_type":"code","metadata":{"id":"QOebjyEcPG9f","colab_type":"code","colab":{}},"source":["images, attributes = next(iter(train_dataset))\n","loss = compute_generator_loss(images, attributes)\n","print(loss.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b5Zls6m5NOv5","colab_type":"text"},"source":["# Compute the discriminator loss."]},{"cell_type":"code","metadata":{"id":"ViRU0tj7NPfZ","colab_type":"code","colab":{}},"source":["def compute_discriminator_loss(input_image, input_attributes):\n","\n","  target_attributes = tf.random.shuffle(input_attributes)\n","\n","  scaled_input_attributes = input_attributes * 2. - 1.\n","  scaled_target_attributes = target_attributes * 2. - 1.\n","\n","  # Generate\n","  input_features = encoder(input_image, training=False)  \n","  fake_image = decoder([input_features, scaled_target_attributes], training=False)\n","\n","  # Discriminate\n","  input_image_prediction, input_image_attributes = discriminator(input_image, training=True)\n","  fake_image_prediction, fake_image_attributes = discriminator(fake_image, training=True)\n","\n","  # Discriminator losses\n","  input_image_gan_loss, fake_image_gan_loss = discriminator_loss_function(input_image_prediction, fake_image_prediction)  \n","  gradient_penalty_value = gradient_penalty(discriminator, input_image, fake_image)      \n","\n","  input_image_attributes_loss = tf.compat.v1.losses.sigmoid_cross_entropy(input_attributes, input_image_attributes)  \n","\n","  discriminator_loss = (  input_image_gan_loss \n","                        + fake_image_gan_loss \n","                        + gradient_penalty_value * d_gradient_penalty_weight \n","                        + input_image_attributes_loss * d_attribute_loss_weight\n","                        )  \n","  \n","  return(discriminator_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gD5EpsqItFav","colab_type":"text"},"source":["### Test computation of the discriminator loss."]},{"cell_type":"code","metadata":{"id":"ieWxLG9TSvje","colab_type":"code","colab":{}},"source":["images, attributes = next(iter(train_dataset))\n","loss = compute_discriminator_loss(images, attributes)\n","print(loss.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hf1OgYwVm4fi","colab_type":"text"},"source":["# Train the model."]},{"cell_type":"code","metadata":{"id":"4NXxgonbX_o6","colab_type":"code","colab":{}},"source":["model_loss_frequency = 500\n","model_save_frequency = 2000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d5X43kesJbGw","colab_type":"code","colab":{}},"source":["def save_models():\n","  if(save_current_weights):  \n","    encoder.save_weights(encoder_gdrive_filename())        \n","    decoder.save_weights(decoder_gdrive_filename())      \n","    discriminator.save_weights(discriminator_gdrive_filename())  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJVAe5MrfCSN","colab_type":"code","colab":{}},"source":["def train(train_dataset, epochs=100):  \n","\n","  for epoch in range(epochs):\n","\n","    batch_index = 0    \n","    for dataset_batch in train_dataset:  \n","      batch_index = batch_index + 1    \n","\n","      images, attributes = dataset_batch\n","      \n","      if(batch_index%6 == 0):\n","\n","        with tf.GradientTape(watch_accessed_variables=False) as encoder_tape, tf.GradientTape(watch_accessed_variables=False) as decoder_tape:\n","          encoder_tape.watch(encoder.trainable_variables)\n","          decoder_tape.watch(decoder.trainable_variables)\n","          generator_loss = compute_generator_loss(images, attributes)\n","\n","        enoder_gradients = encoder_tape.gradient(generator_loss, encoder.trainable_variables)\n","        decoder_gradients = decoder_tape.gradient(generator_loss, decoder.trainable_variables)\n","\n","        encoder_optimizer.apply_gradients(zip(enoder_gradients, encoder.trainable_variables))\n","        decoder_optimizer.apply_gradients(zip(decoder_gradients, decoder.trainable_variables))\n","\n","      else:\n","        with tf.GradientTape(watch_accessed_variables=False) as discriminator_tape:\n","          discriminator_tape.watch(discriminator.trainable_variables)\n","          discriminator_loss = compute_discriminator_loss(images, attributes)\n","\n","        discriminator_gradients = discriminator_tape.gradient(discriminator_loss, discriminator.trainable_variables)\n","        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))        \n","\n","      if(batch_index%model_loss_frequency == 0):\n","        print('epoch -', epoch, 'generator loss -', generator_loss.numpy(), 'discriminator loss -', discriminator_loss.numpy())\n","\n","      if(batch_index%model_save_frequency == 0):\n","        save_models()\n","\n","    # Save model weights at the end of epoch.\n","    save_models()\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZWMK-qlUfUf-","colab_type":"code","colab":{}},"source":["train(train_dataset, epochs=10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vIubn8o5Z-VH","colab_type":"code","colab":{}},"source":["save_models()"],"execution_count":0,"outputs":[]}]}