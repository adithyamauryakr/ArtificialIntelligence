{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CycleGAN.ipynb","provenance":[{"file_id":"https://github.com/look4pritam/miccai18/blob/master/miccai18.ipynb","timestamp":1587091210019}],"private_outputs":true,"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"euM-0r-85VDa","colab_type":"text"},"source":["# Install TensorFlow.\n","*   Install TensorFlow-1.14.0 with CUDA-10 support.\n","*   Install Keras-2.2.4, which compatible with TensorFlow-1.14.0."]},{"cell_type":"code","metadata":{"id":"iyh4xR_l5PV9","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x\n","!pip uninstall tensorflow -y\n","!pip uninstall tensorflow-gpu -y\n","!pip install --upgrade tensorflow-gpu==1.14.0\n","!pip install --upgrade keras==2.2.4"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VaPC9vH86lww","colab_type":"text"},"source":["# Use installed TensorFlow version by running Runtime -> Restart runtime."]},{"cell_type":"markdown","metadata":{"id":"vyy3gyE-6UMu","colab_type":"text"},"source":["# Install keras-contrib module."]},{"cell_type":"code","metadata":{"id":"1TlaM-SmI6OQ","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x\n","!sudo pip install git+https://www.github.com/keras-team/keras-contrib.git"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4OnZ0jJf685g","colab_type":"text"},"source":["# Import python modules."]},{"cell_type":"code","metadata":{"id":"ABMgyjrMXqyz","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x\n","from keras.models import Model\n","from keras.models import Input\n","from keras.layers import Conv2D"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KFjoDCm5xY99","colab_type":"code","colab":{}},"source":["from keras.layers import LeakyReLU\n","from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zv8Fyp9YCsxR","colab_type":"text"},"source":["# Create the weight initializer.\n","* Gaussian distribution - N(0.0, 0.02)."]},{"cell_type":"code","metadata":{"id":"-qiZIkQyCmTL","colab_type":"code","colab":{}},"source":["from keras.initializers import RandomNormal\n","\n","initializer = RandomNormal(mean=0.0, stddev=0.02)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HMDcrWS1D0io","colab_type":"text"},"source":["# Create the optimizer.\n","*   Adam optimizer.\n","*   Learning rate = 0.0002 - First 100 epochs.\n","*   Learning rate is decreased from 0.0002 to 0.0 in 100 epochs."]},{"cell_type":"code","metadata":{"id":"ICG_1Z4C_XTb","colab_type":"code","colab":{}},"source":["from keras.optimizers import Adam\n","\n","optimizer = Adam(lr=0.0002, beta_1=0.5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B2-lA-rS7OJf","colab_type":"text"},"source":["# Create the discriminator model."]},{"cell_type":"code","metadata":{"id":"2qgtOP7hhAIy","colab_type":"code","colab":{}},"source":["def create_discriminator(image_shape):\n","\t# Source image input.\n","\tinput_image = Input(shape=image_shape)\n"," \n","\t# Create layer C64.\n","\tlayer = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=initializer)(input_image)\n","\tlayer = LeakyReLU(alpha=0.2)(layer)\n"," \n","\t# Create layer C128.\n","\tlayer = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=initializer)(layer)\n","\tlayer = InstanceNormalization(axis=-1)(layer)\n","\tlayer = LeakyReLU(alpha=0.2)(layer)\n"," \n","\t# Create layer C256.\n","\tlayer = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=initializer)(layer)\n","\tlayer = InstanceNormalization(axis=-1)(layer)\n","\tlayer = LeakyReLU(alpha=0.2)(layer)\n"," \n","\t# Create layer C512.\n","\tlayer = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=initializer)(layer)\n","\tlayer = InstanceNormalization(axis=-1)(layer)\n","\tlayer = LeakyReLU(alpha=0.2)(layer)\n"," \n","\t# Create second last layer.\n","\tlayer = Conv2D(512, (4,4), padding='same', kernel_initializer=initializer)(layer)\n","\tlayer = InstanceNormalization(axis=-1)(layer)\n","\tlayer = LeakyReLU(alpha=0.2)(layer)\n"," \n","\t# Create the discriminator output layer.\n","\tmodel_output = Conv2D(1, (4,4), strides=(1,1), padding='same', kernel_initializer=initializer)(layer)\n"," \n","\t# Create the discriminator model.\n","\tdiscriminator_model = Model(input_image, model_output)\n"," \n","\t# Compile the discriminator model.\n","\t# Loss - MSE - Weighted by 1/2 = 0.5\n","\tdiscriminator_model.compile(loss='mse', optimizer=optimizer, loss_weights=[0.5])\n","\treturn( discriminator_model )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mu2mXqNg_Ev5","colab_type":"code","colab":{}},"source":["# Define input image shape.\n","image_shape = (256, 256, 3)\n","# Create the discriminator model.\n","model = create_discriminator(image_shape)\n","# Show the model summary.\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S5jmWbSGCVFi","colab_type":"text"},"source":["# Define the ResNet block."]},{"cell_type":"code","metadata":{"id":"B6juNSDJxfLV","colab_type":"code","colab":{}},"source":["from keras.layers import Activation\n","from keras.layers import Concatenate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-mr8-H8goah","colab_type":"code","colab":{}},"source":["def create_resnet_block(number_of_filters, input_layer):\n","\n","\t# Create first convolutional layer.\n","\tblock_output = Conv2D(number_of_filters, (3,3), padding='same', kernel_initializer=initializer)(input_layer)\n","\tblock_output = InstanceNormalization(axis=-1)(block_output)\n","\tblock_output = Activation('relu')(block_output)\n"," \n","\t# Create second convolutional layer.\n","\tblock_output = Conv2D(number_of_filters, (3,3), padding='same', kernel_initializer=initializer)(block_output)\n","\tblock_output = InstanceNormalization(axis=-1)(block_output)\n"," \n","\t# Concatenate and merge channel-wise with input layer.\n","\tblock_output = Concatenate()([block_output, input_layer])\n","\treturn(block_output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nz2ZfUYXvfoS","colab_type":"text"},"source":["# Create the generator model."]},{"cell_type":"code","metadata":{"id":"XGltPiYnxQSP","colab_type":"code","colab":{}},"source":["from keras.layers import Conv2DTranspose"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5EjM_RxSjUnf","colab_type":"code","colab":{}},"source":["def create_generator(image_shape=(256,256,3), resnet_blocks=9):\n","\n","\t# Create input image.\n","\tinput_image = Input(shape=image_shape)\n"," \n","  ######################################################################\n","  # c7s1-k - Conv2D-InstanceNormalization-ReLU block. \n","\t#        - Conv2D 7×7 with k filters and stride 1.\n","  \n","\t# dk     - Conv2D-InstanceNormalization-ReLU block.\n","\t#        - Conv2D 3×3 with k filters and stride 2.\n","\n","\t# Rk     - Residual block \n","\t#        - 2 3×3 Conv2D layers with k filters on both layers.\n","\t\n","\t# uk     - Fractional-strided Conv2D-InstanceNormalization-ReLU block. \n","\t#        - Conv2D 3×3 with k filters and stride 1/2.\n","  ######################################################################\n","\n","\t# Create c7s1-64 block.\n","\tlayer = Conv2D(64, (7,7), strides=(1,1), padding='same', kernel_initializer=initializer)(input_image)\n","\tlayer = InstanceNormalization(axis=-1)(layer)\n","\tlayer = Activation('relu')(layer)\n"," \n","\t# Create d128 block.\n","\tlayer = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=initializer)(layer)\n","\tlayer = InstanceNormalization(axis=-1)(layer)\n","\tlayer = Activation('relu')(layer)\n"," \n","\t# Create d256 block.\n","\tlayer = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=initializer)(layer)\n","\tlayer = InstanceNormalization(axis=-1)(layer)\n","\tlayer = Activation('relu')(layer)\n"," \n","\t# Create R256 blocks.\n","\tfor _ in range(resnet_blocks):\n","\t\tlayer = create_resnet_block(256, layer)\n","\t\n","\t# Create u128 block.\n","\tlayer = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=initializer)(layer)\n","\tlayer = InstanceNormalization(axis=-1)(layer)\n","\tlayer = Activation('relu')(layer)\n"," \n","\t# Create u64 block.\n","\tlayer = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=initializer)(layer)\n","\tlayer = InstanceNormalization(axis=-1)(layer)\n","\tlayer = Activation('relu')(layer)\n"," \n","\t# Create c7s1-3 block.\n","\tlayer = Conv2D(3, (7,7), strides=(1,1), padding='same', kernel_initializer=initializer)(layer)\n","\tlayer = InstanceNormalization(axis=-1)(layer)\n","\toutput_image = Activation('tanh')(layer)\n"," \n","\t# Create the generator model.\n","\tgenerator_model = Model(input_image, output_image)\n","\treturn( generator_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hgknbgQa1mvb","colab_type":"code","colab":{}},"source":["# Define input image shape.\n","image_shape = (256, 256, 3)\n","# Create the generator model.\n","model = create_generator(image_shape)\n","# Show the model summary.\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"epcrBYet7ecf","colab_type":"text"},"source":["# Create a composite model for updating the generators using adversarial and cycle loss."]},{"cell_type":"code","metadata":{"id":"5FDHXNdVTqUb","colab_type":"code","colab":{}},"source":["def create_composite_model(generator_A_to_B, discriminator_2, generator_B_to_A, image_shape):\n","\t# Generator for domain A to B is trainable.\t\n","\tgenerator_A_to_B.trainable = True\n","\n","\t# Discriminator for domain B is NOT trainable.\n","\tdiscriminator_2.trainable = False\n","\n","\t# Generator for domain B to A is NOT trainable.\n","\tgenerator_B_to_A.trainable = False\n","\n","\t# Define domain B discriminator output.\n","\tgenerator_input = Input(shape=image_shape)\n","\tgenerator_A2B_output = generator_A_to_B(generator_input)\n","\tdiscriminator_B_output = discriminator_2(generator_A2B_output)\n"," \n","\t# Define identity output.\n","\tidentity_input = Input(shape=image_shape)\n","\tidentity_output = generator_A_to_B(identity_input)\n"," \n","\t# Define forward cycle output.\n","\tforward_cycle_output = generator_B_to_A(generator_A2B_output)\n"," \n","\t# Define backward cycle output.\n","\tgenerator_B2A_output = generator_B_to_A(identity_input)\n","\tbackward_cycle_output = generator_A_to_B(generator_B2A_output)\n"," \n","\t# Create the composite model.\n","\tcomposite_model = Model([generator_input, identity_input], \n","\t                        [discriminator_B_output, identity_output, \n","\t\t\t\t\t\t\t\t\t\t\t\t\t forward_cycle_output, backward_cycle_output])\n"," \n","\t# Compile the composite model with weighted loss using least squares loss and L1 loss.\n","\tcomposite_model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=optimizer)\n","\treturn(composite_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5cAoon0TTmnn","colab_type":"code","colab":{}},"source":["# select a batch of random samples, returns images and target\n","def generate_real_samples(dataset, n_samples, patch_shape):\n","\t# choose random instances\n","\tix = np.random.randint(0, dataset.shape[0], n_samples)\n","\t# retrieve selected images\n","\tX = dataset[ix]\n","\t# generate 'real' class labels (1)\n","\ty = np.ones((n_samples, patch_shape, patch_shape, 1))\n","\treturn X, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HOFtaJzKLgmg","colab_type":"code","colab":{}},"source":["# generate a batch of images, returns images and targets\n","def generate_fake_samples(g_model, dataset, patch_shape):\n","\t# generate fake instance\n","\tX = g_model.predict(dataset)\n","\t# create 'fake' class labels (0)\n","\ty = np.zeros((len(X), patch_shape, patch_shape, 1))\n","\treturn X, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GH1hkjwfLvEs","colab_type":"code","colab":{}},"source":["# update image pool for fake images\n","def update_image_pool(pool, images, max_size=50):\n","\tselected = list()\n","\tfor image in images:\n","\t\tif len(pool) < max_size:\n","\t\t\t# stock the pool\n","\t\t\tpool.append(image)\n","\t\t\tselected.append(image)\n","\t\telif np.random.random() < 0.5:\n","\t\t\t# use image, but don't add it to the pool\n","\t\t\tselected.append(image)\n","\t\telse:\n","\t\t\t# replace an existing image and use replaced image\n","\t\t\tix = np.random.randint(0, len(pool))\n","\t\t\tselected.append(pool[ix])\n","\t\t\tpool[ix] = image\n","\treturn np.asarray(selected)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wJA3MkaTtC2","colab_type":"code","colab":{}},"source":["# train cyclegan models\n","def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset, number_of_epochs=100, batch_size=1):\n","\n","\t# determine the output square shape of the discriminator\n","\tn_patch = d_model_A.output_shape[1]\n","\t# unpack dataset\n","\ttrainA, trainB = dataset\n","\t# prepare image pool for fakes\n","\tpoolA, poolB = list(), list()\n","\t# calculate the number of batches per training epoch\n","\tbat_per_epo = int(len(trainA) / batch_size)\n","\t# calculate the number of training iterations\n","\tn_steps = bat_per_epo * number_of_epochs\n","\t# manually enumerate epochs\n","\tfor i in range(n_steps):\n","\t\t# select a batch of real samples\n","\t\tX_realA, y_realA = generate_real_samples(trainA, batch_size, n_patch)\n","\t\tX_realB, y_realB = generate_real_samples(trainB, batch_size, n_patch)\n","\t\t# generate a batch of fake samples\n","\t\tX_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n","\t\tX_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n","\t\t# update fakes from pool\n","\t\tX_fakeA = update_image_pool(poolA, X_fakeA)\n","\t\tX_fakeB = update_image_pool(poolB, X_fakeB)\n","\t\t# update generator B->A via adversarial and cycle loss\n","\t\tg_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n","\t\t# update discriminator for A -> [real/fake]\n","\t\tdA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n","\t\tdA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n","\t\t# update generator A->B via adversarial and cycle loss\n","\t\tg_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n","\t\t# update discriminator for B -> [real/fake]\n","\t\tdB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n","\t\tdB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n","\t\t# summarize performance\n","\t\tprint('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UgZmJ2SvQ9yC","colab_type":"code","colab":{}},"source":["import os\n","import keras\n","\n","dataset_url = 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip'\n","\n","zip_file = keras.utils.get_file(origin=dataset_url,\n","                                   fname=\"horse2zebra.zip\",\n","                                   extract=True)\n","\n","#\"ae_photos\" \"apple2orange\" \"summer2winter_yosemite\" \"horse2zebra\" \"monet2photo\" \"cezanne2photo\" \"ukiyoe2photo\" \"vangogh2photo\" \"maps\" \"cityscapes\" \"facades\" \"iphone2dslr_flower\" \"ae_photos\" \n","\n","base_dir = os.path.join(os.path.dirname(zip_file), 'horse2zebra')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RPf9e048SOmL","colab_type":"code","colab":{}},"source":["!ls -al /root/.keras/datasets/horse2zebra"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"naz1Qlt8V_E-","colab_type":"code","colab":{}},"source":["import cv2\n","from glob import glob\n","import numpy as np\n","\n","def load_data(domain, is_testing=False):\n","        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n","        path = glob('%s/%s/*' % (base_dir,data_type))        \n","\n","        imgs = []\n","        for img_path in path:\n","            img = cv2.imread(img_path, cv2.IMREAD_COLOR).astype(np.float)\n","            if not is_testing:\n","                img = cv2.resize(img, (256, 256))\n","\n","                if np.random.random() > 0.5:\n","                    img = np.fliplr(img)\n","            else:\n","                img = cv2.resize(img, (256, 256))\n","            imgs.append(img)\n","\n","        imgs = np.array(imgs)/127.5 - 1.\n","\n","        return imgs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GG3bpSI6aow9","colab_type":"code","colab":{}},"source":["train_horses = load_data('A', is_testing=False)\n","train_zebras = load_data('B', is_testing=False)\n","\n","print(len(train_horses))\n","print(len(train_zebras))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Zvw0pfxveMa","colab_type":"code","colab":{}},"source":["test_horses = load_data('A', is_testing=True)\n","test_zebras = load_data('B', is_testing=True)    \n","\n","print(len(test_horses))\n","print(len(test_zebras))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iYn0YzWtTw5V","colab_type":"code","colab":{}},"source":["# load a dataset as a list of two numpy arrays\n","dataset = [train_horses, train_zebras]\n","\n","# input shape\n","image_shape = (256,256,3)\n","# generator: A -> B\n","g_model_AtoB = create_generator(image_shape)\n","# generator: B -> A\n","g_model_BtoA = create_generator(image_shape)\n","# discriminator: A -> [real/fake]\n","d_model_A = create_discriminator(image_shape)\n","# discriminator: B -> [real/fake]\n","d_model_B = create_discriminator(image_shape)\n","\n","# composite: A -> B -> [real/fake, A]\n","c_model_AtoB = create_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)\n","# composite: B -> A -> [real/fake, B]\n","c_model_BtoA = create_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)\n","\n","number_of_epochs = 1\n","batch_size = 1\n","# train models\n","train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset, number_of_epochs, batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1hwO97CdOZY","colab_type":"code","colab":{}},"source":["g_model_AtoB.save_weights('g_model_AtoB.h5')\n","g_model_BtoA.save_weights('g_model_BtoA.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"STdqk09LvFCG","colab_type":"code","colab":{}},"source":["output_images = g_model_AtoB.predict(test_horses)\n","\n","for input_image, output_image in zip(test_horses, output_images):\n","  input_image = (input_image + 1) * 127.5\n","  output_image = (output_image + 1) * 127.5\n","  cv2.imwrite('input_image.png', input_image)\n","  cv2.imwrite('output_image.png', output_image)"],"execution_count":0,"outputs":[]}]}