{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AttGAN.ipynb","provenance":[{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN.ipynb","timestamp":1588851525549},{"file_id":"1jh3lx63SznwpbVoRYlVcw-gPm1LEliR_","timestamp":1580294178249}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pKa85vKe0hW9","colab_type":"text"},"source":["# Install gdown Python package."]},{"cell_type":"code","metadata":{"id":"nY0sLk470jTD","colab_type":"code","colab":{}},"source":["!pip install -U --no-cache-dir gdown"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwVm1Dt2kYIo","colab_type":"text"},"source":["# Install Tensorflow-Addons."]},{"cell_type":"code","metadata":{"id":"qojNeXFvkZP1","colab_type":"code","colab":{}},"source":["!pip install tensorflow-addons"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lRlVVxjwkgI_","colab_type":"text"},"source":["# Install Tensorflow-datasets."]},{"cell_type":"code","metadata":{"id":"52xH5daykjJY","colab_type":"code","colab":{}},"source":["!pip install tensorflow-datasets"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4LXy2rlgkSCs","colab_type":"text"},"source":["# Use TensorFlow 2.x."]},{"cell_type":"code","metadata":{"id":"gpqilhyfkT7y","colab_type":"code","colab":{}},"source":["try:\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import tensorflow as tf\n","import tensorflow.keras.layers as layers\n","\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2LH4xT28snC","colab_type":"text"},"source":["# Create the optimizer.\n","\n","*   Adam optimizer\n","*   Learning rate = 0.0002\n","*   β1 = 0.5\n","*   β2 = 0.999"]},{"cell_type":"code","metadata":{"id":"l8UuboMN8s9z","colab_type":"code","colab":{}},"source":["optimizer = tf.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DORE70aMkmDZ","colab_type":"text"},"source":["# List available TensorFlow datasets."]},{"cell_type":"code","metadata":{"id":"ufndLNkUknPi","colab_type":"code","colab":{}},"source":["import tensorflow_datasets as tfds\n","\n","tfds.list_builders()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q4jt3WYtlO1r","colab_type":"text"},"source":["# Create AttGAN encoder model."]},{"cell_type":"code","metadata":{"id":"tUa2yT9PlPg7","colab_type":"code","colab":{}},"source":["class UNetGenc(layers.Layer):\n","\n","  def __init__(self, dimension=64, downsamplings_layers=5):\n","    super(UNetGenc, self).__init__()\n","\n","    self._dimension = 64\n","    self._downsamplings_layers = 5\n","\n","  def call(self, inputs):\n","\n","    input_layer = inputs\n","    output_units = self._dimension\n","    \n","    output_layers = []\n","    for layer_index in range(self._downsamplings_layers):\n","      input_layer = layers.Conv2D(output_units, (4,4), strides=(2,2), padding='same')(input_layer)\n","      input_layer = layers.BatchNormalization()(input_layer)\n","      input_layer = layers.LeakyReLU(alpha=0.2)(input_layer)\n","\n","      output_layers.append(input_layer)\n","      output_units = output_units * 2    \n","    \n","    return(output_layers)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KsGuR6-N2FcS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_-pu_vcplanL","colab_type":"text"},"source":["# Create AttGAN decoder model."]},{"cell_type":"code","metadata":{"id":"_2mzE6UAlbAB","colab_type":"code","colab":{}},"source":["def concatenate(a_list, b_list=[]):\n","  # tile all elements of `b_list` and then concat `a_list + b_list` along the channel axis\n","  # `a` shape: (N, H, W, C_a)\n","  # `b` shape: can be (N, 1, 1, C_b) or (N, C_b)\n","  a_list = list(a_list) if isinstance(a_list, (list, tuple)) else [a_list]\n","  b_list = list(b_list) if isinstance(b_list, (list, tuple)) else [b_list]\n","  for i, b in enumerate(b_list):\n","        b = tf.reshape(b, [-1, 1, 1, b.shape[-1]])\n","        b = tf.tile(b, [1, a_list[0].shape[1], a_list[0].shape[2], 1])\n","        b_list[i] = b\n","  return tf.concat(a_list + b_list, axis=-1)\n","\n","class UNetGdec(layers.Layer):\n","\n","  def __init__(self, dimension=64, upsamplings_layers=5, shortcut_layers=1, inject_layers=1):\n","    super(UNetGdec, self).__init__()\n","\n","    self._dimension = 64\n","    self._upsamplings_layers = 5\n","    self._shortcut_layers = shortcut_layers\n","    self._inject_layers = inject_layers\n","\n","  def call(self, inputs):\n","    zs, a = inputs\n","\n","    a = tf.to_float(a)\n","    input_layer = concatenate(zs[-1], a)\n","    output_units = self._dimension\n","    for layer_index in range(self.__upsamplings_layers-1):\n","      input_layer = layers.Conv2DTranspose(output_units, (4, 4), strides=(2,2), padding='same')(input_layer)\n","      input_layer = layers.BatchNormalization()(input_layer)\n","      input_layer = layers.LeakyReLU(alpha=0.2)(input_layer)\n","\n","      if (self._shortcut_layers > layer_index):\n","        input_layer = concatenate([input_layer, zs[-2 - layer_index]])\n","\n","      if (self._inject_layers > layer_index):\n","        input_layer = concatenate(input_layer, a)\n","\n","      output_units = output_units * 2\n","\n","    input_layer = layers.Conv2DTranspose(3, (4, 4), strides=(2,2), padding='same')(input_layer)\n","    input_layer = tf.keras.activations.tanh(input_layer) \n","\n","    output_layer = input_layer\n","\n","    return(output_layer)  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3qYdNQM12Nzl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iT1NghHgl_Eh","colab_type":"text"},"source":["# Create AttGAN discriminator model."]},{"cell_type":"code","metadata":{"id":"i99fuwWQmEH2","colab_type":"code","colab":{}},"source":["import tensorflow_addons as tfa\n","\n","class Discriminator(layers.Layer):\n","\n","  def __init__(self, number_of_attributes, dimension=64, dense_dimension=1024, downsamplings_layers=5): \n","    super(Discriminator, self).__init__()  \n","\n","    self._number_of_attributes = number_of_attributes\n","    self._dimension = dimension\n","    self._dense_dimension = dense_dimension\n","    self._downsamplings_layers = downsamplings_layers\n","\n","    def call(self, inputs):\n","\n","      input_layer = inputs  \n","      output_units = self._dimension  \n","\n","      for layer_index in range(self._downsamplings_layers): \n","        input_layer = layers.Conv2D(output_units, (4,4), strides=(2,2), padding='same')(input_layer)         \n","        input_layer = tfa.layers.InstanceNormalization()(input_layer)\n","        input_layer = layers.LeakyReLU(alpha=0.2)(input_layer)\n","\n","        output_units = output_units * 2\n","\n","      input_layer = layers.Flatten()(input_layer)\n","\n","      discriminator_output = layers.Dense(self._dense_dimension)(input_layer)      \n","      discriminator_output = layers.LeakyReLU(alpha=0.2)(discriminator_output)\n","      discriminator_output = layers.Dense(1)(discriminator_output)\n","      \n","      attribute_output = layers.Dense(self._dense_dimension)(input_layer)      \n","      attribute_output = layers.LeakyReLU(alpha=0.2)(attribute_output)\n","      attribute_output = layers.Dense(self._number_of_attributes)(attribute_output)\n","\n","      return(discriminator_output, attribute_output)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5Y3uQuN2Opc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}