{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AttGAN.ipynb","provenance":[{"file_id":"https://github.com/look4pritam/TensorFlowExamples/blob/master/GAN/AttGAN.ipynb","timestamp":1588851525549},{"file_id":"1jh3lx63SznwpbVoRYlVcw-gPm1LEliR_","timestamp":1580294178249}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pKa85vKe0hW9","colab_type":"text"},"source":["# Install gdown Python package."]},{"cell_type":"code","metadata":{"id":"nY0sLk470jTD","colab_type":"code","colab":{}},"source":["!pip install -U --no-cache-dir gdown"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwVm1Dt2kYIo","colab_type":"text"},"source":["# Install Tensorflow-Addons."]},{"cell_type":"code","metadata":{"id":"qojNeXFvkZP1","colab_type":"code","colab":{}},"source":["!pip install tensorflow-addons"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lRlVVxjwkgI_","colab_type":"text"},"source":["# Install Tensorflow-datasets."]},{"cell_type":"code","metadata":{"id":"52xH5daykjJY","colab_type":"code","colab":{}},"source":["!pip install tensorflow-datasets"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4LXy2rlgkSCs","colab_type":"text"},"source":["# Use TensorFlow 2.x."]},{"cell_type":"code","metadata":{"id":"gpqilhyfkT7y","colab_type":"code","colab":{}},"source":["try:\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import tensorflow as tf\n","import tensorflow.keras.layers as layers\n","import tensorflow.keras.models as models\n","\n","import numpy as np\n","np.random.seed(7)\n","\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6BMN0ODAs9a","colab_type":"text"},"source":["# Configuration parameters."]},{"cell_type":"code","metadata":{"id":"I2r9-5qZx3Vz","colab_type":"code","colab":{}},"source":["auto_tune = tf.data.experimental.AUTOTUNE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MFOoZ6QAAtck","colab_type":"code","colab":{}},"source":["buffer_size = 1000\n","batch_size = 32\n","\n","epochs = 60\n","\n","number_of_attributes = 40\n","image_load_shape = (143, 143, 3)\n","image_shape = (128, 128, 3)\n","\n","adversarial_loss_mode = 'wgan'\n","\n","gradient_penalty_mode = '1-gp'\n","gradient_penalty_mode = 'none' # NEED TO CORRECT THIS\n","gradient_penalty_sample_mode = 'line'\n","\n","d_gradient_penalty_weight = 10.0\n","d_attribute_loss_weight = 1.0\n","g_attribute_loss_weight = 10.0\n","g_reconstruction_loss_weight = 100.0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U_WrESOBUbDU","colab_type":"text"},"source":["# Compute gradient penalty."]},{"cell_type":"code","metadata":{"id":"a_-B3KnigN9T","colab_type":"code","colab":{}},"source":["def _sample_line(real, fake):\n","    shape = [tf.shape(real)[0]] + [1] * (real.shape.ndims - 1)\n","    alpha = tf.random.uniform(shape=shape, minval=0, maxval=1)\n","    sample = real + alpha * (fake - real)\n","    sample.set_shape(real.shape)\n","    return sample\n","\n","\n","def _sample_DRAGAN(real, fake):  # fake is useless\n","    beta = tf.random.uniform(shape=tf.shape(real), minval=0, maxval=1)\n","    fake = real + 0.5 * tf.math.reduce_std(real) * beta\n","    sample = _sample_line(real, fake)\n","    return sample"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MxsfKfrpgPha","colab_type":"code","colab":{}},"source":["def _norm(x):\n","    norm = tf.norm(tf.reshape(x, [tf.shape(x)[0], -1]), axis=1)\n","    return(norm)\n","\n","def _one_mean_gp(grad):\n","    norm = _norm(grad)\n","    gp = tf.reduce_mean((norm - 1)**2)\n","    return(gp)\n","\n","def _zero_mean_gp(grad):\n","    norm = _norm(grad)\n","    gp = tf.reduce_mean(norm**2)\n","    return(gp)\n","\n","def _lipschitz_penalty(grad):\n","    norm = _norm(grad)\n","    gp = tf.reduce_mean(tf.maximum(norm - 1, 0)**2)\n","    return(gp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-r8GA6HrUbY6","colab_type":"code","colab":{}},"source":["def gradient_penalty(f, real, fake):\n","    sample_functions = {\n","        'line': _sample_line,\n","        'real': lambda real, fake: real,\n","        'fake': lambda real, fake: fake,\n","        'dragan': _sample_DRAGAN,\n","    }\n","\n","    gradient_penalty_functions = {\n","        '1-gp': _one_mean_gp,\n","        '0-gp': _zero_mean_gp,\n","        'lp': _lipschitz_penalty,\n","    }\n","\n","    if gradient_penalty_mode == 'none':\n","        gp = tf.constant(0, dtype=real.dtype)\n","    else:\n","        x = sample_functions[gradient_penalty_sample_mode](real, fake)\n","        # NEED TO CORRECT THIS\n","        grad = tf.gradients(f(x), x)[0] \n","        gp = gradient_penalty_functions[gradient_penalty_mode](grad)\n","\n","    return(gp)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kl5H3iS94rfm","colab_type":"text"},"source":["# Load CelebA dataset."]},{"cell_type":"code","metadata":{"id":"1MgTh1gjcKRe","colab_type":"code","colab":{}},"source":["builder = tfds.builder('celeb_a')\n","print(builder.info)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rat7-c2Jk6Kw","colab_type":"text"},"source":["### Download dataset from dataset Google drive."]},{"cell_type":"code","metadata":{"id":"TwPHT5vAk0pp","colab_type":"code","colab":{}},"source":["builder.download_and_prepare()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FnMh0LYKlIfL","colab_type":"text"},"source":["### Download dataset from Google drive."]},{"cell_type":"code","metadata":{"id":"DijemfFpjmT4","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J74njE9SkiOB","colab_type":"code","colab":{}},"source":["!cp -r '/content/drive/My Drive/datasets/tensorflow_datasets' /root/."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDGq9sthlVov","colab_type":"text"},"source":["### View dataset contents."]},{"cell_type":"code","metadata":{"id":"rYqApnKZja0y","colab_type":"code","colab":{}},"source":["!ls -al /root/tensorflow_datasets/celeb_a/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S6IsQNqwo59l","colab_type":"text"},"source":["### Create CelebA dataset splits.\n","* train\n","* validation\n","* test"]},{"cell_type":"code","metadata":{"id":"ZWkPtrzFlo7r","colab_type":"code","colab":{}},"source":["celeba_datasets = builder.as_dataset()\n","print(celeba_datasets)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"umpyIqqQl0ms","colab_type":"code","colab":{}},"source":["train_dataset = celeba_datasets['train']\n","val_dataset = celeba_datasets['validation']\n","test_dataset = celeba_datasets['test']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"obFo7G9HsZG5","colab_type":"text"},"source":["# Preprocess the dataset."]},{"cell_type":"markdown","metadata":{"id":"9ZbZMMuQsk7x","colab_type":"text"},"source":["### Normalize the image to [-1, 1]."]},{"cell_type":"code","metadata":{"id":"_ktprTPusjuz","colab_type":"code","colab":{}},"source":["def normalize(image):\n","  image = tf.cast(image, tf.float32)\n","  image = (image / 127.5) - 1\n","  return(image)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zjvtUqfdum2k","colab_type":"code","colab":{}},"source":["input_image = np.random.rand(image_shape[0], image_shape[1], image_shape[2])\n","output_image = normalize(input_image)\n","print('image shape',output_image.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjY1eyBXsz0F","colab_type":"text"},"source":["### Random crop the image."]},{"cell_type":"code","metadata":{"id":"zF4_Phj7s0J6","colab_type":"code","colab":{}},"source":["def random_crop(image):\n","  cropped_image = tf.image.random_crop(image, size=image_shape)\n","  return(cropped_image)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUjEwqh7u8Ck","colab_type":"code","colab":{}},"source":["input_image = np.random.rand(image_load_shape[0], image_load_shape[1], image_load_shape[2])\n","output_image = random_crop(input_image)\n","print('input image shape',input_image.shape)\n","print('output image shape',output_image.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TtWjAS4XtisQ","colab_type":"text"},"source":["### Random jitter the image."]},{"cell_type":"code","metadata":{"id":"YKO5hc_pti_S","colab_type":"code","colab":{}},"source":["def random_jitter(image):  \n","  image = tf.image.resize(image, [image_load_shape[0], image_load_shape[1]],\n","                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)  \n","  image = random_crop(image)\n","  image = tf.image.random_flip_left_right(image)\n","  return(image)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfu6UeB3wI76","colab_type":"code","colab":{}},"source":["input_image = np.random.rand(image_load_shape[0], image_load_shape[1], image_load_shape[2])\n","output_image = random_jitter(input_image)\n","print('input image shape',input_image.shape)\n","print('output image shape',output_image.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSN1M5XFuN5G","colab_type":"text"},"source":["### Preprocess train image."]},{"cell_type":"code","metadata":{"id":"lypmtexHpSE6","colab_type":"code","colab":{}},"source":["attributes_identifiers = {'5_o_Clock_Shadow': 0, 'Arched_Eyebrows': 1, 'Attractive': 2,\n","          'Bags_Under_Eyes': 3, 'Bald': 4, 'Bangs': 5, 'Big_Lips': 6,\n","          'Big_Nose': 7, 'Black_Hair': 8, 'Blond_Hair': 9, 'Blurry': 10,\n","          'Brown_Hair': 11, 'Bushy_Eyebrows': 12, 'Chubby': 13,\n","          'Double_Chin': 14, 'Eyeglasses': 15, 'Goatee': 16,\n","          'Gray_Hair': 17, 'Heavy_Makeup': 18, 'High_Cheekbones': 19,\n","          'Male': 20, 'Mouth_Slightly_Open': 21, 'Mustache': 22,\n","          'Narrow_Eyes': 23, 'No_Beard': 24, 'Oval_Face': 25,\n","          'Pale_Skin': 26, 'Pointy_Nose': 27, 'Receding_Hairline': 28,\n","          'Rosy_Cheeks': 29, 'Sideburns': 30, 'Smiling': 31,\n","          'Straight_Hair': 32, 'Wavy_Hair': 33, 'Wearing_Earrings': 34,\n","          'Wearing_Hat': 35, 'Wearing_Lipstick': 36,\n","          'Wearing_Necklace': 37, 'Wearing_Necktie': 38, 'Young': 39}\n","identifiers_attributes = {v: k for k, v in attributes_identifiers.items()}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e0pULJz7v0jU","colab_type":"code","colab":{}},"source":["def compute_attributes(attributes_batch):\n","  number_of_samples = len(attributes_batch[identifiers_attributes[0]])\n","  print('number_of_samples', number_of_samples)\n","  attributes_array = np.zeros((number_of_samples, number_of_attributes))\n","  for sample_index in range(number_of_samples):\n","    for index in range(len(identifiers_attributes)):\n","      attribute = identifiers_attributes[index]\n","      #print(attribute, attributes_batch[attribute][sample_index].numpy()*1.0)\n","      attributes_array[sample_index][index] = attributes_batch[attribute][sample_index].numpy()*1.0\n","\n","    #print(attributes_array[sample_index])\n","  return(attributes_array)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"72L03Hqex1Ge","colab_type":"code","colab":{}},"source":["def compute_attributes(attributes_batch):\n","  attributes_array = []\n","  for index in range(len(identifiers_attributes)):\n","      attribute = identifiers_attributes[index]\n","      #print(attribute, attributes_batch[attribute].numpy()*1.0)\n","      attributes_array.append(tf.cast(attributes_batch[attribute], dtype=tf.float32))\n","\n","    #print(attributes_array[sample_index])\n","  return(attributes_array)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8HOOpVQauOMn","colab_type":"code","colab":{}},"source":["def preprocess_train_image(sample):\n","  image = sample['image']\n","  attributes = sample['attributes']\n","  \n","  image = random_jitter(image)\n","  image = normalize(image)\n","\n","  sample['image'] = image\n","  sample['attributes'] = compute_attributes(attributes)\n","  return(sample)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nVRahQCtwljZ","colab_type":"code","colab":{}},"source":["input_image = np.random.rand(image_load_shape[0], image_load_shape[1], image_load_shape[2])\n","attributes = np.random.rand(number_of_attributes)\n","\n","sample = {}\n","sample['image'] = input_image\n","sample['attributes'] = attributes\n","\n","output = preprocess_train_image(sample)\n","print('input image shape',input_image.shape)\n","print('output image shape',output['image'].shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YU8coU3QuU24","colab_type":"text"},"source":["### Preprocess test image."]},{"cell_type":"code","metadata":{"id":"gpN0_XaXuVOp","colab_type":"code","colab":{}},"source":["def preprocess_test_image(sample):\n","  image = sample['image']\n","  attributes = sample['attributes']\n","\n","  image = tf.image.resize(image, [image_shape[0], image_shape[1]], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR) \n","  image = normalize(image)\n","\n","  sample['image'] = image\n","  sample['attributes'] = compute_attributes(attributes)\n","  return(sample)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZn94U9GwqUi","colab_type":"code","colab":{}},"source":["input_image = np.random.rand(image_load_shape[0], image_load_shape[1], image_load_shape[2])\n","attributes = np.random.rand(number_of_attributes)\n","\n","sample = {}\n","sample['image'] = input_image\n","sample['attributes'] = attributes\n","\n","output = preprocess_test_image(sample)\n","print('input image shape',input_image.shape)\n","print('output image shape',output['image'].shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_x_x37erxELV","colab_type":"text"},"source":["### Preprocess dataset splits."]},{"cell_type":"code","metadata":{"id":"tkZ_hKW5xEl3","colab_type":"code","colab":{}},"source":["train_dataset = train_dataset.map(preprocess_train_image, num_parallel_calls=auto_tune).cache().shuffle(buffer_size).padded_batch(batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dvPTaG_sxjVs","colab_type":"code","colab":{}},"source":["val_dataset = val_dataset.map(preprocess_test_image, num_parallel_calls=auto_tune).cache().shuffle(buffer_size).padded_batch(batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2LH4xT28snC","colab_type":"text"},"source":["# Create the optimizer.\n","\n","*   Adam optimizer\n","*   Learning rate = 0.0002\n","*   β1 = 0.5\n","*   β2 = 0.999"]},{"cell_type":"code","metadata":{"id":"l8UuboMN8s9z","colab_type":"code","colab":{}},"source":["generator_optimizer = tf.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n","discriminator_optimizer = tf.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q4jt3WYtlO1r","colab_type":"text"},"source":["# Create AttGAN encoder model."]},{"cell_type":"code","metadata":{"id":"tUa2yT9PlPg7","colab_type":"code","colab":{}},"source":["class UNetGenc(layers.Layer):\n","\n","  def __init__(self, dimension=64, downsamplings_layers=5):\n","    super(UNetGenc, self).__init__()\n","\n","    self._dimension = 64\n","    self._downsamplings_layers = 5\n","\n","  def call(self, inputs):\n","\n","    input_layer = inputs\n","    output_units = self._dimension\n","    \n","    output_layers = []\n","    for layer_index in range(self._downsamplings_layers):\n","      input_layer = layers.Conv2D(output_units, (4,4), strides=(2,2), padding='same')(input_layer)\n","      input_layer = layers.BatchNormalization()(input_layer)\n","      input_layer = layers.LeakyReLU(alpha=0.2)(input_layer)\n","\n","      output_layers.append(input_layer)\n","      output_units = output_units * 2    \n","    \n","    return(output_layers)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KsGuR6-N2FcS","colab_type":"code","colab":{}},"source":["input_image = np.random.rand(batch_size, image_shape[0], image_shape[1], image_shape[2])\n","encoder = UNetGenc()\n","output = encoder(input_image)\n","print('number of layers',len(output))\n","for layer in output:\n","  print('layer shape', layer.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45xdnUJiFS56","colab_type":"text"},"source":["# Concatenate features and attributes.\n","*   Tile all elements of attributes.\n","*   Concat features + attributes along the channel axis.\n","*   features shape - (N, H, W, C_a)\n","*   attributes shape - (N, 1, 1, C_b) or (N, C_b)"]},{"cell_type":"code","metadata":{"id":"Z_dLXktO6Hk5","colab_type":"code","colab":{}},"source":["def concatenate(list_of_features, list_of_attributes=[]):\n","  list_of_features = list(list_of_features) if isinstance(list_of_features, (list, tuple)) else [list_of_features]\n","  list_of_attributes = list(list_of_attributes) if isinstance(list_of_attributes, (list, tuple)) else [list_of_attributes]\n","  for index, attributes in enumerate(list_of_attributes):\n","        attributes = tf.reshape(attributes, [-1, 1, 1, attributes.shape[-1]])\n","        attributes = tf.tile(attributes, [1, list_of_features[0].shape[1], list_of_features[0].shape[2], 1])\n","        list_of_attributes[index] = attributes\n","  return tf.concat(list_of_features + list_of_attributes, axis=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_-pu_vcplanL","colab_type":"text"},"source":["# Create AttGAN decoder model."]},{"cell_type":"code","metadata":{"id":"_2mzE6UAlbAB","colab_type":"code","colab":{}},"source":["class UNetGdec(layers.Layer):\n","\n","  def __init__(self, dimension=64, upsamplings_layers=5, shortcut_layers=1, inject_layers=1):\n","    super(UNetGdec, self).__init__()\n","\n","    self._dimension = 64\n","    self._upsamplings_layers = 5\n","    self._shortcut_layers = shortcut_layers\n","    self._inject_layers = inject_layers\n","\n","  def call(self, inputs):\n","    features, attributes = inputs\n","\n","    #attributes = tensorflow.to_float(attributes)    \n","    output_units = self._dimension\n","\n","    input_layer = concatenate(features[-1], attributes)\n","    for layer_index in range(self._upsamplings_layers - 1):\n","      input_layer = layers.Conv2DTranspose(output_units, (4, 4), strides=(2,2), padding='same')(input_layer)\n","      input_layer = layers.BatchNormalization()(input_layer)\n","      input_layer = layers.LeakyReLU(alpha=0.2)(input_layer)\n","\n","      if (self._shortcut_layers > layer_index):\n","        input_layer = concatenate([input_layer, features[-2 - layer_index]])\n","\n","      if (self._inject_layers > layer_index):\n","        input_layer = concatenate(input_layer, attributes)\n","\n","      output_units = output_units * 2\n","\n","    input_layer = layers.Conv2DTranspose(3, (4, 4), strides=(2,2), padding='same')(input_layer)\n","    input_layer = tf.keras.activations.tanh(input_layer) \n","\n","    output_layer = input_layer\n","\n","    return(output_layer)  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3qYdNQM12Nzl","colab_type":"code","colab":{}},"source":["input_image = np.random.rand(batch_size, image_shape[0], image_shape[1], image_shape[2])\n","attributes = np.random.rand(batch_size, number_of_attributes)\n","\n","encoder = UNetGenc()\n","encoded_input = encoder(input_image)\n","\n","decoder = UNetGdec()\n","decoded_output = decoder([encoded_input, attributes])\n","print(decoded_output.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iT1NghHgl_Eh","colab_type":"text"},"source":["# Create AttGAN discriminator / classification model."]},{"cell_type":"code","metadata":{"id":"P3kFTSm4IIP6","colab_type":"code","colab":{}},"source":["import tensorflow_addons as tfa"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i99fuwWQmEH2","colab_type":"code","colab":{}},"source":["class Discriminator(layers.Layer):\n","\n","  def __init__(self, number_of_attributes=40, dimension=64, dense_dimension=1024, downsamplings_layers=5): \n","    super(Discriminator, self).__init__()  \n","\n","    self._number_of_attributes = number_of_attributes\n","    self._dimension = dimension\n","    self._dense_dimension = dense_dimension\n","    self._downsamplings_layers = downsamplings_layers\n","\n","  def call(self, inputs):\n","\n","      input_layer = inputs  \n","      output_units = self._dimension  \n","\n","      for layer_index in range(self._downsamplings_layers): \n","        input_layer = layers.Conv2D(output_units, (4,4), strides=(2,2), padding='same')(input_layer)         \n","        input_layer = tfa.layers.InstanceNormalization()(input_layer)\n","        input_layer = layers.LeakyReLU(alpha=0.2)(input_layer)\n","\n","        output_units = output_units * 2\n","\n","      input_layer = layers.Flatten()(input_layer)\n","\n","      discriminator_output = layers.Dense(self._dense_dimension)(input_layer) \n","      discriminator_output = tfa.layers.InstanceNormalization()(discriminator_output)     \n","      discriminator_output = layers.LeakyReLU(alpha=0.2)(discriminator_output)\n","      discriminator_output = layers.Dense(1)(discriminator_output)\n","      \n","      attribute_output = layers.Dense(self._dense_dimension)(input_layer)  \n","      attribute_output = tfa.layers.InstanceNormalization()(attribute_output)         \n","      attribute_output = layers.LeakyReLU(alpha=0.2)(attribute_output)\n","      attribute_output = layers.Dense(self._number_of_attributes, activation='sigmoid')(attribute_output)\n","\n","      return([discriminator_output, attribute_output])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5Y3uQuN2Opc","colab_type":"code","colab":{}},"source":["input_image = np.random.rand(batch_size, image_shape[0], image_shape[1], image_shape[2])\n","\n","discriminator = Discriminator(number_of_attributes)\n","discriminator_prediction, attribute_prediction = discriminator(input_image)\n","\n","print('discriminator prediction shape', discriminator_prediction.shape)\n","print('attribute prediction shape', attribute_prediction.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"46wGfhmb1mAY","colab_type":"text"},"source":["# Create adversarial loss functions.\n","*   Generator loss function\n","*   Discriminator loss function"]},{"cell_type":"markdown","metadata":{"id":"dNVC14t63f_a","colab_type":"text"},"source":["## WGAN loss functions.\n","*   Generator loss function\n","*   Discriminator loss function"]},{"cell_type":"code","metadata":{"id":"SRmIcg_33WoD","colab_type":"code","colab":{}},"source":["def wgan_loss_functions():\n","    def discriminator_loss_function(real_logit, fake_logit):\n","        real_loss = - tf.reduce_mean(real_logit)\n","        fake_loss = tf.reduce_mean(fake_logit)\n","        return(real_loss, fake_loss)\n","\n","    def generator_loss_function(fake_logit):\n","        fake_loss = - tf.reduce_mean(fake_logit)\n","        return(fake_loss)\n","\n","    return(discriminator_loss_function, generator_loss_function)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CP56ouow1mgf","colab_type":"code","colab":{}},"source":["def adversarial_loss_functions(adversarial_loss_mode):\n","  if(adversarial_loss_mode == 'wgan'):\n","    return(wgan_loss_functions())\n","  else:\n","    return(wgan_loss_functions())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qshxq7QobTQI","colab_type":"text"},"source":["# Create different models and loss functions.\n","* Encoder model\n","* Decoder model\n","* Discriminator model\n","* Discriminator loss function\n","* Generator loss function"]},{"cell_type":"code","metadata":{"id":"BXNN-azN4r05","colab_type":"code","colab":{}},"source":["encoder = UNetGenc()\n","decoder = UNetGdec()\n","discriminator = Discriminator(number_of_attributes)\n","\n","discriminator_loss_function, generator_loss_function = adversarial_loss_functions(adversarial_loss_mode)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DmRb8Ok3-db5","colab_type":"text"},"source":["# Create composite generator model."]},{"cell_type":"code","metadata":{"id":"CfAvkKJj-dxp","colab_type":"code","colab":{}},"source":["def create_composite_generator(image_shape, number_of_attributes):\n","\n","  xa = layers.Input(shape=image_shape, name='input_image')\n","  a = layers.Input(shape=(number_of_attributes), name='attributes')\n","\n","  b = tf.random.shuffle(a)\n","\n","  a_ = a * 2 - 1\n","  b_ = b * 2 - 1\n","\n","  # Generate\n","  z = encoder(xa)\n","  xa_ = decoder([z, a_])\n","  xb_ = decoder([z, b_])\n","\n","  # Discriminate\n","  xb__logit_gan, xb__logit_att = discriminator(xb_)\n","\n","  xb__loss_gan = generator_loss_function(xb__logit_gan)\n","  xb__loss_att = tf.keras.losses.categorical_crossentropy(b, xb__logit_att)\n","  xa__loss_rec = tf.keras.losses.mae(xa, xa_)\n","\n","  loss = (xb__loss_gan + \n","            xb__loss_att * g_attribute_loss_weight +\n","            xa__loss_rec * g_reconstruction_loss_weight)\n","\n","  composite_model = tf.keras.models.Model([xa, a], [loss])  \n","\n","  return(composite_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fVASb-jsEbsO","colab_type":"code","colab":{}},"source":["composite_generator = create_composite_generator(image_shape, number_of_attributes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b5Zls6m5NOv5","colab_type":"text"},"source":["# Create composite discriminator model."]},{"cell_type":"code","metadata":{"id":"ViRU0tj7NPfZ","colab_type":"code","colab":{}},"source":["def create_composite_discriminator(image_shape, number_of_attributes):\n","\n","  xa = layers.Input(shape=image_shape)\n","  a = layers.Input(shape=(number_of_attributes))\n","\n","  b = tf.random.shuffle(a)\n","\n","  a_ = a * 2 - 1\n","  b_ = b * 2 - 1\n","\n","  # Generate\n","  z = encoder(xa)  \n","  xb_ = decoder([z, b_])\n","\n","  # Discriminate\n","  xa_logit_gan, xa_logit_att = discriminator(xa)\n","  xb__logit_gan, xb__logit_att = discriminator(xb_)\n","\n","  # Discriminator losses\n","  xa_loss_gan, xb__loss_gan = discriminator_loss_function(xa_logit_gan, xb__logit_gan)\n","  gp = gradient_penalty(lambda x: discriminator(x)[0], xa, xb_)\n","  xa_loss_att = tf.losses.categorical_crossentropy(a, xa_logit_att)\n","\n","  loss = (xa_loss_gan + xb__loss_gan +\n","            gp * d_gradient_penalty_weight +\n","            xa_loss_att * d_attribute_loss_weight)\n","  \n","  composite_model = tf.keras.models.Model([xa, a], [loss])  \n","\n","  return(composite_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P1toecYCfq4H","colab_type":"code","colab":{}},"source":["composite_discriminator = create_composite_discriminator(image_shape, number_of_attributes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hf1OgYwVm4fi","colab_type":"text"},"source":["# Train the model."]},{"cell_type":"code","metadata":{"id":"3Dxd0Hmg4mgx","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","sample_image = next(iter(train_dataset))\n","\n","plt.subplot(121)\n","plt.title('sample image')\n","plt.imshow(sample_image['image'][0] * 0.5 + 0.5)\n","print(sample_image['attributes'][0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mf0OCsH_m6xL","colab_type":"code","colab":{}},"source":["for dataset_batch in train_dataset:\n","  attributes = dataset_batch['attributes']\n","  images = dataset_batch['image']\n","  size = len(images)\n","  print(size)\n","  print(attributes)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJVAe5MrfCSN","colab_type":"code","colab":{}},"source":["def train(train_dataset, val_dataset, epochs=60):\n","  for epoch in range(epochs):\n","    for dataset_batch in train_dataset:\n","\n","      attributes = dataset_batch['attributes']\n","      images = dataset_batch['image']\n","\n","      with tf.GradientTape(persistent=True) as tape:\n","        composite_discriminator_loss = composite_discriminator([images, attributes], training=True)\n","        composite_generator_loss = composite_generator([images, attributes], training=True)\n","\n","      composite_discriminator_gradients = tape.gradient(composite_discriminator_loss, composite_discriminator.trainable_variables)\n","      composite_generator_gradients = tape.gradient(composite_generator_loss, composite_generator.trainable_variables)\n","\n","      discriminator_optimizer.apply_gradients(zip(composite_discriminator_gradients, composite_discriminator.trainable_variables))\n","      generator_optimizer.apply_gradients(zip(composite_generator_gradients, composite_generator.trainable_variables))\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZWMK-qlUfUf-","colab_type":"code","colab":{}},"source":["train(train_dataset, val_dataset, epochs)"],"execution_count":0,"outputs":[]}]}